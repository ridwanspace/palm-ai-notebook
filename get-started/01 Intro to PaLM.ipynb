{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LLMs and PalM?\n",
    "\n",
    "This document provides an overview of large language models (LLMs) and Google's next-generation LLM, PaLM 2. LLMs are deep learning models trained on massive datasets of text. PaLM 2 excels at tasks like advanced reasoning, translation, and code generation. It builds on Google's legacy of breakthrough research in machine learning and responsible AI. LLMs are created using unsupervised learning, where the model learns to predict the next word in a sentence given the preceding words. This enables the model to generate coherent, fluent text resembling human writing. The model's large size allows it to learn complex patterns and relationships in language and generate high-quality text for various applications.\n",
    "\n",
    "### Available Models\n",
    "\n",
    "Vertex AI PaLM API models\n",
    "The Vertex AI PaLM API enables you to test, customize, and deploy instances of Googleâ€™s large language models (LLM) called as PaLM, so that you can leverage the capabilities of PaLM in your applications.\n",
    "\n",
    "Model naming scheme\n",
    "Foundation model names have three components: use case, model size, and version number. The naming convention is in the format:\n",
    "`<use case>-<model size>@<version number>`\n",
    "\n",
    "For example, _text-bison@001_ represents the Bison text model, version _001_.\n",
    "\n",
    "The model sizes are as follows:\n",
    "\n",
    "* __Bison__: The best value in terms of capability and cost.\n",
    "* __Gecko__: The smallest and cheapest model for simple tasks.\n",
    "\n",
    "Available models\n",
    "The Vertex AI PaLM API currently supports five models:\n",
    "\n",
    "* `text-bison@001` : Fine-tuned to follow natural language instructions and is suitable for a variety of language tasks.\n",
    "\n",
    "* `chat-bison@001` : Fine-tuned for multi-turn conversation use cases like building a chatbot.\n",
    "\n",
    "* `textembedding-gecko@001` : Returns model embeddings for text inputs.\n",
    "\n",
    "* `code-bison@001`: A model fine-tuned to generate code based on a natural language description of the desired code. For example, it can generate a unit test for a function.\n",
    "\n",
    "* `code-gecko@001`: A model fine-tuned to suggest code completion based on the context in code that's written.\n",
    "\n",
    "* `codechat-bison@001`: A model fine-tuned for chatbot conversations that help with code-related questions.\n",
    "\n",
    "You can find more information about the properties of these foundational models in the [Generative AI Studio documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import vertexai\n",
    "from IPython.display import Markdown, display\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from vertexai.language_models import TextGenerationModel, \\\n",
    "                                     ChatModel, \\\n",
    "                                     InputOutputTextPair, \\\n",
    "                                     CodeGenerationModel, \\\n",
    "                                     CodeChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate service account (authentication)\n",
    "json_path = '../../llm-ai.json' # replace with your own service account\n",
    "credentials = service_account.Credentials.from_service_account_file(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Vertex AI\n",
    "load_dotenv()\n",
    "vertexai.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with `text-bison@001`\n",
    "\n",
    "The text generation model from PaLM API that you will use in this notebook is text-bison@001. It is fine-tuned to follow natural language instructions and is suitable for a variety of language tasks, such as:\n",
    "\n",
    "* Classification\n",
    "* Sentiment analysis\n",
    "* Entity extraction\n",
    "* Extractive question-answering\n",
    "* Summarization\n",
    "* Re-writing text in a different style\n",
    "* Ad copy generation\n",
    "* Concept ideation\n",
    "* Concept simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hello PaLM__\n",
    "\n",
    "Create the first prompt and send it to the text generation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaLM AI (Pathways Language Model) is a large language model from Google AI. It was trained on a massive dataset of text and code, and it can understand and generate human language in a variety of ways. PaLM AI is still under development, but it has already been shown to be capable of impressive feats, such as writing different kinds of creative text, translating languages, answering your questions, and writing different kinds of creative text.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is PaLM AI?\"\n",
    "\n",
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Another Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **A.I.**\n",
       "2. **B.R.A.I.N.**\n",
       "3. **Cognito**\n",
       "4. **Intelli**\n",
       "5. **MindMeld**\n",
       "6. **Nervana**\n",
       "7. **OpenAI**\n",
       "8. **PaLM**\n",
       "9. **Watson**\n",
       "10. **X.AI.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Generate a list of AI company name ideas.\n",
    "A list should be 10 bullet points.\n",
    "Each company name should be 2 words or less.\"\"\" # try your own prompt\n",
    "\n",
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "# return as markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "Prompt templates are useful if you have found a good way to structure your prompt that you can re-use. This can be also be helpful in limiting the open-endedness of freeform prompts. There are many ways to implement prompt templates, and below is just one example using `f-strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* **Meatless Mondays** - A trend where people eat meatless meals on Mondays to reduce their meat consumption.\n",
       "* **Meatless burgers** - A plant-based burger that is made to look and taste like a traditional beef burger.\n",
       "* **Vegan hot dogs** - A plant-based hot dog that is made to look and taste like a traditional pork hot dog.\n",
       "* **Tofu scramble** - A dish made from scrambled tofu that is seasoned to taste like scrambled eggs.\n",
       "* **Chickpea salad** - A salad made from chickpeas, tomatoes, cucumbers, and other vegetables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_ingredient = \"meat\" # try changing this to a different industry\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=f\"\"\"Create a list of trending menus made from {my_ingredient} as main ingredient.\n",
    "    Each trend should be less than 3 words.\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters for `text-bison@001`\n",
    "\n",
    "We can customize how the PaLM API behaves in response to your prompt by using the following parameters for text-bison@001:\n",
    "\n",
    "* `temperature`: higher means more \"creative\" responses\n",
    "* `max_output_tokens`: sets the max number of tokens in the output\n",
    "* `top_p`: higher means it will pull from more possible next tokens, based on cumulative probability\n",
    "* `top_k`: higher means it will sample from more possible next tokens\n",
    "\n",
    "For more detail model parameters, refer to this [documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. The `temperature` parameter (range: 0.0 - 1.0, default 0)\n",
    "\n",
    "The temperature is used for sampling during the response generation, which occurs when top_p and top_k are applied. Temperature controls the degree of randomness in token selection.\n",
    "\n",
    "Lower temperatures are good for prompts that require a more deterministic and less open-ended response. In comparison, higher temperatures can lead to more \"creative\" or diverse results. A temperature of 0 is deterministic: the highest probability response is always selected. For most use cases, try starting with a temperature of 0.2.\n",
    "\n",
    "A higher temperature value will result in a more exploratative output, with a higher likelihood of generating rare or unusual words or phrases. Conversely, a lower temperature value will result in a more conservative output, with a higher likelihood of generating common or expected words or phrases.\n",
    "\n",
    "Example:\n",
    "For example,\n",
    "\n",
    "`temperature = 0.0:`\n",
    "\n",
    "* _The cat sat on the couch, watching the birds outside._\n",
    "* _The cat sat on the windowsill, basking in the sun._\n",
    "\n",
    "\n",
    "`temperature = 0.9:`\n",
    "\n",
    "* _The cat sat on the moon, meowing at the stars._\n",
    "* _The cat sat on the cheeseburger, purring with delight._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 0.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "a park."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_val = 0.0\n",
    "prompt_temperature = \"Complete the sentence: As I walked around in the downtown, I found myself in:\"\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_temperature,\n",
    "    temperature=temp_val,\n",
    ")\n",
    "\n",
    "print(f\"[temperature = {temp_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 1.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "the central business district."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_val = 1.0\n",
    "prompt_temperature = \"Complete the sentence: As I walked around in the downtown, I found myself in:\"\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_temperature,\n",
    "    temperature=temp_val,\n",
    ")\n",
    "\n",
    "print(f\"[temperature = {temp_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. The `max_output_tokens` parameter (range: 1 - 1024, default 128)\n",
    "\n",
    "__Tokens__\n",
    "A single token may be smaller than a word. For example, a token is approximately four characters. So 100 tokens correspond to roughly 60-80 words. It's essential to be aware of the token sizes as models have a limit on input and output tokens.\n",
    "\n",
    "__What is max_output_tokens?__\n",
    "max_output_tokens is the maximum number of tokens that can be generated in the response.\n",
    "\n",
    "__How does max_output_tokens affect the response?__\n",
    "Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[max_output_tokens = 5]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **Automat"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_output_tokens_val = 5\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=\"List ten ways that generative AI can help improve the manufacturing industry\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    "\n",
    "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[max_output_tokens = 500]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **Automate product design.** Generative AI can be used to create new product designs, which can save time and money for manufacturers.\n",
       "2. **Optimize manufacturing processes.** Generative AI can be used to optimize manufacturing processes, such as by identifying the most efficient way to assemble a product or by finding ways to reduce waste.\n",
       "3. **Improve quality control.** Generative AI can be used to identify defects in products, which can help manufacturers to improve quality control and reduce the number of products that need to be scrapped.\n",
       "4. **Personalize products.** Generative AI can be used to create personalized products, such as by customizing the design of a product or by suggesting features that would be most appealing to a particular customer.\n",
       "5. **Reduce costs.** Generative AI can help manufacturers to reduce costs by automating tasks, optimizing processes, and improving quality control.\n",
       "6. Increase productivity.** Generative AI can help manufacturers to increase productivity by automating tasks, optimizing processes, and improving quality control.\n",
       "7. **Speed up product development.** Generative AI can speed up product development by automating tasks, such as generating product designs and identifying defects.\n",
       "8. **Improve customer satisfaction.** Generative AI can help manufacturers to improve customer satisfaction by providing personalized products and services, and by reducing the number of defects in products.\n",
       "9. **Create new business opportunities.** Generative AI can help manufacturers to create new business opportunities by developing new products and services, and by entering new markets.\n",
       "10. **Stay ahead of the competition.** Generative AI can help manufacturers to stay ahead of the competition by automating tasks, optimizing processes, and improving quality control."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_output_tokens_val = 500\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=\"List ten ways that generative AI can help improve the manufacturing industry\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    "\n",
    "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. The `top_p` parameter (range: 0.0 - 1.0, default 0.95)\n",
    "\n",
    "__What is top_p?__\n",
    "top_p controls how the model selects tokens for output by adjusting the probability distribution of the next word in the generated text based on a cumulative probability cutoff. Specifically, it selects the smallest set of tokens whose cumulative probability exceeds the given cutoff probability p, and samples from this set uniformly.\n",
    "\n",
    "For example, suppose tokens `A`, `B`, and `C` have a probability of `0.3`, `0.2`, and `0.1`, and the top_p value is __0.5__. In that case, the model will select either A or B as the next token (using temperature) and not consider C, because the cumulative probability of top_p is <= 0.5. Specify a lower value for less random responses and a higher value for more random responses.\n",
    "\n",
    "__How does top_p affect the response?__\n",
    "The top_p parameter is used to control the diversity of the generated text. A _higher_ top_p parameter value results in more \"diverse\" and \"interesting\" outputs, with the model being allowed to sample from a larger pool of possibilities. In contrast, a _lower_ top_p parameter value resulted in more predictable outputs, with the model being constrained to a smaller set of possible tokens.\n",
    "\n",
    "Example:\n",
    "\n",
    "`top_p = 0.1:`\n",
    "\n",
    "* _The cat sat on the mat._\n",
    "* _The cat sat on the floor._\n",
    "\n",
    "`top_p = 0.9:`\n",
    "\n",
    "* _The cat sat on the windowsill, soaking up the sun's rays._\n",
    "* _The cat sat on the edge of the bed, watching the birds outside._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 0.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "* \"Climate change is real, and it's here to stay. We need to take action now, before it's too late.\"\n",
       "* \"We can't afford to ignore climate change any longer. It's time for bold action.\"\n",
       "* \"The future of our planet is at stake. We need to elect a leader who will take action on climate change.\"\n",
       "* \"Climate change is the most important issue facing our planet today. We need to elect a leader who will make it a top priority.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_p_val = 0.0\n",
    "prompt_top_p_example = (\n",
    "    \"Create a slogan for a candidate focusing on climate change.\"\n",
    ")\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_p_example, temperature=0.9, top_p=top_p_val\n",
    ")\n",
    "\n",
    "print(f\"[top_p = {top_p_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 1.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"We need a leader who will take action on climate change. I'm that leader.\"\n",
       "\n",
       "This slogan is effective because it is short, memorable, and to the point. It also highlights the candidate's position on climate change and their commitment to taking action."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_p_val = 1.0\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_p_example, temperature=0.9, top_p=top_p_val\n",
    ")\n",
    "\n",
    "print(f\"[top_p = {top_p_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. The `top_k` parameter (range: 0.0 - 40, default 40)\n",
    "\n",
    "__What is top_k?__\n",
    "\n",
    "top_k changes how the model selects tokens for output. A top_k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding). In contrast, a top_k of 3 means that the next token is selected from the top 3 most probable tokens (using temperature). For each token selection step, the top_k tokens with the highest probabilities are sampled. Then tokens are further filtered based on top_p with the final token selected using temperature sampling.\n",
    "\n",
    "__How does top_k affect the response?__\n",
    "\n",
    "Specify a lower value for less random responses and a higher value for more random responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_k = 1]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Day 1:\n",
       "* Morning: Start your day with a sunrise yoga session at Uluwatu Temple. This is one of the most popular temples in Bali and offers stunning views of the Indian Ocean.\n",
       "* Afternoon: After yoga, head to the Ubud Monkey Forest. This is a sacred forest that is home to hundreds of monkeys. You can walk through the forest and see the monkeys up close.\n",
       "* Evening: Enjoy a traditional Balinese dinner at a local restaurant. There are many great restaurants in Ubud that serve traditional Balinese cuisine.\n",
       "\n",
       "Day 2:\n",
       "* Morning: Start your day with a visit to the Tanah Lot Temple. This temple is built on a rock formation that is surrounded by the ocean. It is one of the most iconic temples in Bali.\n",
       "* Afternoon: After visiting the temple, head to the Tirta Empul Temple. This temple is dedicated to the god of water and is a popular spot for locals to come and bathe in the holy water.\n",
       "* Evening: Enjoy a sunset cruise on the Balinese Sea. This is a great way to see the sunset and get a different perspective of the island."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_top_k_example = \"Write a 2-day itinerary for Bali.\"\n",
    "top_k_val = 1\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_k_example, \n",
    "    max_output_tokens=300, \n",
    "    temperature=0.9, \n",
    "    top_k=top_k_val\n",
    ")\n",
    "\n",
    "print(f\"[top_k = {top_k_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_k = 40]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Day 1:**\n",
       "\n",
       "* Morning: Start your day with a sunrise hike up to Mount Batur, a 1,717-meter-high volcano. The views from the top are stunning, and you'll be rewarded with a glimpse of the sun rising over the clouds.\n",
       "* Afternoon: After your hike, head to the Tegalalang Rice Terraces, a UNESCO World Heritage Site. Take a walk through the fields and admire the beautiful scenery. You can also stop for a traditional Balinese lunch at one of the many restaurants in the area.\n",
       "* Evening: In the evening, enjoy a relaxing sunset cruise on the Ubud River. As you cruise, you'll see the sun sink below the horizon and light up the sky with beautiful colors.\n",
       "\n",
       "**Day 2:**\n",
       "\n",
       "* Morning: Start your day with a visit to the Sacred Monkey Forest in Ubud. This is a popular tourist attraction, but it's worth a visit for the chance to see the monkeys up close.\n",
       "* Afternoon: In the afternoon, head to the Tanah Lot Temple, a Hindu temple that is built on a rock formation in the sea. The temple is a popular spot for sunset, and you can enjoy stunning views of the ocean and the surrounding cliffs.\n",
       "* Evening: In the evening, enjoy a traditional Balinese dance performance. There are many different dance performances to choose from, so you can find one that suits your interests."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_k_val = 40\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_k_example,\n",
    "    max_output_tokens=300,\n",
    "    temperature=0.9,\n",
    "    top_k=top_k_val,\n",
    ")\n",
    "\n",
    "print(f\"[top_k = {top_k_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat model with `chat-bison@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chat-bison@001` model lets you have a freeform conversation across multiple turns. The application tracks what was previously said in the conversation. As such, if you expect to use conversations in your application, use the `chat-bison@001` model because it has been fine-tuned for multi-turn conversation use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chat_model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Course Outline\n",
      "\n",
      "This course is designed to teach Python developers the basics of Golang. It covers the following topics:\n",
      "\n",
      "* Golang syntax\n",
      "* Data types\n",
      "* Control flow statements\n",
      "* Functions\n",
      "* Packages\n",
      "* Modules\n",
      "* Concurrency\n",
      "* Testing\n",
      "* Deployment\n",
      "\n",
      "## Learning Objectives\n",
      "\n",
      "By the end of this course, you will be able to:\n",
      "\n",
      "* Write simple Golang programs\n",
      "* Use Golang data types and control flow statements\n",
      "* Define and call functions\n",
      "* Use packages and modules\n",
      "* Write concurrent programs\n",
      "* Write unit tests for your Golang programs\n",
      "* Deploy Golang programs to production\n",
      "\n",
      "## Prerequisites\n",
      "\n",
      "This course assumes that you have a basic understanding of programming. You should be familiar with the following concepts:\n",
      "\n",
      "* Variables\n",
      "* Data types\n",
      "* Control flow statements\n",
      "* Functions\n",
      "* Modules\n",
      "\n",
      "## Resources\n",
      "\n",
      "The following resources are provided to help you learn Golang:\n",
      "\n",
      "* [The Go Programming Language Book](https://golang.org/doc/book/)\n",
      "* [The Go Programming Language Tutorial](https://golang.org/doc/tutorial/)\n",
      "* [The Go Programming Language Playground](https://play.golang.org/)\n",
      "* [The Go Programming Language Forums](https://golang.org/forum/)\n",
      "* [The Go Programming Language Slack](https://golang.org/slack/)\n",
      "\n",
      "## Schedule\n",
      "\n",
      "This course is self-paced and can be completed at your own pace. The course material is organized into five modules:\n",
      "\n",
      "* Module 1: Golang Basics\n",
      "* Module 2: Data Types and Control Flow Statements\n",
      "* Module 3: Functions and Packages\n",
      "* Module 4: Concurrency\n",
      "* Module 5: Testing and Deployment\n",
      "\n",
      "Each module contains a series of lessons and exercises. You should complete the lessons and exercises in each module before moving on to the next module.\n",
      "\n",
      "## Assessment\n",
      "\n",
      "There is no formal assessment for this course. However, you are encouraged to complete the lessons and exercises in each module. You can also submit your code to the Go Playground for feedback.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This course has provided you with a basic introduction to Golang. You should now be able to write simple Golang programs. To learn more about Golang, you can refer to the resources listed in the Resources section.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chat.send_message(\n",
    "        \"\"\"\n",
    "Hello! Can you write a course outline to learn Golang for Python developer?\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the model should respond based on what was previously said in the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrency in Go is achieved through the use of goroutines. A goroutine is a lightweight thread that is managed by the Go runtime. Goroutines are created using the `go` keyword.\n",
      "\n",
      "For example, the following code creates a goroutine that prints the message \"Hello, world!\" to the console:\n",
      "\n",
      "```\n",
      "go func() {\n",
      "  fmt.Println(\"Hello, world!\")\n",
      "}()\n",
      "```\n",
      "\n",
      "The `go` keyword tells the Go runtime to create a new goroutine and run the function inside it. The function inside the goroutine is called a closure. A closure is a function that has access to the variables of the enclosing scope.\n",
      "\n",
      "In the example above, the closure has access to the variable `fmt`. This allows the closure to print the message \"Hello, world!\" to the console.\n",
      "\n",
      "Goroutines are lightweight and efficient. They are also easy to create and manage. This makes them a powerful tool for writing concurrent programs in Go.\n",
      "\n",
      "Here are some additional resources on concurrency in Go:\n",
      "\n",
      "* [The Go Programming Language Book](https://golang.org/doc/book/)\n",
      "* [The Go Programming Language Tutorial](https://golang.org/doc/tutorial/)\n",
      "* [The Go Programming Language Playground](https://play.golang.org/)\n",
      "* [The Go Programming Language Forums](https://golang.org/forum/)\n",
      "* [The Go Programming Language Slack](https://golang.org/slack/)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chat.send_message(\n",
    "        \"\"\"\n",
    "Could you give me short explanation on Concurrency in Go?\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Chat model with the SDK\n",
    "\n",
    "We can also provide a context and examples to the model. The model will then respond based on the provided context and examples. We can also use `temperature`, `max_output_tokens`, `top_p`, and `top_k`. These parameters should be used when we start our chat with `chat_model.start_chat()`.\n",
    "\n",
    "For more information on chat models, please refer to the [documentation on chat model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#chat_model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, all of Tom's favorite movies are based on Marvel Comics.\n"
     ]
    }
   ],
   "source": [
    "chat = chat_model.start_chat(\n",
    "    context=\"My name is Tom. You are my personal assistant. My favorite movies are all list in Marvel Cinematic Universe.\",\n",
    "    examples=[\n",
    "        InputOutputTextPair(\n",
    "            input_text=\"Who do you work for?\",\n",
    "            output_text=\"I work for Tom.\",\n",
    "        ),\n",
    "        InputOutputTextPair(\n",
    "            input_text=\"What do I like?\",\n",
    "            output_text=\"Tom likes watching all Marvel Cinematic Universe movies.\",\n",
    "        ),\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=200,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "print(chat.send_message(\"Are my favorite movies based on a comic?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first Marvel Cinematic Universe movie was Iron Man, which came out in 2008.\n"
     ]
    }
   ],
   "source": [
    "print(chat.send_message(\"When the first movie came out?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most popular Marvel Cinematic Universe movie is Avengers: Endgame, which came out in 2019.\n"
     ]
    }
   ],
   "source": [
    "print(chat.send_message(\"What is the most popular movie?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Endgame grossed over $2.79 billion worldwide, making it the highest-grossing film of all time.\n"
     ]
    }
   ],
   "source": [
    "print(chat.send_message(\"How much is gross revenue for it?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation with `code-bison@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code generation model (Codey) from PaLM API that you will use in this notebook is code-bison@001. It is fine-tuned to follow natural language instructions to generate required code and is suitable for a variety of coding tasks, such as:\n",
    "\n",
    "* writing functions\n",
    "* writing classes\n",
    "* web-apges\n",
    "* unit tests\n",
    "* docstrings\n",
    "* code translations, and many more use-cases.\n",
    "\n",
    "Currently it supports the following languages:\n",
    "\n",
    "* C++\n",
    "* C#\n",
    "* Go\n",
    "* GoogleSQL\n",
    "* Java\n",
    "* JavaScript\n",
    "* Kotlin\n",
    "* PHP\n",
    "* Python\n",
    "* Ruby\n",
    "* Rust\n",
    "* Scala\n",
    "* Swift\n",
    "* TypeScript\n",
    "\n",
    "You can find our more details [here](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generation_model = CodeGenerationModel.from_pretrained(\"code-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model parameters for `code-bison@001`\n",
    "\n",
    "We can customize how the PaLM API code generation behaves in response to our prompt by using the following parameters for `code-bison@001`:\n",
    "\n",
    "* `prefix`: it represents the beginning of a piece of meaningful programming code or a natural language prompt that describes code to be generated.\n",
    "\n",
    "* `temperature`: higher means more \"creative\" code responses. range: (0.0 - 1.0, default 0).\n",
    "\n",
    "* `max_output_tokens`: sets the max number of tokens in the output. range: (1 - 2048, default 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hello Codey, Create some code for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def scrape_website(url):\n",
       "  \"\"\"Scrape the given website and return the HTML content.\n",
       "\n",
       "  Args:\n",
       "    url: The URL of the website to scrape.\n",
       "\n",
       "  Returns:\n",
       "    The HTML content of the website.\n",
       "  \"\"\"\n",
       "\n",
       "  # Get the HTML content of the website.\n",
       "  response = requests.get(url)\n",
       "  response.raise_for_status()\n",
       "  html_content = response.content\n",
       "\n",
       "  # Return the HTML content.\n",
       "  return html_content\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"write a python function to do scrape a website\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it on your own\n",
    "\n",
    "Some examples:\n",
    "\n",
    "* write node js model using mongodb\n",
    "* write python code to validate email address\n",
    "* write a standard SQL function that concatenating 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def sentence_similarity(sentence1, sentence2):\n",
       "\n",
       "  \"\"\"\n",
       "  This function takes two sentences as input and returns the similarity score in percentage with 2 decimals.\n",
       "\n",
       "  Args:\n",
       "    sentence1 (str): The first sentence.\n",
       "    sentence2 (str): The second sentence.\n",
       "\n",
       "  Returns:\n",
       "    float: The similarity score in percentage with 2 decimals.\n",
       "  \"\"\"\n",
       "\n",
       "  # Step 1: Remove stop words and punctuation from the sentences.\n",
       "\n",
       "  sentence1_no_stopwords = remove_stopwords(sentence1)\n",
       "  sentence2_no_stopwords = remove_stopwords(sentence2)\n",
       "\n",
       "  # Step 2: Stem the words in the sentences.\n",
       "\n",
       "  sentence1_stemmed = stem_words(sentence1_no_stopwords)\n",
       "  sentence2_stemmed = stem_words(sentence2_no_stopwords)\n",
       "\n",
       "  # Step 3: Calculate the Jaccard similarity score.\n",
       "\n",
       "  jaccard_similarity_score = jaccard_similarity(sentence1_stemmed, sentence2_stemmed)\n",
       "\n",
       "  # Step 4: Return the similarity score in percentage with 2 decimals.\n",
       "\n",
       "  return round(jaccard_similarity_score * 100, 2)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"\"\"write a python function named as \"sentence_similairty\"\\\n",
    "            where it takes two arguments \"sentence1\" and \"sentence2\". \\\n",
    "            It then returns the similarity score in percentage with 2 decimals. \\n\n",
    "          \"\"\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix, max_output_tokens=1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Prompt Template\n",
    "\n",
    "Prompt templates are useful if we have found a good way to structure our prompt that we can re-use. This can be also be helpful in limiting the open-endedness of freeform prompts. There are many ways to implement prompt templates, and below is just one example using `f-strings`. This way we can structure the prompts as per the expected funcationality of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "#include <iostream>\n",
       "#include <string>\n",
       "#include <vector>\n",
       "#include <map>\n",
       "\n",
       "using namespace std;\n",
       "\n",
       "// This function parses a JSON object and extracts all of the names that start with capital letters.\n",
       "// The function also ensures that there are no duplicate names in the final list.\n",
       "vector<string> getNames(const string& json) {\n",
       "  // Create a map to store the names.\n",
       "  map<string, bool> names;\n",
       "\n",
       "  // Parse the JSON object.\n",
       "  stringstream ss(json);\n",
       "  json::value_type root;\n",
       "  ss >> root;\n",
       "\n",
       "  // Iterate over the object's properties.\n",
       "  for (json::object_iterator it = root.begin(); it != root.end(); ++it) {\n",
       "    // Get the property name.\n",
       "    string name = it->first;\n",
       "\n",
       "    // Check if the name starts with a capital letter.\n",
       "    if (name[0] >= 'A' && name[0] <= 'Z') {\n",
       "      // Add the name to the map.\n",
       "      names[name] = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Create a vector to store the names.\n",
       "  vector<string> namesList;\n",
       "\n",
       "  // Iterate over the map and add the names to the vector.\n",
       "  for (map<string, bool>::iterator it = names.begin(); it != names.end(); ++it) {\n",
       "    namesList.push_back(it->first);\n",
       "  }\n",
       "\n",
       "  // Return the vector of names.\n",
       "  return namesList;\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "language = \"C++ function\"\n",
    "file_format = \"json\"\n",
    "extract_info = \"names\"\n",
    "requirments = \"\"\"\n",
    "              - the name should be start with capital letters.\n",
    "              - There should be no duplicate names in the final list.\n",
    "              \"\"\"\n",
    "\n",
    "prefix = f\"\"\"Create a {language} to parse {file_format} and extract {extract_info} \n",
    "            with the following requirements: {requirments}.\n",
    "        \"\"\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix, max_output_tokens=1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code completion with `code-gecko@001`\n",
    "\n",
    "Code completion uses the code-gecko foundation model to generate and complete code based on code being written. `code-gecko` completes code that was recently typed by a user.\n",
    "\n",
    "Code completion API has few more parameters than code generation.\n",
    "\n",
    "* __prefix__: _required_ : For code models, prefix represents the beginning of a piece of meaningful programming code or a natural language prompt that describes code to be generated.\n",
    "\n",
    "* __suffix__: _optional_ : For code completion, suffix represents the end of a piece of meaningful programming code. The model attempts to fill in the code in between the prefix and suffix.\n",
    "\n",
    "* __temperature__: _required_ : Temperature controls the degree of randomness in token selection. Same as for other models. range: (0.0 - 1.0, default 0)\n",
    "\n",
    "* __maxOutputTokens__: _required_ : Maximum number of tokens that can be generated in the response. __range: (1 - 64, default 64)__\n",
    "\n",
    "* __stopSequences__: _optional_ : Specifies a list of strings that tells the model to stop generating text if one of the strings is encountered in the response. The strings are case-sensitive.\n",
    "\n",
    "To learn more about creating prompts for code completion, see [Create prompts for code completion](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-completion#:~:text=code%20completion%2C%20see-,Create%20prompts%20for%20code%20completion,-.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_completion_model = CodeGenerationModel.from_pretrained(\"code-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "     return string_s.find(x)\n",
       "          \n",
       "          def find_y_in_string(string_s, y):\n",
       "              return string_s.find(y)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "          def find_x_in_string(string_s, x):\n",
    "         \"\"\"\n",
    "\n",
    "response = code_completion_model.predict(prefix=prefix,\n",
    "                                         max_output_tokens=64)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   assert reverse_string(\"\") == \"\"\n",
      "         def test_one_character_string()\n",
      "            assert reverse_string(\"a\") == \"a\"\n",
      "         def test_two_character_string()\n",
      "            assert reverse_string(\"ab\") == \"ba\"\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "         def reverse_string(s):\n",
    "            return s[::-1]\n",
    "         def test_empty_input_string()\n",
    "         \"\"\"\n",
    "\n",
    "response = code_completion_model.predict(prefix=prefix,\n",
    "                                         max_output_tokens=64)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code chat with `codechat-bison@001`\n",
    "\n",
    "The `codechat-bison@001` model lets us have a freeform conversation across multiple turns from a code context. The application tracks what was previously said in the conversation. As such, if we expect to use conversations in our application for code generation, use the `codechat-bison@001` model because it has been fine-tuned for multi-turn conversation use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a function that calculates the minimum of two numbers:\n",
      "\n",
      "```\n",
      "def min(a, b):\n",
      "  \"\"\"\n",
      "  Calculates the minimum of two numbers.\n",
      "\n",
      "  Args:\n",
      "    a: The first number.\n",
      "    b: The second number.\n",
      "\n",
      "  Returns:\n",
      "    The smaller of the two numbers.\n",
      "  \"\"\"\n",
      "\n",
      "  if a < b:\n",
      "    return a\n",
      "  else:\n",
      "    return b\n",
      "```\n",
      "\n",
      "This function takes two numbers as input and returns the smaller of the two numbers. For example, if you call the function with the numbers 5 and 10, it will return 5.\n"
     ]
    }
   ],
   "source": [
    "code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
    "\n",
    "code_chat = code_chat_model.start_chat()\n",
    "\n",
    "\n",
    "print(code_chat.send_message(\n",
    "        \"Please help write a function to calculate the min of two numbers\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the model should respond based on what was previously asked in the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! The function `min` takes two numbers as input, `a` and `b`. It then compares the two numbers and returns the smaller of the two.\n",
      "\n",
      "The first line of the function is a comment. It explains what the function does.\n",
      "\n",
      "The second line of the function is an `if` statement. The `if` statement checks if `a` is less than `b`. If it is, then the function returns `a`. Otherwise, the function returns `b`.\n",
      "\n",
      "For example, if you call the function with the numbers 5 and 10, the `if` statement will be true because 5 is less than 10. So, the function will return 5.\n"
     ]
    }
   ],
   "source": [
    "print(code_chat.send_message(\n",
    "        \"can you explain the code line by as I were 10 year old guy?\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take another example and ask the model to give more general code suggestion for a specific problem that are are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most scalable way to sort a list in Python is to use the built-in `sort()` function. This function takes a list as its argument and sorts it in ascending order. The `sort()` function is very efficient and can sort lists of any size.\n",
      "\n",
      "To use the `sort()` function, simply pass the list you want to sort as its argument. For example, to sort the list `[1, 2, 3, 4, 5]`, you would use the following code:\n",
      "\n",
      "```\n",
      "sorted([1, 2, 3, 4, 5])\n",
      "```\n",
      "\n",
      "The `sort()` function will return a new list that is sorted in ascending order. The original list will not be changed.\n",
      "\n",
      "If you need to sort a list in descending order, you can use the `reverse()` function. The `reverse()` function takes a list as its argument and reverses the order of the elements in the list. To sort the list `[1, 2, 3, 4, 5]` in descending order, you would use the following code:\n",
      "\n",
      "```\n",
      "sorted([1, 2, 3, 4, 5], reverse=True)\n",
      "```\n",
      "\n",
      "The `reverse()` function will return a new list that is sorted in descending order. The original list will not be changed.\n"
     ]
    }
   ],
   "source": [
    "code_chat = code_chat_model.start_chat()\n",
    "\n",
    "print(code_chat.send_message(\n",
    "        \"what is the most scalable way to sort a list in python?\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue to ask follow-up questions to the origianl query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To measure the iteration per second for the following code:\n",
      "\n",
      "```\n",
      "for i in range(1000000):\n",
      "    pass\n",
      "```\n",
      "\n",
      "You can use the following code:\n",
      "\n",
      "```\n",
      "import time\n",
      "\n",
      "start = time.time()\n",
      "for i in range(1000000):\n",
      "    pass\n",
      "end = time.time()\n",
      "\n",
      "print(end - start)\n",
      "```\n",
      "\n",
      "This code will print the time it takes to execute the loop. You can then divide the number of iterations by the time to get the iteration per second.\n"
     ]
    }
   ],
   "source": [
    "print(code_chat.send_message(\n",
    "        \"how would i measure the iteration per second for the following code?\",\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
