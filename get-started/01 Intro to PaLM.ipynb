{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LLMs and PalM?\n",
    "\n",
    "This document provides an overview of large language models (LLMs) and Google's next-generation LLM, PaLM 2. LLMs are deep learning models trained on massive datasets of text. PaLM 2 excels at tasks like advanced reasoning, translation, and code generation. It builds on Google's legacy of breakthrough research in machine learning and responsible AI. LLMs are created using unsupervised learning, where the model learns to predict the next word in a sentence given the preceding words. This enables the model to generate coherent, fluent text resembling human writing. The model's large size allows it to learn complex patterns and relationships in language and generate high-quality text for various applications.\n",
    "\n",
    "### Available Models\n",
    "\n",
    "Vertex AI PaLM API models\n",
    "The Vertex AI PaLM API enables you to test, customize, and deploy instances of Googleâ€™s large language models (LLM) called as PaLM, so that you can leverage the capabilities of PaLM in your applications.\n",
    "\n",
    "Model naming scheme\n",
    "Foundation model names have three components: use case, model size, and version number. The naming convention is in the format:\n",
    "`<use case>-<model size>@<version number>`\n",
    "\n",
    "For example, _text-bison@001_ represents the Bison text model, version _001_.\n",
    "\n",
    "The model sizes are as follows:\n",
    "\n",
    "* __Bison__: The best value in terms of capability and cost.\n",
    "* __Gecko__: The smallest and cheapest model for simple tasks.\n",
    "\n",
    "Available models\n",
    "The Vertex AI PaLM API currently supports five models:\n",
    "\n",
    "* `text-bison@001` : Fine-tuned to follow natural language instructions and is suitable for a variety of language tasks.\n",
    "\n",
    "* `chat-bison@001` : Fine-tuned for multi-turn conversation use cases like building a chatbot.\n",
    "\n",
    "* `textembedding-gecko@001` : Returns model embeddings for text inputs.\n",
    "\n",
    "* `code-bison@001`: A model fine-tuned to generate code based on a natural language description of the desired code. For example, it can generate a unit test for a function.\n",
    "\n",
    "* `code-gecko@001`: A model fine-tuned to suggest code completion based on the context in code that's written.\n",
    "\n",
    "* `codechat-bison@001`: A model fine-tuned for chatbot conversations that help with code-related questions.\n",
    "\n",
    "You can find more information about the properties of these foundational models in the [Generative AI Studio documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import vertexai\n",
    "from IPython.display import Markdown, display\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from vertexai.language_models import TextGenerationModel, \\\n",
    "                                     ChatModel, \\\n",
    "                                     InputOutputTextPair, \\\n",
    "                                     CodeGenerationModel, \\\n",
    "                                     CodeChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate service account (authentication)\n",
    "json_path = '../llm-ai.json' # replace with your own service account\n",
    "credentials = service_account.Credentials.from_service_account_file(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Vertex AI\n",
    "load_dotenv()\n",
    "vertexai.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with `text-bison@001`\n",
    "\n",
    "The text generation model from PaLM API that you will use in this notebook is text-bison@001. It is fine-tuned to follow natural language instructions and is suitable for a variety of language tasks, such as:\n",
    "\n",
    "* Classification\n",
    "* Sentiment analysis\n",
    "* Entity extraction\n",
    "* Extractive question-answering\n",
    "* Summarization\n",
    "* Re-writing text in a different style\n",
    "* Ad copy generation\n",
    "* Concept ideation\n",
    "* Concept simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hello PaLM__\n",
    "\n",
    "Create the first prompt and send it to the text generation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaLM AI (Pathways Language Model) is a large language model from Google AI. It was trained on a massive dataset of text and code, and it can understand and generate human language in a variety of ways. PaLM AI is still under development, but it has already been shown to be capable of impressive feats, such as writing different kinds of creative text, translating languages, answering your questions, and writing different kinds of creative text.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is PaLM AI?\"\n",
    "\n",
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Another Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **A.I.**\n",
       "2. **B.R.A.I.N.**\n",
       "3. **Cognito**\n",
       "4. **Intelli**\n",
       "5. **MindMeld**\n",
       "6. **Nervana**\n",
       "7. **OpenAI**\n",
       "8. **PaLM**\n",
       "9. **Watson**\n",
       "10. **X.AI.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Generate a list of AI company name ideas.\n",
    "A list should be 10 bullet points.\n",
    "Each company name should be 2 words or less.\"\"\" # try your own prompt\n",
    "\n",
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "# return as markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "Prompt templates are useful if you have found a good way to structure your prompt that you can re-use. This can be also be helpful in limiting the open-endedness of freeform prompts. There are many ways to implement prompt templates, and below is just one example using `f-strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* **Meatless Mondays** - A trend where people eat meatless meals on Mondays to reduce their meat consumption.\n",
       "* **Meatless burgers** - A plant-based burger that is made to look and taste like a traditional beef burger.\n",
       "* **Vegan hot dogs** - A plant-based hot dog that is made to look and taste like a traditional pork hot dog.\n",
       "* **Tofu scramble** - A dish made from scrambled tofu that is seasoned to taste like scrambled eggs.\n",
       "* **Chickpea salad** - A salad made from chickpeas, tomatoes, cucumbers, and other vegetables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_ingredient = \"meat\" # try changing this to a different industry\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=f\"\"\"Create a list of trending menus made from {my_ingredient} as main ingredient.\n",
    "    Each trend should be less than 3 words.\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters for `text-bison@001`\n",
    "\n",
    "We can customize how the PaLM API behaves in response to your prompt by using the following parameters for text-bison@001:\n",
    "\n",
    "* `temperature`: higher means more \"creative\" responses\n",
    "* `max_output_tokens`: sets the max number of tokens in the output\n",
    "* `top_p`: higher means it will pull from more possible next tokens, based on cumulative probability\n",
    "* `top_k`: higher means it will sample from more possible next tokens\n",
    "\n",
    "For more detail model parameters, refer to this [documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. The `temperature` parameter (range: 0.0 - 1.0, default 0)\n",
    "\n",
    "The temperature is used for sampling during the response generation, which occurs when top_p and top_k are applied. Temperature controls the degree of randomness in token selection.\n",
    "\n",
    "Lower temperatures are good for prompts that require a more deterministic and less open-ended response. In comparison, higher temperatures can lead to more \"creative\" or diverse results. A temperature of 0 is deterministic: the highest probability response is always selected. For most use cases, try starting with a temperature of 0.2.\n",
    "\n",
    "A higher temperature value will result in a more exploratative output, with a higher likelihood of generating rare or unusual words or phrases. Conversely, a lower temperature value will result in a more conservative output, with a higher likelihood of generating common or expected words or phrases.\n",
    "\n",
    "Example:\n",
    "For example,\n",
    "\n",
    "`temperature = 0.0:`\n",
    "\n",
    "* _The cat sat on the couch, watching the birds outside._\n",
    "* _The cat sat on the windowsill, basking in the sun._\n",
    "\n",
    "\n",
    "`temperature = 0.9:`\n",
    "\n",
    "* _The cat sat on the moon, meowing at the stars._\n",
    "* _The cat sat on the cheeseburger, purring with delight._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 0.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "a park."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_val = 0.0\n",
    "prompt_temperature = \"Complete the sentence: As I walked around in the downtown, I found myself in:\"\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_temperature,\n",
    "    temperature=temp_val,\n",
    ")\n",
    "\n",
    "print(f\"[temperature = {temp_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 1.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "a beautiful cafe with outdoor seating."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_val = 1.0\n",
    "prompt_temperature = \"Complete the sentence: As I walked around in the downtown, I found myself in:\"\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_temperature,\n",
    "    temperature=temp_val,\n",
    ")\n",
    "\n",
    "print(f\"[temperature = {temp_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. The `max_output_tokens` parameter (range: 1 - 1024, default 128)\n",
    "\n",
    "__Tokens__\n",
    "A single token may be smaller than a word. For example, a token is approximately four characters. So 100 tokens correspond to roughly 60-80 words. It's essential to be aware of the token sizes as models have a limit on input and output tokens.\n",
    "\n",
    "__What is max_output_tokens?__\n",
    "max_output_tokens is the maximum number of tokens that can be generated in the response.\n",
    "\n",
    "__How does max_output_tokens affect the response?__\n",
    "Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[max_output_tokens = 5]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **Automat"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_output_tokens_val = 5\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=\"List ten ways that generative AI can help improve the manufacturing industry\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    "\n",
    "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[max_output_tokens = 100]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **Automate product design.** Generative AI can be used to create new product designs, which can save time and money for manufacturers.\n",
       "2. **Optimize manufacturing processes.** Generative AI can be used to optimize manufacturing processes, such as by identifying the most efficient way to assemble a product or by finding ways to reduce waste.\n",
       "3. **Improve quality control.** Generative AI can be used to identify defects in products, which can help manufacturers to improve quality control"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_output_tokens_val = 100\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=\"List ten ways that generative AI can help improve the manufacturing industry\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    "\n",
    "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. The `top_p` parameter (range: 0.0 - 1.0, default 0.95)\n",
    "\n",
    "__What is top_p?__\n",
    "top_p controls how the model selects tokens for output by adjusting the probability distribution of the next word in the generated text based on a cumulative probability cutoff. Specifically, it selects the smallest set of tokens whose cumulative probability exceeds the given cutoff probability p, and samples from this set uniformly.\n",
    "\n",
    "For example, suppose tokens `A`, `B`, and `C` have a probability of `0.3`, `0.2`, and `0.1`, and the top_p value is __0.5__. In that case, the model will select either A or B as the next token (using temperature) and not consider C, because the cumulative probability of top_p is <= 0.5. Specify a lower value for less random responses and a higher value for more random responses.\n",
    "\n",
    "__How does top_p affect the response?__\n",
    "The top_p parameter is used to control the diversity of the generated text. A _higher_ top_p parameter value results in more \"diverse\" and \"interesting\" outputs, with the model being allowed to sample from a larger pool of possibilities. In contrast, a _lower_ top_p parameter value resulted in more predictable outputs, with the model being constrained to a smaller set of possible tokens.\n",
    "\n",
    "Example:\n",
    "\n",
    "`top_p = 0.1:`\n",
    "\n",
    "* _The cat sat on the mat._\n",
    "* _The cat sat on the floor._\n",
    "\n",
    "`top_p = 0.9:`\n",
    "\n",
    "* _The cat sat on the windowsill, soaking up the sun's rays._\n",
    "* _The cat sat on the edge of the bed, watching the birds outside._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 0.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"We need a leader who will take action on climate change. I will fight for a clean energy future and protect our planet for our children.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_p_val = 0.0\n",
    "prompt_top_p_example = (\n",
    "    \"Create a slogan for a candidate focusing on climate change.\"\n",
    ")\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_p_example, temperature=0.9, top_p=top_p_val\n",
    ")\n",
    "\n",
    "print(f\"[top_p = {top_p_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 1.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**\"We need to act now to protect our planet.\"**\n",
       "\n",
       "This slogan is short, simple, and to the point. It gets to the heart of the issue and what the candidate is running for. It also uses strong, emotional language to appeal to voters' sense of urgency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_p_val = 1.0\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_p_example, temperature=0.9, top_p=top_p_val\n",
    ")\n",
    "\n",
    "print(f\"[top_p = {top_p_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. The `top_k` parameter (range: 0.0 - 40, default 40)\n",
    "\n",
    "__What is top_k?__\n",
    "\n",
    "top_k changes how the model selects tokens for output. A top_k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding). In contrast, a top_k of 3 means that the next token is selected from the top 3 most probable tokens (using temperature). For each token selection step, the top_k tokens with the highest probabilities are sampled. Then tokens are further filtered based on top_p with the final token selected using temperature sampling.\n",
    "\n",
    "__How does top_k affect the response?__\n",
    "\n",
    "Specify a lower value for less random responses and a higher value for more random responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_k = 1]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Day 1:\n",
       "* Morning: Start your day with a sunrise yoga session at Uluwatu Temple. This is one of the most popular temples in Bali and offers stunning views of the Indian Ocean.\n",
       "* Afternoon: After yoga, head to the Ubud Monkey Forest. This is a sacred forest that is home to hundreds of monkeys. You can walk through the forest and see the monkeys up close.\n",
       "* Evening: Enjoy a traditional Balinese dinner at a local restaurant. There are many great restaurants in Ubud that serve traditional Balinese cuisine.\n",
       "\n",
       "Day 2:\n",
       "* Morning: Start your day with a visit to the Tanah Lot Temple. This temple is built on a rock formation that is surrounded by the ocean. It is one of the most iconic temples in Bali.\n",
       "* Afternoon: After visiting the temple, head to the Tirta Empul Temple. This temple is dedicated to the god of water and is a popular spot for locals to come and bathe in the holy water.\n",
       "* Evening: Enjoy a sunset cruise on the Balinese Sea. This is a great way to see the sunset and get a different perspective of the island."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_top_k_example = \"Write a 2-day itinerary for Bali.\"\n",
    "top_k_val = 1\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_k_example, \n",
    "    max_output_tokens=300, \n",
    "    temperature=0.9, \n",
    "    top_k=top_k_val\n",
    ")\n",
    "\n",
    "print(f\"[top_k = {top_k_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_k = 40]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Day 1:\n",
       "\n",
       "* Morning: Start your day with a visit to the Ubud Monkey Forest. This sacred forest is home to hundreds of monkeys, and you'll have the chance to see them up close. Be sure to bring some food to feed them!\n",
       "* Afternoon: After lunch, take a walk through the rice fields of Tegalalang. This is a beautiful area, and you'll get some amazing photos of the rice terraces.\n",
       "* Evening: In the evening, enjoy a traditional Balinese dance performance. There are many different types of dances to choose from, so you're sure to find one that you enjoy.\n",
       "\n",
       "Day 2:\n",
       "\n",
       "* Morning: Start your day with a visit to the Tanah Lot Temple. This temple is built on a rock formation that juts out into the sea, and it's a popular spot for sunrise.\n",
       "* Afternoon: After lunch, take a surfing lesson at Kuta Beach. Bali is known for its great surfing, and Kuta Beach is one of the best places to learn.\n",
       "* Evening: In the evening, enjoy a sunset dinner cruise. This is a great way to see the island from a different perspective, and you'll get to enjoy some delicious food and drinks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_k_val = 40\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_k_example,\n",
    "    max_output_tokens=300,\n",
    "    temperature=0.9,\n",
    "    top_k=top_k_val,\n",
    ")\n",
    "\n",
    "print(f\"[top_k = {top_k_val}]\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat model with `chat-bison@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chat-bison@001` model lets you have a freeform conversation across multiple turns. The application tracks what was previously said in the conversation. As such, if you expect to use conversations in your application, use the `chat-bison@001` model because it has been fine-tuned for multi-turn conversation use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chat_model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='## Course Outline\\n\\nThis course is designed to teach Python developers the basics of Golang. It covers the following topics:\\n\\n* Golang syntax\\n* Data types\\n* Control flow statements\\n* Functions\\n* Packages\\n* Modules\\n* Concurrency\\n* Testing\\n* Deployment\\n\\n## Learning Objectives\\n\\nBy the end of this course, you will be able to:\\n\\n* Write simple Golang programs\\n* Use Golang data types and control flow statements\\n* Define and call functions\\n* Use packages and modules\\n* Write concurrent programs\\n* Write unit tests for your Golang programs\\n* Deploy Golang programs to production\\n\\n## Prerequisites\\n\\nThis course assumes that you have a basic understanding of programming. You should be familiar with the following concepts:\\n\\n* Variables\\n* Data types\\n* Control flow statements\\n* Functions\\n* Modules\\n\\n## Resources\\n\\nThe following resources are provided to help you learn Golang:\\n\\n* [The Go Programming Language Book](https://golang.org/doc/book/)\\n* [The Go Programming Language Tutorial](https://golang.org/doc/tutorial/)\\n* [The Go Programming Language Playground](https://play.golang.org/)\\n* [The Go Programming Language Forums](https://golang.org/forum/)\\n* [The Go Programming Language Slack](https://golang.org/slack/)\\n\\n## Schedule\\n\\nThis course is self-paced and can be completed at your own pace. The course material is organized into five modules:\\n\\n* Module 1: Golang Basics\\n* Module 2: Data Types and Control Flow Statements\\n* Module 3: Functions and Packages\\n* Module 4: Concurrency\\n* Module 5: Testing and Deployment\\n\\nEach module contains a series of lessons and exercises. You should complete the lessons and exercises in each module before moving on to the next module.\\n\\n## Assessment\\n\\nThere is no formal assessment for this course. However, you are encouraged to complete the lessons and exercises in each module. You can also submit your code to the Go Playground for feedback.\\n\\n## Conclusion\\n\\nThis course has provided you with a basic introduction to Golang. You should now be able to write simple Golang programs. To learn more about Golang, you can refer to the resources listed in the Resources section.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': '## Course Outline\\n\\nThis course is designed to teach Python developers the basics of Golang. It covers the following topics:\\n\\n* Golang syntax\\n* Data types\\n* Control flow statements\\n* Functions\\n* Packages\\n* Modules\\n* Concurrency\\n* Testing\\n* Deployment\\n\\n## Learning Objectives\\n\\nBy the end of this course, you will be able to:\\n\\n* Write simple Golang programs\\n* Use Golang data types and control flow statements\\n* Define and call functions\\n* Use packages and modules\\n* Write concurrent programs\\n* Write unit tests for your Golang programs\\n* Deploy Golang programs to production\\n\\n## Prerequisites\\n\\nThis course assumes that you have a basic understanding of programming. You should be familiar with the following concepts:\\n\\n* Variables\\n* Data types\\n* Control flow statements\\n* Functions\\n* Modules\\n\\n## Resources\\n\\nThe following resources are provided to help you learn Golang:\\n\\n* [The Go Programming Language Book](https://golang.org/doc/book/)\\n* [The Go Programming Language Tutorial](https://golang.org/doc/tutorial/)\\n* [The Go Programming Language Playground](https://play.golang.org/)\\n* [The Go Programming Language Forums](https://golang.org/forum/)\\n* [The Go Programming Language Slack](https://golang.org/slack/)\\n\\n## Schedule\\n\\nThis course is self-paced and can be completed at your own pace. The course material is organized into five modules:\\n\\n* Module 1: Golang Basics\\n* Module 2: Data Types and Control Flow Statements\\n* Module 3: Functions and Packages\\n* Module 4: Concurrency\\n* Module 5: Testing and Deployment\\n\\nEach module contains a series of lessons and exercises. You should complete the lessons and exercises in each module before moving on to the next module.\\n\\n## Assessment\\n\\nThere is no formal assessment for this course. However, you are encouraged to complete the lessons and exercises in each module. You can also submit your code to the Go Playground for feedback.\\n\\n## Conclusion\\n\\nThis course has provided you with a basic introduction to Golang. You should now be able to write simple Golang programs. To learn more about Golang, you can refer to the resources listed in the Resources section.', 'author': '1'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.0, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Insult', 'Profanity']}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Insult': 0.1, 'Profanity': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[## Course Outline\n",
      "\n",
      "This course is designed to teach Python developers the basics of Golang. It covers the following topics:\n",
      "\n",
      "* Golang syntax\n",
      "* Data types\n",
      "* Control flow statements\n",
      "* Functions\n",
      "* Packages\n",
      "* Modules\n",
      "* Concurrency\n",
      "* Testing\n",
      "* Deployment\n",
      "\n",
      "## Learning Objectives\n",
      "\n",
      "By the end of this course, you will be able to:\n",
      "\n",
      "* Write simple Golang programs\n",
      "* Use Golang data types and control flow statements\n",
      "* Define and call functions\n",
      "* Use packages and modules\n",
      "* Write concurrent programs\n",
      "* Write unit tests for your Golang programs\n",
      "* Deploy Golang programs to production\n",
      "\n",
      "## Prerequisites\n",
      "\n",
      "This course assumes that you have a basic understanding of programming. You should be familiar with the following concepts:\n",
      "\n",
      "* Variables\n",
      "* Data types\n",
      "* Control flow statements\n",
      "* Functions\n",
      "* Modules\n",
      "\n",
      "## Resources\n",
      "\n",
      "The following resources are provided to help you learn Golang:\n",
      "\n",
      "* [The Go Programming Language Book](https://golang.org/doc/book/)\n",
      "* [The Go Programming Language Tutorial](https://golang.org/doc/tutorial/)\n",
      "* [The Go Programming Language Playground](https://play.golang.org/)\n",
      "* [The Go Programming Language Forums](https://golang.org/forum/)\n",
      "* [The Go Programming Language Slack](https://golang.org/slack/)\n",
      "\n",
      "## Schedule\n",
      "\n",
      "This course is self-paced and can be completed at your own pace. The course material is organized into five modules:\n",
      "\n",
      "* Module 1: Golang Basics\n",
      "* Module 2: Data Types and Control Flow Statements\n",
      "* Module 3: Functions and Packages\n",
      "* Module 4: Concurrency\n",
      "* Module 5: Testing and Deployment\n",
      "\n",
      "Each module contains a series of lessons and exercises. You should complete the lessons and exercises in each module before moving on to the next module.\n",
      "\n",
      "## Assessment\n",
      "\n",
      "There is no formal assessment for this course. However, you are encouraged to complete the lessons and exercises in each module. You can also submit your code to the Go Playground for feedback.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This course has provided you with a basic introduction to Golang. You should now be able to write simple Golang programs. To learn more about Golang, you can refer to the resources listed in the Resources section.])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chat.send_message(\n",
    "        \"\"\"\n",
    "Hello! Can you write a course outline to learn Golang for Python developer?\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the model should respond based on what was previously said in the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='Concurrency in Go is a way of executing multiple tasks at the same time. This is done by using goroutines, which are lightweight threads that are managed by the Go runtime. Goroutines are created using the `go` keyword, and they can be executed in parallel or sequentially.\\n\\nTo understand how concurrency works in Go, it is helpful to think of a goroutine as a function that is running in the background. When you create a goroutine, it is immediately started and it will continue to run until it finishes or it is interrupted. Goroutines can communicate with each other by sharing data through channels.\\n\\nOne of the benefits of using concurrency in Go is that it can improve the performance of your programs. This is because goroutines can run in parallel, which means that they can take advantage of multiple CPU cores. Additionally, goroutines are lightweight, so they do not consume a lot of memory.\\n\\nHowever, it is important to use concurrency carefully in Go. If you are not careful, you can create race conditions, which can lead to errors in your program. A race condition occurs when two or more goroutines try to access the same data at the same time. This can cause the data to be corrupted, which can lead to unexpected behavior in your program.\\n\\nTo avoid race conditions, you should use channels to synchronize access to shared data. Channels are a way of passing data between goroutines. When you send data on a channel, the goroutine that is sending the data will block until the goroutine that is receiving the data is ready to receive it. This ensures that the data is not corrupted.\\n\\nConcurrency is a powerful tool that can be used to improve the performance of your Go programs. However, it is important to use concurrency carefully to avoid race conditions.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'Concurrency in Go is a way of executing multiple tasks at the same time. This is done by using goroutines, which are lightweight threads that are managed by the Go runtime. Goroutines are created using the `go` keyword, and they can be executed in parallel or sequentially.\\n\\nTo understand how concurrency works in Go, it is helpful to think of a goroutine as a function that is running in the background. When you create a goroutine, it is immediately started and it will continue to run until it finishes or it is interrupted. Goroutines can communicate with each other by sharing data through channels.\\n\\nOne of the benefits of using concurrency in Go is that it can improve the performance of your programs. This is because goroutines can run in parallel, which means that they can take advantage of multiple CPU cores. Additionally, goroutines are lightweight, so they do not consume a lot of memory.\\n\\nHowever, it is important to use concurrency carefully in Go. If you are not careful, you can create race conditions, which can lead to errors in your program. A race condition occurs when two or more goroutines try to access the same data at the same time. This can cause the data to be corrupted, which can lead to unexpected behavior in your program.\\n\\nTo avoid race conditions, you should use channels to synchronize access to shared data. Channels are a way of passing data between goroutines. When you send data on a channel, the goroutine that is sending the data will block until the goroutine that is receiving the data is ready to receive it. This ensures that the data is not corrupted.\\n\\nConcurrency is a powerful tool that can be used to improve the performance of your Go programs. However, it is important to use concurrency carefully to avoid race conditions.', 'author': 'bot'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.0, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult']}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.2, 'Insult': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[Concurrency in Go is a way of executing multiple tasks at the same time. This is done by using goroutines, which are lightweight threads that are managed by the Go runtime. Goroutines are created using the `go` keyword, and they can be executed in parallel or sequentially.\n",
      "\n",
      "To understand how concurrency works in Go, it is helpful to think of a goroutine as a function that is running in the background. When you create a goroutine, it is immediately started and it will continue to run until it finishes or it is interrupted. Goroutines can communicate with each other by sharing data through channels.\n",
      "\n",
      "One of the benefits of using concurrency in Go is that it can improve the performance of your programs. This is because goroutines can run in parallel, which means that they can take advantage of multiple CPU cores. Additionally, goroutines are lightweight, so they do not consume a lot of memory.\n",
      "\n",
      "However, it is important to use concurrency carefully in Go. If you are not careful, you can create race conditions, which can lead to errors in your program. A race condition occurs when two or more goroutines try to access the same data at the same time. This can cause the data to be corrupted, which can lead to unexpected behavior in your program.\n",
      "\n",
      "To avoid race conditions, you should use channels to synchronize access to shared data. Channels are a way of passing data between goroutines. When you send data on a channel, the goroutine that is sending the data will block until the goroutine that is receiving the data is ready to receive it. This ensures that the data is not corrupted.\n",
      "\n",
      "Concurrency is a powerful tool that can be used to improve the performance of your Go programs. However, it is important to use concurrency carefully to avoid race conditions.])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chat.send_message(\n",
    "        \"\"\"\n",
    "Could you give me short explanation on Concurrency in Go?\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Chat model with the SDK\n",
    "\n",
    "We can also provide a context and examples to the model. The model will then respond based on the provided context and examples. We can also use `temperature`, `max_output_tokens`, `top_p`, and `top_k`. These parameters should be used when we start our chat with `chat_model.start_chat()`.\n",
    "\n",
    "For more information on chat models, please refer to the [documentation on chat model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#chat_model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='Yes, all Marvel Cinematic Universe movies are based on comic books.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'Yes, all Marvel Cinematic Universe movies are based on comic books.', 'author': '1'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'safetyRatings': [{'probabilityScore': 0.0, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.0, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1], 'categories': ['Derogatory', 'Insult']}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Insult': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[Yes, all Marvel Cinematic Universe movies are based on comic books.])\n"
     ]
    }
   ],
   "source": [
    "chat = chat_model.start_chat(\n",
    "    context=\"My name is Tom. You are my personal assistant. My favorite movies are all list in Marvel Cinematic Universe.\",\n",
    "    examples=[\n",
    "        InputOutputTextPair(\n",
    "            input_text=\"Who do you work for?\",\n",
    "            output_text=\"I work for Tom.\",\n",
    "        ),\n",
    "        InputOutputTextPair(\n",
    "            input_text=\"What do I like?\",\n",
    "            output_text=\"Tom likes watching all Marvel Cinematic Universe movies.\",\n",
    "        ),\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=200,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "print(chat.send_message(\"Are my favorite movies based on a comic?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='The first Marvel Cinematic Universe movie was Iron Man in 2008.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'The first Marvel Cinematic Universe movie was Iron Man in 2008.', 'author': 'bot'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'safetyRatings': [{'probabilityScore': 0.0, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.0, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1], 'categories': ['Insult', 'Sexual']}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Insult': 0.1, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[The first Marvel Cinematic Universe movie was Iron Man in 2008.])\n"
     ]
    }
   ],
   "source": [
    "print(chat.send_message(\"When the first movie came out?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='The most popular movie in the Marvel Cinematic Universe is Avengers: Endgame.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'The most popular movie in the Marvel Cinematic Universe is Avengers: Endgame.', 'author': 'bot'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.0, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1], 'categories': ['Insult', 'Sexual']}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Insult': 0.1, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[The most popular movie in the Marvel Cinematic Universe is Avengers: Endgame.])\n"
     ]
    }
   ],
   "source": [
    "print(chat.send_message(\"What is the most popular movie?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='Avengers: Endgame grossed over $2.79 billion worldwide, making it the highest-grossing film of all time.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'Avengers: Endgame grossed over $2.79 billion worldwide, making it the highest-grossing film of all time.', 'author': 'bot'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.0, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.2], 'categories': ['Derogatory', 'Insult', 'Religion & Belief', 'Sexual']}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Insult': 0.1, 'Religion & Belief': 0.1, 'Sexual': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[Avengers: Endgame grossed over $2.79 billion worldwide, making it the highest-grossing film of all time.])\n"
     ]
    }
   ],
   "source": [
    "print(chat.send_message(\"How much is gross revenue for it?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation with `code-bison@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code generation model (Codey) from PaLM API that you will use in this notebook is code-bison@001. It is fine-tuned to follow natural language instructions to generate required code and is suitable for a variety of coding tasks, such as:\n",
    "\n",
    "* writing functions\n",
    "* writing classes\n",
    "* web-apges\n",
    "* unit tests\n",
    "* docstrings\n",
    "* code translations, and many more use-cases.\n",
    "\n",
    "Currently it supports the following languages:\n",
    "\n",
    "* C++\n",
    "* C#\n",
    "* Go\n",
    "* GoogleSQL\n",
    "* Java\n",
    "* JavaScript\n",
    "* Kotlin\n",
    "* PHP\n",
    "* Python\n",
    "* Ruby\n",
    "* Rust\n",
    "* Scala\n",
    "* Swift\n",
    "* TypeScript\n",
    "\n",
    "You can find our more details [here](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generation_model = CodeGenerationModel.from_pretrained(\"code-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model parameters for `code-bison@001`\n",
    "\n",
    "We can customize how the PaLM API code generation behaves in response to our prompt by using the following parameters for `code-bison@001`:\n",
    "\n",
    "* `prefix`: it represents the beginning of a piece of meaningful programming code or a natural language prompt that describes code to be generated.\n",
    "\n",
    "* `temperature`: higher means more \"creative\" code responses. range: (0.0 - 1.0, default 0).\n",
    "\n",
    "* `max_output_tokens`: sets the max number of tokens in the output. range: (1 - 2048, default 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hello Codey, Create some code for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def scrape_website(url):\n",
       "  \"\"\"Scrape the given website and return the results.\n",
       "\n",
       "  Args:\n",
       "    url: The URL of the website to scrape.\n",
       "\n",
       "  Returns:\n",
       "    A list of dictionaries, where each dictionary contains the data for one\n",
       "    item scraped from the website.\n",
       "  \"\"\"\n",
       "\n",
       "  # Get the HTML content of the website.\n",
       "  response = requests.get(url)\n",
       "  response.raise_for_status()\n",
       "  html_content = response.content\n",
       "\n",
       "  # Parse the HTML content into a list of data items.\n",
       "  soup = BeautifulSoup(html_content, 'html.parser')\n",
       "  data_items = soup.find_all('div', class_='data-item')\n",
       "\n",
       "  # Return the list of data items.\n",
       "  return [item.get_text() for item in data_items]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"write a python function to do scrape a website\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it on your own\n",
    "\n",
    "Some examples:\n",
    "\n",
    "* write node js model using mongodb\n",
    "* write python code to validate email address\n",
    "* write a standard SQL function that concatenating 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def sentence_similarity(sentence1, sentence2):\n",
       "\n",
       "    # Step 1: Tokenize the sentences.\n",
       "\n",
       "    sentence1_tokens = sentence1.split()\n",
       "    sentence2_tokens = sentence2.split()\n",
       "\n",
       "    # Step 2: Calculate the intersection of the two sets of tokens.\n",
       "\n",
       "    intersection = set(sentence1_tokens).intersection(sentence2_tokens)\n",
       "\n",
       "    # Step 3: Calculate the union of the two sets of tokens.\n",
       "\n",
       "    union = set(sentence1_tokens).union(sentence2_tokens)\n",
       "\n",
       "    # Step 4: Calculate the similarity score.\n",
       "\n",
       "    similarity_score = len(intersection) / len(union)\n",
       "\n",
       "    # Step 5: Return the similarity score.\n",
       "\n",
       "    return round(similarity_score * 100, 2)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"\"\"write a python function named as \"sentence_similairty\"\\\n",
    "            where it takes two arguments \"sentence1\" and \"sentence2\". \\\n",
    "            It then returns the similarity score in percentage with 2 decimals. \\n\n",
    "          \"\"\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix, max_output_tokens=1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Prompt Template\n",
    "\n",
    "Prompt templates are useful if we have found a good way to structure our prompt that we can re-use. This can be also be helpful in limiting the open-endedness of freeform prompts. There are many ways to implement prompt templates, and below is just one example using `f-strings`. This way we can structure the prompts as per the expected funcationality of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "#include <iostream>\n",
       "#include <fstream>\n",
       "#include <string>\n",
       "#include <vector>\n",
       "#include <map>\n",
       "\n",
       "using namespace std;\n",
       "\n",
       "// This function parses a JSON file and extracts all of the names.\n",
       "// The names are returned in a vector of strings.\n",
       "vector<string> getNamesFromJSON(const string& filename) {\n",
       "  // Open the JSON file.\n",
       "  ifstream file(filename);\n",
       "\n",
       "  // Create a map to store the names.\n",
       "  map<string, bool> names;\n",
       "\n",
       "  // Read the JSON file line by line.\n",
       "  string line;\n",
       "  while (getline(file, line)) {\n",
       "    // Parse the line as JSON.\n",
       "    json j = json::parse(line);\n",
       "\n",
       "    // Get all of the names from the JSON object.\n",
       "    vector<string> names = j.get<vector<string>>();\n",
       "\n",
       "    // Add the names to the map.\n",
       "    for (string name : names) {\n",
       "      names[name] = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Get all of the names from the map.\n",
       "  vector<string> namesList;\n",
       "  for (string name : names) {\n",
       "    namesList.push_back(name);\n",
       "  }\n",
       "\n",
       "  // Return the list of names.\n",
       "  return namesList;\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "language = \"C++ function\"\n",
    "file_format = \"json\"\n",
    "extract_info = \"names\"\n",
    "requirments = \"\"\"\n",
    "              - the name should be start with capital letters.\n",
    "              - There should be no duplicate names in the final list.\n",
    "              \"\"\"\n",
    "\n",
    "prefix = f\"\"\"Create a {language} to parse {file_format} and extract {extract_info} \n",
    "            with the following requirements: {requirments}.\n",
    "        \"\"\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix, max_output_tokens=1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code completion with `code-gecko@001`\n",
    "\n",
    "Code completion uses the code-gecko foundation model to generate and complete code based on code being written. `code-gecko` completes code that was recently typed by a user.\n",
    "\n",
    "Code completion API has few more parameters than code generation.\n",
    "\n",
    "* __prefix__: _required_ : For code models, prefix represents the beginning of a piece of meaningful programming code or a natural language prompt that describes code to be generated.\n",
    "\n",
    "* __suffix__: _optional_ : For code completion, suffix represents the end of a piece of meaningful programming code. The model attempts to fill in the code in between the prefix and suffix.\n",
    "\n",
    "* __temperature__: _required_ : Temperature controls the degree of randomness in token selection. Same as for other models. range: (0.0 - 1.0, default 0)\n",
    "\n",
    "* __maxOutputTokens__: _required_ : Maximum number of tokens that can be generated in the response. __range: (1 - 64, default 64)__\n",
    "\n",
    "* __stopSequences__: _optional_ : Specifies a list of strings that tells the model to stop generating text if one of the strings is encountered in the response. The strings are case-sensitive.\n",
    "\n",
    "To learn more about creating prompts for code completion, see [Create prompts for code completion](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-completion#:~:text=code%20completion%2C%20see-,Create%20prompts%20for%20code%20completion,-.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_completion_model = CodeGenerationModel.from_pretrained(\"code-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "     return string_s.find(x)\n",
       "\n",
       "          def find_y_in_string(string_s, y):\n",
       "              return string_s.find(y)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "          def find_x_in_string(string_s, x):\n",
    "         \"\"\"\n",
    "\n",
    "response = code_completion_model.predict(prefix=prefix,\n",
    "                                         max_output_tokens=64)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   assert reverse_string(\"\") == \"\"\n",
      "         def test_one_character_string()\n",
      "            assert reverse_string(\"a\") == \"a\"\n",
      "         def test_two_character_string()\n",
      "            assert reverse_string(\"ab\") == \"ba\"\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "         def reverse_string(s):\n",
    "            return s[::-1]\n",
    "         def test_empty_input_string()\n",
    "         \"\"\"\n",
    "\n",
    "response = code_completion_model.predict(prefix=prefix,\n",
    "                                         max_output_tokens=64)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code chat with `codechat-bison@001`\n",
    "\n",
    "The `codechat-bison@001` model lets us have a freeform conversation across multiple turns from a code context. The application tracks what was previously said in the conversation. As such, if we expect to use conversations in our application for code generation, use the `codechat-bison@001` model because it has been fine-tuned for multi-turn conversation use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='Sure, here is a function that calculates the minimum of two numbers:\\n\\n```\\ndef min(a, b):\\n  \"\"\"\\n  Calculates the minimum of two numbers.\\n\\n  Args:\\n    a: The first number.\\n    b: The second number.\\n\\n  Returns:\\n    The smaller of the two numbers.\\n  \"\"\"\\n\\n  if a < b:\\n    return a\\n  else:\\n    return b\\n```\\n\\nThis function takes two numbers as input and returns the smaller of the two numbers. For example, if you call the function with the numbers 5 and 10, it will return 5.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'Sure, here is a function that calculates the minimum of two numbers:\\n\\n```\\ndef min(a, b):\\n  \"\"\"\\n  Calculates the minimum of two numbers.\\n\\n  Args:\\n    a: The first number.\\n    b: The second number.\\n\\n  Returns:\\n    The smaller of the two numbers.\\n  \"\"\"\\n\\n  if a < b:\\n    return a\\n  else:\\n    return b\\n```\\n\\nThis function takes two numbers as input and returns the smaller of the two numbers. For example, if you call the function with the numbers 5 and 10, it will return 5.', 'author': '1'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'scores': [], 'categories': []}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[Sure, here is a function that calculates the minimum of two numbers:\n",
      "\n",
      "```\n",
      "def min(a, b):\n",
      "  \"\"\"\n",
      "  Calculates the minimum of two numbers.\n",
      "\n",
      "  Args:\n",
      "    a: The first number.\n",
      "    b: The second number.\n",
      "\n",
      "  Returns:\n",
      "    The smaller of the two numbers.\n",
      "  \"\"\"\n",
      "\n",
      "  if a < b:\n",
      "    return a\n",
      "  else:\n",
      "    return b\n",
      "```\n",
      "\n",
      "This function takes two numbers as input and returns the smaller of the two numbers. For example, if you call the function with the numbers 5 and 10, it will return 5.])\n"
     ]
    }
   ],
   "source": [
    "code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
    "\n",
    "code_chat = code_chat_model.start_chat()\n",
    "\n",
    "\n",
    "print(code_chat.send_message(\n",
    "        \"Please help write a function to calculate the min of two numbers\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the model should respond based on what was previously asked in the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='Sure! The first line of the function is called a docstring. It tells you what the function does. The next line defines the two arguments that the function takes: `a` and `b`. These are the two numbers that you want to find the minimum of.\\n\\nThe next line is the `if` statement. This statement checks if `a` is less than `b`. If it is, then the function returns `a`. Otherwise, it returns `b`.\\n\\nSo, if you call the function with the numbers 5 and 10, the `if` statement will be true because 5 is less than 10. So, the function will return 5.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'Sure! The first line of the function is called a docstring. It tells you what the function does. The next line defines the two arguments that the function takes: `a` and `b`. These are the two numbers that you want to find the minimum of.\\n\\nThe next line is the `if` statement. This statement checks if `a` is less than `b`. If it is, then the function returns `a`. Otherwise, it returns `b`.\\n\\nSo, if you call the function with the numbers 5 and 10, the `if` statement will be true because 5 is less than 10. So, the function will return 5.', 'author': 'bot'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'scores': [0.1], 'categories': ['Health']}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Health': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[Sure! The first line of the function is called a docstring. It tells you what the function does. The next line defines the two arguments that the function takes: `a` and `b`. These are the two numbers that you want to find the minimum of.\n",
      "\n",
      "The next line is the `if` statement. This statement checks if `a` is less than `b`. If it is, then the function returns `a`. Otherwise, it returns `b`.\n",
      "\n",
      "So, if you call the function with the numbers 5 and 10, the `if` statement will be true because 5 is less than 10. So, the function will return 5.])\n"
     ]
    }
   ],
   "source": [
    "print(code_chat.send_message(\n",
    "        \"can you explain the code line by as I were 10 year old guy?\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take another example and ask the model to give more general code suggestion for a specific problem that are are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='The most scalable way to sort a list in Python is to use the built-in `sort()` function. This function takes a list as its argument and sorts it in ascending order. The `sort()` function is very efficient and can sort lists of any size.\\n\\nTo use the `sort()` function, simply pass the list you want to sort as its argument. For example, to sort the list `[1, 2, 3, 4, 5]`, you would use the following code:\\n\\n```\\nsorted([1, 2, 3, 4, 5])\\n```\\n\\nThe `sort()` function will return a new list that is sorted in ascending order. The original list will not be changed.\\n\\nIf you need to sort a list in descending order, you can use the `reverse()` function. The `reverse()` function takes a list as its argument and reverses the order of the elements in the list. To sort a list in descending order, you would use the following code:\\n\\n```\\nsorted([1, 2, 3, 4, 5], reverse=True)\\n```\\n\\nThe `reverse()` function will return a new list that is sorted in descending order. The original list will not be changed.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'The most scalable way to sort a list in Python is to use the built-in `sort()` function. This function takes a list as its argument and sorts it in ascending order. The `sort()` function is very efficient and can sort lists of any size.\\n\\nTo use the `sort()` function, simply pass the list you want to sort as its argument. For example, to sort the list `[1, 2, 3, 4, 5]`, you would use the following code:\\n\\n```\\nsorted([1, 2, 3, 4, 5])\\n```\\n\\nThe `sort()` function will return a new list that is sorted in ascending order. The original list will not be changed.\\n\\nIf you need to sort a list in descending order, you can use the `reverse()` function. The `reverse()` function takes a list as its argument and reverses the order of the elements in the list. To sort a list in descending order, you would use the following code:\\n\\n```\\nsorted([1, 2, 3, 4, 5], reverse=True)\\n```\\n\\nThe `reverse()` function will return a new list that is sorted in descending order. The original list will not be changed.', 'author': '1'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'scores': [], 'categories': []}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[The most scalable way to sort a list in Python is to use the built-in `sort()` function. This function takes a list as its argument and sorts it in ascending order. The `sort()` function is very efficient and can sort lists of any size.\n",
      "\n",
      "To use the `sort()` function, simply pass the list you want to sort as its argument. For example, to sort the list `[1, 2, 3, 4, 5]`, you would use the following code:\n",
      "\n",
      "```\n",
      "sorted([1, 2, 3, 4, 5])\n",
      "```\n",
      "\n",
      "The `sort()` function will return a new list that is sorted in ascending order. The original list will not be changed.\n",
      "\n",
      "If you need to sort a list in descending order, you can use the `reverse()` function. The `reverse()` function takes a list as its argument and reverses the order of the elements in the list. To sort a list in descending order, you would use the following code:\n",
      "\n",
      "```\n",
      "sorted([1, 2, 3, 4, 5], reverse=True)\n",
      "```\n",
      "\n",
      "The `reverse()` function will return a new list that is sorted in descending order. The original list will not be changed.])\n"
     ]
    }
   ],
   "source": [
    "code_chat = code_chat_model.start_chat()\n",
    "\n",
    "print(code_chat.send_message(\n",
    "        \"what is the most scalable way to sort a list in python?\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue to ask follow-up questions to the origianl query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCandidateTextGenerationResponse(text='To measure the iteration per second for the following code, you can use the following steps:\\n\\n1. Create a timer object.\\n2. Start the timer.\\n3. Execute the code.\\n4. Stop the timer.\\n5. Calculate the number of iterations per second.\\n\\nHere is an example of how to do this in Python:\\n\\n```\\nimport time\\n\\n# Create a timer object.\\ntimer = time.time()\\n\\n# Start the timer.\\nstart = time.time()\\n\\n# Execute the code.\\nfor i in range(1000000):\\n    pass\\n\\n# Stop the timer.\\nend = time.time()\\n\\n# Calculate the number of iterations per second.\\niterations_per_second = 1000000 / (end - start)\\n\\nprint(iterations_per_second)\\n```\\n\\nThis code will print the number of iterations per second that the code was executed.', _prediction_response=Prediction(predictions=[{'candidates': [{'content': 'To measure the iteration per second for the following code, you can use the following steps:\\n\\n1. Create a timer object.\\n2. Start the timer.\\n3. Execute the code.\\n4. Stop the timer.\\n5. Calculate the number of iterations per second.\\n\\nHere is an example of how to do this in Python:\\n\\n```\\nimport time\\n\\n# Create a timer object.\\ntimer = time.time()\\n\\n# Start the timer.\\nstart = time.time()\\n\\n# Execute the code.\\nfor i in range(1000000):\\n    pass\\n\\n# Stop the timer.\\nend = time.time()\\n\\n# Calculate the number of iterations per second.\\niterations_per_second = 1000000 / (end - start)\\n\\nprint(iterations_per_second)\\n```\\n\\nThis code will print the number of iterations per second that the code was executed.', 'author': 'bot'}], 'groundingMetadata': [{}], 'citationMetadata': [{'citations': []}], 'safetyAttributes': [{'blocked': False, 'scores': [], 'categories': []}]}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[To measure the iteration per second for the following code, you can use the following steps:\n",
      "\n",
      "1. Create a timer object.\n",
      "2. Start the timer.\n",
      "3. Execute the code.\n",
      "4. Stop the timer.\n",
      "5. Calculate the number of iterations per second.\n",
      "\n",
      "Here is an example of how to do this in Python:\n",
      "\n",
      "```\n",
      "import time\n",
      "\n",
      "# Create a timer object.\n",
      "timer = time.time()\n",
      "\n",
      "# Start the timer.\n",
      "start = time.time()\n",
      "\n",
      "# Execute the code.\n",
      "for i in range(1000000):\n",
      "    pass\n",
      "\n",
      "# Stop the timer.\n",
      "end = time.time()\n",
      "\n",
      "# Calculate the number of iterations per second.\n",
      "iterations_per_second = 1000000 / (end - start)\n",
      "\n",
      "print(iterations_per_second)\n",
      "```\n",
      "\n",
      "This code will print the number of iterations per second that the code was executed.])\n"
     ]
    }
   ],
   "source": [
    "print(code_chat.send_message(\n",
    "        \"how would i measure the iteration per second for the following code?\",\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
