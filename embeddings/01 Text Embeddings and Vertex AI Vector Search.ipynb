{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we learn how to use Google Cloud AI tools to quickly bring the power of Large Language Models to enterprise systems.\n",
    "\n",
    "This tutorial covers the following -\n",
    "\n",
    "- What are embeddings - what business challenges do they help solve ?\n",
    "- Understanding Text with Vertex AI Text Embeddings\n",
    "- Find Embeddings fast with Vertex AI Vector Search\n",
    "- Grounding LLM outputs with Vector Search\n",
    "\n",
    "This tutorial is based on [the blog post](https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings), combined with sample code.\n",
    "\n",
    "\n",
    "## Bringing Gen AI and LLMs to production services\n",
    "Many people are now starting to think about how to bring Gen AI and LLMs to production services, and facing with several challenges.\n",
    "\n",
    "* \"How to integrate LLMs or AI chatbots with existing IT systems, databases and business data?\"\n",
    "* \"We have thousands of products. How can I let LLM memorize them all precisely?\"\n",
    "* \"How to handle the hallucination issues in AI chatbots to build a reliable service?\"\n",
    "\n",
    "Here is a quick solution: **grounding** with **embeddings** and **vector search**.\n",
    "\n",
    "What is grounding? What are embedding and vector search? In this tutorial, we will learn these crucial concepts to build reliable Gen AI services for enterprise use. But before we dive deeper, let's try the demo below.\n",
    "\n",
    "![Search Gif](../assets/embeddings/search.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try the Stack Overflow semantic search demo (like the gif ebove):\n",
    "\n",
    "This demo is available as a [public live demo](https://ai-demos.dev/). Select \"STACKOVERFLOW\" and enter any coding question as a query, so it runs a text search on 8 million questions posted on Stack Overflow. Try the text semantic search with some queries like 'How to shuffle rows in SQL?' or arbitrary programming questions.\n",
    "\n",
    "In this tutorial, we are going to see how to build a similar search experience - what is involved in building solutions like this using `Vertex AI Embeddings API` and `Vector Search`.\n",
    "\n",
    "\n",
    "## 1. What is Embeddings?\n",
    "With the rise of LLMs, why is it becoming important for IT engineers and ITDMs to understand how they work?\n",
    "\n",
    "In traditional IT systems, most data is organized as structured or tabular data, using simple keywords, labels, and categories in databases and search engines.\n",
    "\n",
    "![Traditional search](../assets/embeddings/search-traditional.png)\n",
    "\n",
    "In contrast, AI-powered services arrange data into a simple data structure known as `embeddings`.\n",
    "\n",
    "![Embedding search](../assets/embeddings/search-embedding.png)\n",
    "\n",
    "Once trained with specific content like text, images, or any content, AI creates a space called \"embedding space\", which is essentially a map of the content's meaning.\n",
    "\n",
    "![Embedding Space 1](../assets/embeddings/embedding-space1.png)\n",
    "\n",
    "AI can identify the location of each content on the map, that's what embedding is.\n",
    "\n",
    "![Embedding Space 2](../assets/embeddings/embedding-space2.png)\n",
    "\n",
    "Let's take an example where a text discusses _movies, music, and actors_, with a distribution of 10%, 2%, and 30%, respectively. In this case, the AI can create an embedding with three values: 0.1, 0.02, and 0.3, in 3 dimensional space.\n",
    "\n",
    "![Embedding Space 3](../assets/embeddings/embedding-space3.png)\n",
    "\n",
    "AI can put content with similar meanings closely together in the space.\n",
    "\n",
    "This is how Google organizes data across various services like Google Search, YouTube, Play, and many others, to provide search results and recommendations with relevant content.\n",
    "\n",
    "Embeddings can also be used to represent different types of things in businesses, such as products, users, user activities, conversations, music & videos, signals from IoT sensors, and so on.\n",
    "\n",
    "AI and Embeddings are now playing a crucial role in creating a new way of human-computer interaction.\n",
    "\n",
    "![Content Embedding Overview](../assets/embeddings/content-embedding-overview.png)\n",
    "\n",
    "AI organizes data into embeddings, which represent what the user is looking for, the meaning of contents, or many other things you have in your business. This creates a new level of user experience that is becoming the new standard.\n",
    "\n",
    "To learn more about embeddings, [Foundational courses: Embeddings on Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) and [Meet AI’s multitool: Vector embeddings by Dale Markowitz](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings) are great materials.\n",
    "\n",
    "## Vertex AI Embeddings for Text\n",
    "\n",
    "With the [Vertex AI Embeddings for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), you can easily create a text embedding with LLM. The product is also available on [Vertex AI Model Garden](https://cloud.google.com/model-garden)\n",
    "\n",
    "![Text Embedding Model Garden](../assets/embeddings/text-embedding-model-garden.png)\n",
    "\n",
    "This API is designed to extract embeddings from texts. It can take text input up to 3,072 input tokens, and outputs 768 dimensional text embeddings.\n",
    "\n",
    "#### LLM text embedding business use cases\n",
    "With the embedding API, you can apply the innovation of embeddings, combined with the LLM capability, to various text processing tasks, such as:\n",
    "\n",
    "* **LLM-enabled Semantic Search**: text embeddings can be used to represent both the meaning and intent of a user's query and documents in the * embedding space. Documents that have similar meaning to the user's query intent will be found fast with vector search technology. The model is capable of generating text embeddings that capture the subtle nuances of each sentence and paragraphs in the document.\n",
    "\n",
    "* **LLM-enabled Text Classification**: LLM text embeddings can be used for text classification with a deep understanding of different contexts without any training or fine-tuning (so-called zero-shot learning). This wasn't possible with the past language models without task-specific training.\n",
    "\n",
    "* **LLM-enabled Recommendation**: The text embedding can be used for recommendation systems as a strong feature for training recommendation models such as Two-Tower model. The model learns the relationship between the query and candidate embeddings, resulting in next-gen user experience with semantic product recommendation.\n",
    "\n",
    "LLM-enabled Clustering, Anomaly Detection, Sentiment Analysis, and more, can be also handled with the LLM-level deep semantics understanding.\n",
    "\n",
    "#### Sorting 8 million texts at \"librarian-level\" precision\n",
    "\n",
    "Vertex AI Embeddings for Text has an embedding space with `768 dimensions`. As explained earlier, the space represents a huge map of a wide variety of texts in the world, organized by their meanings. With each input text, the model can find a location (embedding) in the map.\n",
    "\n",
    "By visualizing the embedding space, you can actually observe how the model sorts the texts at the \"librarian-level\" precision.\n",
    "\n",
    "**Exercise: Try the Nomic AI Atlas**\n",
    "\n",
    "[Nomic AI](http://nomic.ai/) provides a platform called Atlas for storing, visualizing and interacting with embedding spaces with high scalability and in a smooth UI, and they worked with Google for visualizing the embedding space of the 8 million Stack Overflow questions. You can try exploring around the space, zooming in and out to each data point on your browser on this page, courtesy of Nomic AI.\n",
    "\n",
    "The embedding space represents a huge map of texts, organized by their meanings With each input text, the model can find a location (embedding) in the map Like a librarian reading through millions of texts, sorting them with millions of nano-categories\n",
    "\n",
    "Try exploring it [here](https://atlas.nomic.ai/map/edaff028-12b5-42a0-8e8b-6430c9b8222b/bcb42818-3581-4fb5-ac30-9883d01f98ec). Zoom into a few categories, point each dots, and see how the LLM is sorting similar questions close together in the space.\n",
    "\n",
    "Nomic AI provides a platform called Atlas for storing, visualizing and interacting with embedding spaces with high scalability and in a smooth UI, and they worked with Google for visualizing the embedding space of the 8 million Stack Overflow questions. You can try exploring around the space, zooming in and out to each data point on your browser on this page, courtesy of Nomic AI.\n",
    "\n",
    "The embedding space represents a huge map of texts, organized by their meanings With each input text, the model can find a location (embedding) in the map Like a librarian reading through millions of texts, sorting them with millions of nano-categories\n",
    "\n",
    "Try exploring it here. Zoom into a few categories, point each dots, and see how the LLM is sorting similar questions close together in the space.\n",
    "\n",
    "\n",
    "##### The librarian-level semantic understanding\n",
    "\n",
    "Here are the examples of the librarian-level semantic understanding by Embeddings API with Stack Overflow questions.\n",
    "\n",
    "![StackOverflow](../assets/embeddings/stackoverflow-embedding.png)\n",
    "\n",
    "For example, the model thinks the question _“Does moving the request line to a header frame require an app change?”_ is similar to the question _“Does an application developed on HTTP1x require modifications to run on HTTP2?”_. That is because The model knows both questions talk about what's the change required to support the HTTP2 header frame.\n",
    "\n",
    "Note that this demo didn't require any training or fine-tuning with computer programming specific datasets. This is the innovative part of the zero-shot learning capability of the LLM. It can be applied to a wide variety of industries, including finance, healthcare, retail, manufacturing, construction, media, and more, for deep semantic search on the industry-focused business documents without spending time and cost for collecting industry specific datasets and training models.\n",
    "\n",
    "\n",
    "## Text Embeddings in Action\n",
    "\n",
    "Lets try using Text Embeddings in action with actual sample code.\n",
    "\n",
    "### 1. Setup the Environment\n",
    "\n",
    "Before get started with the Vertex AI services, we need to setup the following.\n",
    "\n",
    "* Install Python SDK\n",
    "* Environment variables\n",
    "* Authentication using Service Account\n",
    "* Enable APIs\n",
    "* Set IAM permissions (Vertex AI User, BigQuery User and Storage Admin)\n",
    "* Install Python SDK\n",
    "\n",
    "```bash\n",
    "!pip install --upgrade --user google-cloud-aiplatform google-cloud-storage google-cloud-bigquery[pandas]\n",
    "```\n",
    "\n",
    "Vertex AI, Cloud Storage and BigQuery APIs can be accessed with multiple ways including REST API and Python SDK. In this tutorial we will use the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import vertexai\n",
    "from IPython.display import Markdown, display\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate service account (authentication)\n",
    "json_path = '../llm-ai.json' # replace with your own service account\n",
    "credentials = service_account.Credentials.from_service_account_file(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Vertex AI\n",
    "load_dotenv()\n",
    "vertexai.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01301158\n"
     ]
    }
   ],
   "source": [
    "# generate an unique id for this session\n",
    "from datetime import datetime\n",
    "\n",
    "UID = datetime.now().strftime(\"%m%d%H%M\")\n",
    "\n",
    "print(UID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Getting Started with Vertex AI Embeddings for Text\n",
    "\n",
    "Now it's ready to get started with embeddings!\n",
    "\n",
    "##### A. Data Preparation\n",
    "\n",
    "We will be using the [Stack Overflow public dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow) hosted on BigQuery table `bigquery-public-data.stackoverflow.posts_questions`. This is a very big dataset with 23 million rows that doesn't fit into the memory. We are going to limit it to 1000 rows for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73250763</td>\n",
       "      <td>Error CS0246: The type or namespace name 'Stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73206525</td>\n",
       "      <td>Keycloak 19.0 behind nginx (https) admin conso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73475664</td>\n",
       "      <td>Citing Institutional Author or Organization in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73399777</td>\n",
       "      <td>Azure build failing due to Method not found: '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73426773</td>\n",
       "      <td>UnboundLocalError: local variable 'raw_labels'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title\n",
       "0  73250763  Error CS0246: The type or namespace name 'Stre...\n",
       "1  73206525  Keycloak 19.0 behind nginx (https) admin conso...\n",
       "2  73475664  Citing Institutional Author or Organization in...\n",
       "3  73399777  Azure build failing due to Method not found: '...\n",
       "4  73426773  UnboundLocalError: local variable 'raw_labels'..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the BQ Table into a Pandas Dataframe\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "QUESTIONS_SIZE = 1000\n",
    "\n",
    "bq_client = bigquery.Client(project=os.environ[\"PROJECT_ID\"], credentials=credentials)\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "        SELECT distinct q.id, q.title\n",
    "        FROM (SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions`\n",
    "        where Score > 0 ORDER BY View_Count desc) AS q\n",
    "        LIMIT {limit} ;\n",
    "        \"\"\"\n",
    "query = QUERY_TEMPLATE.format(limit=QUESTIONS_SIZE)\n",
    "query_job = bq_client.query(query)\n",
    "rows = query_job.result()\n",
    "df = rows.to_dataframe()\n",
    "\n",
    "# examine the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. Call the API to generate embeddings\n",
    "\n",
    "With the Stack Overflow dataset, we will use the title column (the question title) and generate embedding for it with Embeddings for Text API. The API is available under the [`vertexai`](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai) package of the SDK.\n",
    "\n",
    "From the package, import `TextEmbeddingModel` and get a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text embeddings model\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use `textembedding-gecko@001` model for getting text embeddings. Please take a look at [Supported models](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings#supported_models) on the doc to see the list of supported models.\n",
    "\n",
    "Once you get the model, you can call its [get_embeddings](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models.TextEmbeddingModel#vertexai_language_models_TextEmbeddingModel_get_embeddings) function to get embeddings. You can pass up to 5 texts at once in a call. But there is a caveat. By default, the text embeddings API has a \"request per minute\" quota set to `60` for **new Cloud projects** and `600` for **projects with usage history** (see [Quotas and limits](https://cloud.google.com/vertex-ai/docs/quotas#request_quotas) to check the latest quota value for `base_model:textembedding-gecko`). So, rather than using the function directly, you may want to define a wrapper like below to:\n",
    "* limit under 10 calls per second\n",
    "* pass 5 texts each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm  # to show a progress bar\n",
    "\n",
    "# get embeddings for a list of texts\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "\n",
    "def get_embeddings_wrapper(texts):\n",
    "    embs = []\n",
    "    for i in tqdm.tqdm(range(0, len(texts), BATCH_SIZE)):\n",
    "        time.sleep(1)  # to avoid the quota error\n",
    "        result = model.get_embeddings(texts[i : i + BATCH_SIZE])\n",
    "        embs = embs + [e.values for e in result]\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will get embedding for the question titles and add them as a new column embedding to the `DataFrame`. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:06<00:00,  1.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73250763</td>\n",
       "      <td>Error CS0246: The type or namespace name 'Stre...</td>\n",
       "      <td>[-0.02774483524262905, 0.010048817843198776, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73206525</td>\n",
       "      <td>Keycloak 19.0 behind nginx (https) admin conso...</td>\n",
       "      <td>[-0.0495707169175148, 0.021820401772856712, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73475664</td>\n",
       "      <td>Citing Institutional Author or Organization in...</td>\n",
       "      <td>[-0.007552702911198139, -0.008695865981280804,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73399777</td>\n",
       "      <td>Azure build failing due to Method not found: '...</td>\n",
       "      <td>[-0.0029502741526812315, -0.025221263989806175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73426773</td>\n",
       "      <td>UnboundLocalError: local variable 'raw_labels'...</td>\n",
       "      <td>[0.005036971066147089, 0.03410046175122261, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  73250763  Error CS0246: The type or namespace name 'Stre...   \n",
       "1  73206525  Keycloak 19.0 behind nginx (https) admin conso...   \n",
       "2  73475664  Citing Institutional Author or Organization in...   \n",
       "3  73399777  Azure build failing due to Method not found: '...   \n",
       "4  73426773  UnboundLocalError: local variable 'raw_labels'...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.02774483524262905, 0.010048817843198776, -...  \n",
       "1  [-0.0495707169175148, 0.021820401772856712, 0....  \n",
       "2  [-0.007552702911198139, -0.008695865981280804,...  \n",
       "3  [-0.0029502741526812315, -0.025221263989806175...  \n",
       "4  [0.005036971066147089, 0.03410046175122261, 0....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get embeddings for the question titles and add them as \"embedding\" column\n",
    "df = df.assign(embedding=get_embeddings_wrapper(list(df.title)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Look at the embedding similarities\n",
    "\n",
    "Let's see how these embeddings are organized in the embedding space with their meanings by quickly calculating the similarities between them and sorting them.\n",
    "\n",
    "As embeddings are vectors, you can calculate similarity between two embeddings by using one of the popular metrics like the followings:\n",
    "\n",
    "![Embedding Similarity](../assets/embeddings/embedding-similarity.png)\n",
    "\n",
    "Which metric should we use? Usually it depends on how each model is trained. In case of the model `textembedding-gecko@001`, we need to use **inner product (dot product)**.\n",
    "\n",
    "In the following code, it picks up one question randomly and uses the numpy `np.dot` function to calculate the similarities between the question and other questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54679927, 0.499551  , 0.48490172, 0.59063316, 0.61370155])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# pick one of them as a key question\n",
    "key = random.randint(0, len(df))\n",
    "\n",
    "# calc dot product between the key and other questions\n",
    "embs = np.array(df.embedding.to_list())\n",
    "similarities = np.dot(embs[key], embs.T)\n",
    "\n",
    "# print similarities for the first 5 questions\n",
    "similarities[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sort the questions with the similarities and print the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key question: Mat Select option cant get data\n",
      "\n",
      "1.0000 Mat Select option cant get data\n",
      "0.7215 Mirth: Mapper: Cannot get data to map to variables\n",
      "0.6698 Why should I use the the .selectable modifier instead of .clickable for select one item in Compose LazyColumn?\n",
      "0.6595 Getting Value error when using DecisionBoundaryDisplay\n",
      "0.6580 Option to exclude patterns from auto import not working?\n",
      "0.6562 jQuery - select function is not working on mobile\n",
      "0.6560 Cannot read properties of undefined 'on'\n",
      "0.6461 Unable to use contains for quick search in Nuxeo\n",
      "0.6426 Error: Clickable element \"Save\" was not found by text|CSS|XPath\n",
      "0.6393 Why do I encounter \"INVALID_PARAMETER VALUE\" error when opening \"Models\" tab in MLFlow UI?\n",
      "0.6390 loadTs is not a function\n",
      "0.6335 How to get data from list of map to display in Recordable list view\n",
      "0.6324 How to show the information of the selected row with DataTable React.js\n",
      "0.6323 Is there a way to enable/disable subsets of data taken by a formula?\n",
      "0.6293 Want to retrieve random value from the response\n",
      "0.6275 what is the best way to select hundred of parameters with spring data\n",
      "0.6266 How to take Nested list as an input.?\n",
      "0.6259 TypeORMError: Entity metadata for x#y was not found\n",
      "0.6247 Option configuration-cache doesn't accept value 'true'. Possible values are [OFF, ON, WARN]\n",
      "0.6238 Strapi 2FA availability\n"
     ]
    }
   ],
   "source": [
    "# print the question\n",
    "print(f\"Key question: {df.title[key]}\\n\")\n",
    "\n",
    "# sort and print the questions by similarities\n",
    "sorted_questions = sorted(\n",
    "    zip(df.title, similarities), key=lambda x: x[1], reverse=True\n",
    ")[:20]\n",
    "\n",
    "for i, (question, similarity) in enumerate(sorted_questions):\n",
    "    print(f\"{similarity:.4f} {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Find embeddings fast with Vertex AI Vector Search\n",
    "\n",
    "As we have explained above, you can find similar embeddings by calculating the distance or similarity between the embeddings.\n",
    "\n",
    "But this isn't easy when you have millions or billions of embeddings. For example, if you have 1 million embeddings with 768 dimensions, you need to repeat the distance calculations for 1 million x 768 times. This would take some seconds - too slow.\n",
    "\n",
    "So the researchers have been studying a technique called [Approximate Nearest Neighbor (ANN)](https://en.wikipedia.org/wiki/Nearest_neighbor_search) for faster search. ANN uses \"vector quantization\" for separating the space into multiple spaces with a tree structure. This is similar to the index in relational databases for improving the query performance, enabling very fast and scalable search with billions of embeddings.\n",
    "\n",
    "With the rise of LLMs, the ANN is getting popular quite rapidly, known as the Vector Search technology.\n",
    "\n",
    "![ANN](../assets/embeddings/ann.png)\n",
    "\n",
    "In 2020, Google Research published a new ANN algorithm called [ScaNN](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html). It is considered one of the best ANN algorithms in the industry, also the most important foundation for search and recommendation in major Google services such as Google Search, YouTube and many others.\n",
    "\n",
    "##### A. What is Vertex AI Vector Search?\n",
    "\n",
    "Google Cloud developers can take the full advantage of Google's vector search technology with [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) (previously called Matching Engine). With this fully managed service, developers can just add the embeddings to its index and issue a search query with a key embedding for the blazingly fast vector search. In the case of the Stack Overflow demo, **Vector Search can find relevant questions from 8 million embeddings in tens of milliseconds**.\n",
    "\n",
    "![Vector Search](../assets/embeddings/vector-search.png)\n",
    "\n",
    "With Vector Search, we don't need to spend much time and money building your own vector search service from scratch or using open source tools if our goal is high scalability, availability and maintainability for production systems.\n",
    "\n",
    "##### B. Get Started with Vector Search\n",
    "\n",
    "When we already have the embeddings, then getting started with Vector Search is pretty easy. In this section, we will follow the steps below.\n",
    "\n",
    "**Setting up Vector Search**\n",
    "\n",
    "* Save the embeddings in JSON files on Cloud Storage\n",
    "* Build an Index\n",
    "* Create an Index Endpoint\n",
    "* Deploy the Index to the endpoint\n",
    "\n",
    "**Use Vector Search**\n",
    "\n",
    "* Query with the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Save the embeddings in a JSON file\n",
    "\n",
    "To load the embeddings to Vector Search, we need to save them in JSON files with JSONL format. See more information in the docs at [Input data format and structure](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure#data-file-formats).\n",
    "\n",
    "First, export the id and embedding columns from the DataFrame in JSONL format, and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save id and embedding as a json file\n",
    "jsonl_string = df[[\"id\", \"embedding\"]].to_json(orient=\"records\", lines=True)\n",
    "with open(\"questions.json\", \"w\") as f:\n",
    "    f.write(jsonl_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73250763</td>\n",
       "      <td>[-0.0277448352, 0.0100488178, -0.0102676898, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73206525</td>\n",
       "      <td>[-0.0495707169, 0.0218204018, 0.00418983170000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73475664</td>\n",
       "      <td>[-0.0075527029, -0.008695866, -0.0365824923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73399777</td>\n",
       "      <td>[-0.0029502742, -0.025221264, 0.00913337710000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73426773</td>\n",
       "      <td>[0.0050369711, 0.0341004618, 0.0198147688, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                          embedding\n",
       "0  73250763  [-0.0277448352, 0.0100488178, -0.0102676898, 0...\n",
       "1  73206525  [-0.0495707169, 0.0218204018, 0.00418983170000...\n",
       "2  73475664  [-0.0075527029, -0.008695866, -0.0365824923, 0...\n",
       "3  73399777  [-0.0029502742, -0.025221264, 0.00913337710000...\n",
       "4  73426773  [0.0050369711, 0.0341004618, 0.0198147688, 0.0..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first few lines of the json file\n",
    "pd.read_json(\"questions.json\", lines=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D. Then, create a new Cloud Storage bucket and copy the file to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# create function to create a new bucket and upload a file\n",
    "def create_bucket_and_upload_file(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Create a new bucket in GCS and upload a file to it.\"\"\"\n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "    # Create a new bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    bucket.location = \"US\"  # you can change the location\n",
    "    bucket = storage_client.create_bucket(bucket, location=bucket.location)\n",
    "\n",
    "    # Upload a file\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}. URI: gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage, creating the bucket if it doesn't exist.\"\"\"\n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "    # Check if the bucket exists\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    if not bucket.exists():\n",
    "        # Create a new bucket if it does not exist\n",
    "        bucket.location = \"us-central1\"  # You can change the location if needed\n",
    "        bucket = storage_client.create_bucket(bucket, location=bucket.location)\n",
    "        print(f\"Bucket {bucket_name} created.\")\n",
    "    else:\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "\n",
    "    # Upload a file\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}. URI: gs://{bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ridwa\\AppData\\Local\\Temp\\ipykernel_7968\\3303099630.py:10: DeprecationWarning: Assignment to 'Bucket.location' is deprecated, as it is only valid before the bucket is created. Instead, pass the location to `Bucket.create`.\n",
      "  bucket.location = \"us-central1\"  # You can change the location if needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket example_bukcet created.\n",
      "File questions.json uploaded to questions.json in bucket example_bukcet. URI: gs://example_bukcet/questions.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "bucket_name = \"example_bukcet\"  # Replace with your bucket name\n",
    "source_file_name = \"questions.json\"  # Replace with the name of your file\n",
    "destination_blob_name = \"questions.json\"  # The name you want for the file in the bucket\n",
    "\n",
    "upload_file_to_gcs(bucket_name, source_file_name, destination_blob_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json file was successfully uploaded to bucket\n",
    "\n",
    "![GCS](../assets/embeddings/cloud-storage.png)\n",
    "\n",
    "\n",
    "##### E. Create an Index\n",
    "\n",
    "Now it's ready to load the embeddings to Vector Search. Its APIs are available under the [`aiplatform`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform) package of the SDK.\n",
    "\n",
    "Create an [`MatchingEngineIndex`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex) with its _create_tree_ah_index_ function (_Matching Engine_ is the previous name of Vector Search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the aiplatform package\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/840606066459/locations/us-central1/indexes/950721316258840576/operations/3394750878131945472\n",
      "MatchingEngineIndex created. Resource name: projects/840606066459/locations/us-central1/indexes/950721316258840576\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/840606066459/locations/us-central1/indexes/950721316258840576')\n"
     ]
    }
   ],
   "source": [
    "# create index\n",
    "my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=f\"embvs-tutorial-index-{UID}\",\n",
    "    contents_delta_uri=f\"gs://{bucket_name}\",\n",
    "    dimensions=768,\n",
    "    location=\"us-central1\",\n",
    "    approximate_neighbors_count=20,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `create_tree_ah_index` function, it starts building an Index. This will take under a few minutes if the dataset is small, otherwise about 50 minutes or more depending on the size of the dataset. We can check status of the index creation on the [Vector Search Console > INDEXES](https://console.cloud.google.com/vertex-ai/matching-engine/indexes) tab.\n",
    "\n",
    "![Vector Search Creation](../assets/embeddings/vector-search-creation.png)\n",
    "\n",
    "**The parameters for creating index**\n",
    "\n",
    "* `contents_delta_uri`: The URI of Cloud Storage directory where we stored the embedding JSON files\n",
    "* `dimensions`: Dimension size of each embedding. In this case, it is 768 as we are using the embeddings from the Text Embeddings API.\n",
    "* `approximate_neighbors_count`: how many similar items we want to retrieve in typical cases\n",
    "* `distance_measure_type`: what metrics to measure distance/similarity between embeddings. In this case it's DOT_PRODUCT_DISTANCE\n",
    "\n",
    "See the [document](https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index) for more details on creating Index and the parameters.\n",
    "\n",
    "**Batch Update or Streaming Update?**\n",
    "\n",
    "There are two types of index: `Index for Batch Update` (used in this tutorial) and `Index for Streaming Updates`. The Batch Update index can be updated with a batch process whereas the Streaming Update index can be updated in real-time. The latter one is more suited for use cases where you want to add or update each embeddings in the index more often, and crucial to serve with the latest embeddings, such as e-commerce product search.\n",
    "\n",
    "##### F. Create Index Endpoint and deploy the Index\n",
    "\n",
    "To use the Index, we need to create an Index Endpoint. It works as a server instance accepting query requests for our Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336/operations/3254576339730038784\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336')\n"
     ]
    }
   ],
   "source": [
    "# create IndexEndpoint\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=f\"embvs-tutorial-index-endpoint-{UID}\",\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial utilizes a [`Public Endpoint`](https://cloud.google.com/vertex-ai/docs/vector-search/setup/setup#choose-endpoint) and does not support [`Virtual Private Cloud (VPC)`](https://cloud.google.com/vpc/docs/private-services-access). Unless you have a specific requirement for VPC, we recommend using a Public Endpoint. Despite the term \"public\" in its name, it does not imply open access to the public internet. Rather, it functions like other endpoints in Vertex AI services, which are secured by default through IAM. Without explicit IAM permissions, as we have previously established, no one can access the endpoint.\n",
    "\n",
    "With the Index Endpoint, deploy the Index by specifying an unique deployed index ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_INDEX_ID = f\"embvs_tutorial_deployed_{UID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336/operations/8186017931700731904\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint.MatchingEngineIndexEndpoint object at 0x000001D1C96E72B0> \n",
       "resource name: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deploy the Index to the Index Endpoint\n",
    "my_index_endpoint.deploy_index(index=my_index, deployed_index_id=DEPLOYED_INDEX_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is the first time to deploy an Index to an Index Endpoint, it will take around 25 minutes to automatically build and initiate the backend for it. After the first deployment, it will finish in seconds. To see the status of the index deployment, open the [Vector Search Console > INDEX ENDPOINTS](https://console.cloud.google.com/vertex-ai/matching-engine/index-endpoints) tab and click the Index Endpoint.\n",
    "\n",
    "\n",
    "![Vector Search Index Endpoint](../assets/embeddings/vector-search-index-endpoint.png)\n",
    "\n",
    "##### G. Utilities\n",
    "\n",
    "Sometimes it takes tens of minutes to create or deploy Indexes and we would lose connection with the Jupyter/Colab runtime. In that case, instead of creating or deploying new Index again, we can check the Vector Search Console and get the existing ones to continue.\n",
    "\n",
    "**Get an existing Index**\n",
    "\n",
    "To get an Index object that already exists, replace the following [our-index-id] with the index ID and run the cell. We can check the ID on the [Vector Search Console > INDEXES tab](https://console.cloud.google.com/vertex-ai/matching-engine/indexes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.matching_engine.matching_engine_index.MatchingEngineIndex object at 0x000001D1C96E7310> \n",
      "resource name: projects/840606066459/locations/us-central1/indexes/950721316258840576\n"
     ]
    }
   ],
   "source": [
    "my_index_id = \"950721316258840576\"  # @param {type:\"string\"}\n",
    "my_index = aiplatform.MatchingEngineIndex(my_index_id)\n",
    "print(my_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get an existing Index Endpoint**\n",
    "\n",
    "To get an Index Endpoint object that already exists, replace the following [our-index-endpoint-id] with the Index Endpoint ID and run the cell. We can check the ID on the [Vector Search Console > INDEX ENDPOINTS tab](https://console.cloud.google.com/vertex-ai/matching-engine/index-endpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint.MatchingEngineIndexEndpoint object at 0x000001D1C978CA90> \n",
      "resource name: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n"
     ]
    }
   ],
   "source": [
    "my_index_endpoint_id = \"1115384177634574336\"  # @param {type:\"string\"}\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(my_index_endpoint_id)\n",
    "print(my_index_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### H. Run Query\n",
    "\n",
    "Finally it's ready to use Vector Search. In the following code, it creates an embedding for a test question, and find similar question with the Vector Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings_wrapper([\"How to read JSON with Python?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.009405541233718395, -0.012011843733489513, 0.03272425755858421, -0.017712930217385292, 0.024488620460033417, -0.0064089735969901085, 0.027680082246661186, 0.01308376993983984, -0.028244134038686752, 0.01817595772445202, 0.025589874014258385, 0.038305673748254776, 0.03444906696677208, -0.024641847237944603, -0.020275678485631943, 0.0040742806158959866, -0.04790573567152023, -0.04560501500964165, 0.0008088672766461968, 0.026284685358405113, -0.04548338055610657, -0.019667573273181915, 0.005759553983807564, 0.009407688863575459, -0.031886711716651917, -0.10478324443101883, 0.030070681124925613, 0.013050236739218235, -0.007181359920650721, -0.03439131751656532, -0.04584177955985069, 0.03567810729146004, -0.025517383590340614, -0.04891308397054672, 0.021599987521767616, 0.04497474059462547, 0.04183082655072212, 0.008171860128641129, 0.002165100071579218, -0.0056815375573933125, 0.013008587062358856, -0.0028206240385770798, 0.038202106952667236, 0.02001797780394554, -0.025354227051138878, 0.008420979604125023, -0.03555190935730934, 0.01546707283705473, -0.06307845562696457, -0.04537934809923172, 0.001563728554174304, -0.017431553453207016, 0.010842551477253437, 0.04135799780488014, -0.03739609196782112, 0.04034332558512688, -0.051763810217380524, 0.0034832381643354893, -0.03924331068992615, 0.03336631506681442, 0.011150507256388664, 0.0009618012118153274, 0.029157845303416252, -0.060436561703681946, 0.01971537061035633, -0.004958311561495066, 0.05458982288837433, 0.030573520809412003, -0.005572524853050709, -0.007578466087579727, 0.053300753235816956, 0.009207691997289658, 0.014612754806876183, -0.0026316852308809757, 0.024347325786948204, -0.013198011554777622, -0.0045662205666303635, 0.035272978246212006, 0.05979427695274353, -0.05761045217514038, 0.02443026751279831, -0.002674683229997754, -0.05190866068005562, -0.11701173335313797, -0.0353415347635746, 0.05694330856204033, 0.034433744847774506, 0.04705679416656494, -0.02644514851272106, 0.03461979329586029, -0.001336922636255622, -0.014876046217978, 0.001050901017151773, 0.03102789632976055, -0.0005641120369546115, -0.0400431863963604, 0.00014373000885825604, -0.03025025501847267, 0.04045774042606354, -0.014080624096095562, -0.055911995470523834, -0.014119904488325119, 0.02831895649433136, -0.0588798001408577, 0.027826689183712006, -0.01232415996491909, -0.045025259256362915, 0.002369251102209091, -0.05210235342383385, -0.10770098119974136, -0.04667215049266815, -0.019126811996102333, -0.07183384895324707, -0.008153972215950489, -0.025171348825097084, -0.06190849468111992, 0.04910101741552353, -0.03490752726793289, -0.021734731271862984, -0.0020719750318676233, -0.0362006314098835, -0.028311191126704216, 0.023530853912234306, -0.013428081758320332, -0.03341395780444145, 0.05636700615286827, 0.017887458205223083, -0.013040095567703247, -0.04141630977392197, -0.01343829371035099, 0.033080123364925385, -0.007773988880217075, 0.029670901596546173, -0.0007462711655534804, 0.040538590401411057, 0.03589671105146408, -0.016277411952614784, 0.03277809917926788, 0.05429578572511673, 0.03574594110250473, -0.026561932638287544, 0.014824701473116875, -0.026595469564199448, 0.025581520050764084, -0.003565022489055991, -0.07308468222618103, 0.00736099062487483, 0.03201419860124588, -0.04714366793632507, -0.021274758502840996, 0.0023853809107095003, -0.03578733280301094, 0.005731017328798771, -0.017657453194260597, 0.05254562199115753, 0.04305574297904968, 0.039887044578790665, 0.06994614005088806, 0.015508620999753475, -0.0593050941824913, 0.015341680496931076, 0.01345802191644907, 0.06042591482400894, -0.030097244307398796, 0.05903387442231178, 0.04705330729484558, -0.0413324236869812, -0.017642321065068245, 0.031630780547857285, 0.00884218979626894, -0.015881625935435295, -0.10116404294967651, -0.01668095961213112, -0.02221027761697769, 0.025841806083917618, -0.025453679263591766, -0.0007449196418747306, -0.02315189130604267, 0.0491163544356823, -0.06466196477413177, 0.025629082694649696, -0.0786285549402237, 0.0478944256901741, -0.02673632651567459, -0.01625691168010235, 0.016142716631293297, 0.00992589257657528, -0.07790639996528625, -0.041277263313531876, -0.0004246697935741395, -0.03370783105492592, 0.013937464915215969, 0.022733811289072037, -0.0722714513540268, -0.0076833488419651985, -0.007965010590851307, 0.06705913692712784, -0.16775766015052795, -0.06874255836009979, 0.06232908368110657, 0.012101182714104652, -0.013107853941619396, -0.002861978719010949, 0.042909882962703705, 0.002783077070489526, 0.03888816013932228, -0.026683418080210686, 0.019997723400592804, -0.01528625562787056, -0.019104810431599617, 0.01596354879438877, -0.012022223323583603, 0.02061534859240055, -0.0027251706924289465, 0.00926592480391264, 0.010751012712717056, 0.0037164250388741493, -0.020319418981671333, -0.10295058786869049, 0.00738461734727025, -0.0032584357541054487, -0.022770956158638, 0.05696180462837219, -0.0071901739574968815, -0.05670534819364548, 0.04357171431183815, 0.012764299288392067, 0.010803613811731339, 0.049586743116378784, 0.008161025121808052, -0.06728173047304153, 0.009258481673896313, -0.01704864390194416, -0.0034605516120791435, -0.014956998638808727, 0.014374053105711937, 0.014377729035913944, 0.028404422104358673, -0.016245506703853607, 0.05673561617732048, 0.0038824626244604588, 0.06851314008235931, 0.007960989139974117, 0.03172094747424126, 0.04686364531517029, 0.0004561798123177141, 0.021900659427046776, -0.0029774114955216646, 0.03259344398975372, -0.022206969559192657, 0.000994886620901525, -0.07888390123844147, -0.03715208172798157, 0.0020504097919911146, -0.016140678897500038, -0.009979471564292908, -0.0702810287475586, 0.04491866007447243, 0.020261937752366066, -0.010948126204311848, -0.002023999812081456, 0.006055581849068403, 0.0388670414686203, 0.07607360929250717, -0.04791808873414993, -0.027975155040621758, -0.01363363303244114, -0.018944168463349342, 0.07770050317049026, 0.0017296579899266362, -0.03345213830471039, -0.030621763318777084, 0.05044005811214447, 0.041248273104429245, -0.03037947416305542, -0.004558802582323551, 0.009261545725166798, 0.006750657223165035, 0.00489142956212163, 0.011325410567224026, 0.003695480292662978, -0.009884914383292198, 0.00607575848698616, 0.01188632845878601, 0.012463290244340897, -0.005119042936712503, -0.03080969862639904, -0.009984433650970459, -0.035066116601228714, 0.034038614481687546, 0.001124973059631884, -0.00096624850993976, 0.0035994562786072493, 0.06334702670574188, 0.013010368682444096, -0.03559878468513489, 0.07447987049818039, 0.007522433064877987, -0.05750492215156555, 0.07557831704616547, -0.026812303811311722, -0.014922724105417728, -0.046319492161273956, -0.022422421723604202, -0.016772788017988205, -0.019045287743210793, 0.029931651428341866, 0.035060539841651917, 0.028941676020622253, -0.011328307911753654, 0.020416582003235817, -0.031976379454135895, 0.06043435260653496, 0.005578980315476656, 0.001667780801653862, 0.03969006612896919, 0.015188322402536869, -0.005451595410704613, 0.007934166118502617, 0.06532749533653259, -0.0016943675000220537, 0.011117988266050816, 0.09056678414344788, 0.006437903735786676, 0.032064326107501984, -0.027281926944851875, 0.06933295726776123, -0.03555307909846306, 0.07696924358606339, -0.04521070793271065, -0.013910106383264065, -0.009242608211934566, 0.0026633215602487326, -0.010925782844424248, 0.008126860484480858, -0.0317317396402359, 0.013541994616389275, 0.03506600111722946, -0.03413309156894684, -0.030786769464612007, 0.04768766835331917, -0.0012435460230335593, -0.011731283739209175, -0.017343902960419655, -0.025725750252604485, 0.005475661717355251, -0.07298757880926132, -0.024025479331612587, -0.0014129040064290166, -0.03647536784410477, -0.0097417077049613, 0.0110955024138093, -0.030498649924993515, 0.04670373350381851, 0.013340793550014496, -0.04026664420962334, 0.019031275063753128, 0.025594869628548622, 0.028848987072706223, -0.004249069839715958, -0.056157633662223816, -0.010055155493319035, -0.024517608806490898, 0.042330916970968246, 0.03485653176903725, 0.07904478162527084, -0.05689629912376404, 0.06618761271238327, 0.01154029555618763, -0.012377836741507053, 0.01101685594767332, 0.00137447495944798, -0.0058490135706961155, -0.06539253890514374, -0.08122792840003967, 0.03676869720220566, -0.035203494131565094, 0.009333551861345768, -0.002331760013476014, -0.01235158834606409, -0.06154106929898262, 0.02319961227476597, -0.014437658712267876, -0.014916720800101757, 0.005904727149754763, 0.03613216057419777, 0.03582793474197388, 0.022168586030602455, -0.0505661815404892, -0.04435017332434654, 0.023987917229533195, 0.05183030664920807, 0.05564417690038681, -0.01982720009982586, -0.05073324218392372, 0.0235381368547678, -0.040274281054735184, -0.02025665156543255, 0.006704953033477068, -0.03267030417919159, 0.03917095065116882, 0.0497499443590641, -0.036501195281744, -0.034201014786958694, -0.010104290209710598, 0.06597360968589783, 0.06972101330757141, -0.0026399448979645967, 0.027432788163423538, -0.06940139085054398, 0.03464058041572571, 0.023460226133465767, 0.0036331324372440577, 0.024530842900276184, -0.040553465485572815, 0.002743464196100831, -0.019170677289366722, -0.04982541501522064, 0.021056698635220528, -0.06479507684707642, 0.02145977132022381, 0.021186113357543945, -0.06321157515048981, 0.016085579991340637, 0.007440377026796341, 0.022961996495723724, -0.038888294249773026, 0.0447777695953846, -0.027787085622549057, -0.03934856131672859, -0.044209036976099014, -0.005060940980911255, 0.05056824907660484, -0.055745504796504974, -0.012991415336728096, -0.029987821355462074, 0.049768347293138504, 0.03195181488990784, 0.04697234556078911, 0.026080286130309105, 0.008615882135927677, -0.0013011530973017216, -0.00651027075946331, 0.03302232548594475, 0.033251989632844925, -0.06842260807752609, -0.010218802839517593, -0.04856405407190323, -0.01915089786052704, -0.018345894291996956, -0.04339415207505226, -0.01728481985628605, 0.015486670657992363, 0.06625054776668549, 0.032638564705848694, -0.00748792476952076, 0.023509887978434563, -0.018413661047816277, -0.0016446973895654082, -0.020751606673002243, 0.017478957772254944, -0.027480076998472214, -0.01131656114012003, 0.058719702064991, -0.025909816846251488, -0.013106144964694977, 0.025572746992111206, 0.02869458496570587, -0.07087593525648117, 0.008952917531132698, -0.02361172065138817, 0.019199678674340248, -0.008431979455053806, -0.020118489861488342, -0.022047778591513634, -0.027878735214471817, 0.033224936574697495, 0.037571556866168976, -0.020418377593159676, 0.025277892127633095, -0.05005266144871712, -0.043313853442668915, -0.0013844326604157686, 0.015994878485798836, 5.71481286897324e-05, -0.006420361343771219, -0.0060396017506718636, -0.0006307688308879733, 0.010909659788012505, -0.02350228652358055, -0.00747166620567441, -0.01847964897751808, 0.042159296572208405, -0.049156900495290756, 0.01142707746475935, -0.01330725010484457, -0.025139642879366875, -0.027839770540595055, 0.051626939326524734, 0.00035166149609722197, 0.02525675669312477, 0.05911809578537941, 0.05086046829819679, -0.005639732349663973, -0.0309197548776865, -0.008041858673095703, 0.022252414375543594, 0.01064322143793106, -0.022999247536063194, -0.008968922309577465, 0.04893689230084419, 0.0297132208943367, -0.0019788944628089666, -0.029695898294448853, -0.0388132780790329, -0.05738961696624756, 0.02114461362361908, -0.0068572587333619595, -0.0002843628462869674, -0.02242492511868477, 0.011479685083031654, -0.07128656655550003, -0.04600324481725693, 0.0016498701879754663, 0.019428424537181854, 0.030856667086482048, 0.004046690184623003, -0.029566911980509758, -0.008327114395797253, 0.024126442149281502, 0.03822105750441551, 0.03468494117259979, -0.0177521500736475, 0.006306358613073826, -0.013732526451349258, -0.004192684311419725, 0.024798162281513214, 0.021819449961185455, 0.035255301743745804, 0.09639793634414673, -0.05477118864655495, -0.056853439658880234, 0.049215879291296005, 0.032660987228155136, -0.01267235167324543, -0.007255178410559893, 0.0349372960627079, 0.017212064936757088, -0.025858666747808456, 0.06510608643293381, 0.0044657220132648945, -0.018888570368289948, 0.005257074721157551, 0.09281989187002182, 0.0035018818452954292, -0.03608611598610878, 0.03488946333527565, -0.03323695436120033, -0.03675823286175728, -0.060281287878751755, -0.05275874212384224, -0.003414213191717863, 0.039886996150016785, -0.011399499140679836, 0.07491794228553772, -0.046955861151218414, 0.05050976946949959, -0.004940585233271122, -0.006338633596897125, 0.02021941728889942, -0.03674279898405075, -0.047571659088134766, -0.00917345006018877, -0.02320891059935093, 0.0015298626385629177, 0.011906237341463566, 0.0021804182324558496, 0.03474365174770355, 0.01594766601920128, 0.022678513079881668, -0.021234989166259766, 0.009212500415742397, 0.01545855961740017, 0.046257615089416504, -0.025777524337172508, -0.029070012271404266, -0.011586875654757023, -0.03835533559322357, 0.002360930200666189, -0.016194812953472137, 0.06258746981620789, -0.0011230743257328868, -0.03213541582226753, -0.03139938414096832, -0.05450543016195297, -0.054268546402454376, -0.06025371700525284, 0.009112192317843437, -0.017393629997968674, 0.0062911720015108585, 0.06572273373603821, 0.01384107768535614, -0.012850498780608177, -0.028029613196849823, 0.0010624639689922333, 0.03689565509557724, 0.036331143230199814, 0.019664090126752853, 0.0035171271301805973, -0.010305105708539486, 0.030517777428030968, 0.09307146072387695, -0.016641797497868538, 0.005053794011473656, -0.029343413189053535, -0.009740050882101059, -0.048618022352457047, 0.03340449929237366, -0.04816016927361488, 0.03585260361433029, 0.07535581290721893, -0.04823709651827812, -0.0016612833132967353, -0.05507494881749153, 0.022804588079452515, -0.010494275018572807, 0.03308647871017456, -0.027359597384929657, -0.035769786685705185, -0.06561020016670227, 0.06064876914024353, -0.04930534586310387, -0.05770910903811455, -0.025095131248235703, 0.015944400802254677, -0.04298275709152222, -0.007515574339777231, 0.027208615094423294, 0.036349501460790634, 0.03565063327550888, 0.03414659947156906, 0.04129470884799957, 0.04122219979763031, 0.0009549072710797191, -0.0378691591322422, -0.018710188567638397, -0.00697312830016017, -0.07826270163059235, -0.02227046713232994, -0.0024284638930112123, 0.04039071500301361, -0.015789486467838287, 0.004606278147548437, -0.04150260239839554, 0.04219915345311165, 0.004939835052937269, 0.008716474287211895, 0.0339205227792263, -0.0391547754406929, 0.030248988419771194, 0.018964190036058426, -0.0071404194459319115, -0.008945801295340061, 0.006992113310843706, -0.02231123112142086, 0.08108548820018768, -0.022835761308670044, 0.002834383863955736, 0.07116922736167908, -0.0013440129114314914, -0.044087592512369156, 0.010068644769489765, -0.0005871530738659203, -0.05671265721321106, 0.008831478655338287, -0.016451118513941765, 0.015659654513001442, 0.04055564105510712, -0.004092225339263678, 0.02688610926270485, -0.039833180606365204, 0.005472841206938028, 0.014951088465750217, -0.005988622084259987, -0.029969271272420883, -0.014444608241319656, 0.024750713258981705, 0.023520058020949364, 0.02278045192360878, -0.025541163980960846, -0.002927010413259268, 0.03128902241587639, -0.12035907059907913, -0.00456769997254014, 0.025762226432561874, -0.07414259016513824, -0.04842625930905342, -0.01878488063812256, 0.0038452865555882454, 0.014512247405946255, 0.02492365799844265, 0.03791609779000282, 0.02959803305566311, -0.007136414293199778, 0.05852475017309189, 0.01145981065928936, -0.02145380526781082, -0.010346930474042892, -0.005807379260659218, -0.035368990153074265, -0.014441882260143757, -0.07006049156188965, 0.006388558074831963, -0.0299727413803339, -0.03006409853696823, 0.04204794019460678, -0.07381865382194519, 0.018344445154070854, 0.028881138190627098, -0.028294775635004044, -0.006991240195930004, -0.023285824805498123, 0.0241595059633255, -0.0794181153178215, -0.0041163088753819466, -0.04206950590014458, -0.042402058839797974, -0.018020909279584885, 0.0021970763336867094, -0.042670637369155884, -0.011893452145159245, -0.012806162238121033, 0.00519692525267601, 0.05904112383723259, 0.013996024616062641, -0.007725341245532036, 0.0044306558556854725, -0.03900851309299469, 0.07657540589570999, -0.0034674941562116146, 0.012259934097528458, 0.02910681813955307, 0.03593311458826065, -0.008850148878991604, 0.0608014352619648, -0.011205712333321571, 0.011674987152218819, -0.007126201875507832, -0.03751956298947334, -0.07334183901548386, -0.03166947886347771, 0.04158002883195877, 0.027084801346063614, 0.0270862840116024, -0.008331753313541412, 0.02600436843931675, 0.0018710318254306912, 0.025796735659241676, 0.0472906231880188, 0.057780224829912186, 0.04565747454762459, -0.02402176707983017, -0.06469849497079849, 0.038180816918611526, 0.00464139087125659, 0.0007065230747684836, -0.07221797108650208, -0.027431920170783997]]\n"
     ]
    }
   ],
   "source": [
    "# inspect the emnbeddings of our input\n",
    "print(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7774 read_json instead of open and json.load\n",
      "0.7162 how to sum variables extracted from .json file?\n",
      "0.7132 How to convert json to nested Maps using Gson?\n",
      "0.7013 How to get batch predictions with jsonl data in sagemaker?\n",
      "0.6919 How do I get React to accept my Promise of data?\n",
      "0.6886 Easy Way To Import A Table from MS SQL Server to Django?\n",
      "0.6855 Need help in extracting values from Json for Jmeter\n",
      "0.6849 Pandas ValueError: Can only compare identically-labeled Series objects from single dataframe?\n",
      "0.6748 View JSONModel in redis-cli after using python redis-om to save it to the database\n",
      "0.6732 How to authenticate private API token in Python to access RightSignature\n",
      "0.6715 Strange behavior on json_decode in PHP code\n",
      "0.6710 How do I solve 'UserWarning: DataFrame columns are not unique, some columns will be omitted'?\n",
      "0.6708 To Find data inside tag how to write REGEX Expression?\n",
      "0.6650 how to scrape the instagram followers popup with python playwright\n",
      "0.6612 In Keras, how to use Model.predict function when looping over a tensorflow Dataset?\n",
      "0.6608 Parsing dictionary from VMware module\n",
      "0.6590 JSON result as String or Mono<T> in Spring WebClient?\n",
      "0.6576 Using PDF file in Keras OCR or PyTesseract - Python, is it possible?\n",
      "0.6570 How can I test for a numpy.datetime64() type/format?\n",
      "0.6531 404 error when getting file in static, django\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "response = my_index_endpoint.find_neighbors(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=test_embeddings,\n",
    "    num_neighbors=20,\n",
    ")\n",
    "\n",
    "# show the result\n",
    "import numpy as np\n",
    "\n",
    "for idx, neighbor in enumerate(response[0]):\n",
    "    id = np.int64(neighbor.id)\n",
    "    similar = df.query(\"id == @id\", engine=\"python\")\n",
    "    print(f\"{neighbor.distance:.4f} {similar.title.values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_neighbors` function only takes milliseconds to fetch the similar items even when we have billions of items on the Index, thanks to the ScaNN algorithm. Vector Search also supports [autoscaling](https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-public#autoscaling) which can automatically resize the number of nodes based on the demands of your workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Cleaning Up\n",
    "\n",
    "In case we are using your own Cloud project, not a temporary project on Qwiklab, please make sure to delete all the Indexes, Index Endpoints and Cloud Storage buckets after finishing this tutorial. Otherwise the remaining objects would **incur unexpected costs**.\n",
    "\n",
    "If we used Workbench, we may also need to delete the Notebooks from [the console](https://console.cloud.google.com/vertex-ai/workbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to delete a bucket and all its contents\n",
    "def delete_bucket_and_contents(bucket_name):\n",
    "    \"\"\"Deletes a bucket and all its contents in Google Cloud Storage.\"\"\"\n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Check if the bucket exists\n",
    "    if bucket.exists():\n",
    "        # Delete all the contents of the bucket\n",
    "        blobs = bucket.list_blobs()\n",
    "        for blob in blobs:\n",
    "            blob.delete()\n",
    "            print(f\"Blob {blob.name} deleted.\")\n",
    "\n",
    "        # Delete the bucket\n",
    "        bucket.delete()\n",
    "        print(f\"Bucket {bucket_name} deleted.\")\n",
    "    else:\n",
    "        print(f\"Bucket {bucket_name} does not exist or is already deleted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob questions.json deleted.\n",
      "Bucket example_bukcet deleted.\n"
     ]
    }
   ],
   "source": [
    "# delete the bucket\n",
    "delete_bucket_and_contents(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying MatchingEngineIndexEndpoint index_endpoint: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n",
      "Undeploy MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336/operations/9050709060155867136\n",
      "MatchingEngineIndexEndpoint index_endpoint undeployed. Resource name: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n",
      "Deleting MatchingEngineIndexEndpoint : projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n",
      "Delete MatchingEngineIndexEndpoint  backing LRO: projects/840606066459/locations/us-central1/operations/7075880623553904640\n",
      "MatchingEngineIndexEndpoint deleted. . Resource name: projects/840606066459/locations/us-central1/indexEndpoints/1115384177634574336\n",
      "Deleting MatchingEngineIndex : projects/840606066459/locations/us-central1/indexes/950721316258840576\n",
      "Delete MatchingEngineIndex  backing LRO: projects/840606066459/locations/us-central1/indexes/950721316258840576/operations/4065224272656728064\n",
      "MatchingEngineIndex deleted. . Resource name: projects/840606066459/locations/us-central1/indexes/950721316258840576\n"
     ]
    }
   ],
   "source": [
    "# delete Index Endpoint\n",
    "my_index_endpoint.undeploy_all()\n",
    "my_index_endpoint.delete(force=True)\n",
    "\n",
    "# delete Index\n",
    "my_index.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Grounding LLM outputs with Vertex AI Vector Search\n",
    "\n",
    "As we have seen, by combining the Embeddings API and Vector Search, we can use the embeddings to \"ground\" LLM outputs to real business data with low latency.\n",
    "\n",
    "For example, if an user asks a question, Embeddings API can convert it to an embedding, and issue an query on Vector Search to find similar embeddings in its index. Those embeddings represent the actual business data in the databases. As we are just retrieving the business data and not generating any artificial texts, there is no risk of having hallucinations in the result.\n",
    "\n",
    "![Summary](../assets/embeddings/summary-1.png)\n",
    "\n",
    "#### The difference between the questions and answers\n",
    "\n",
    "In this tutorial, we have used the Stack Overflow dataset. There is a reason why we had to use it; As the dataset has many pairs of **questions and answers**, so we can just find quesions similar to our question to find answers to it.\n",
    "\n",
    "In many business use cases, the semantics (meaning) of questions and answers are different. Also, there could be cases where we would want to add variety of recommended or personalized items to the results, like product search on e-commerce sites.\n",
    "\n",
    "In these cases, the simple semantics search don't work well. It's more like a recommendation system problem where we may want to train a model (e.g. Two-Tower model) to learn the relationship between the question embedding space and answer embedding space. Also, many production systems adds reranking phase after the semantic search to achieve higher search quality. Please see [Scaling deep retrieval with TensorFlow Recommenders and Vertex AI Matching Engine](https://cloud.google.com/blog/products/ai-machine-learning/scaling-deep-retrieval-tensorflow-two-towers-architecture) to learn more.\n",
    "\n",
    "#### Hybrid of semantic + keyword search\n",
    "Another typical challenge we will face in production system is to support keyword search combined with the semantic search. For example, for e-commerce product search, we may want to let users find product by entering its product name or model number. As LLM doesn't memorize those product names or model numbers, semantic search can't handle those \"usual\" search functionalities.\n",
    "\n",
    "Vertex AI Search is another product we may consider for those requirements. While Vector Search provides a simple semantic search capability only, Search provides a integrated search solution that combines semantic search, keyword search, reranking and filtering, available as an out-of-the-box tool.\n",
    "\n",
    "#### What about Retrieval Augmented Generation (RAG)?\n",
    "\n",
    "In this tutorial, we have looked at the simple combination of LLM embeddings and vector search. From this starting point, we may also extend the design to [Retrieval Augmented Generation (RAG)](https://www.google.com/search?q=Retrieval+Augmented+Generation+(RAG)&oq=Retrieval+Augmented+Generation+(RAG)).\n",
    "\n",
    "RAG is a popular architecture pattern of implementing grounding with LLM with text chat UI. The idea is to have the LLM text chat UI as a frontend for the document retrieval with vector search and summarization of the result.\n",
    "\n",
    "![Summary with RAG](../assets/embeddings/summary-rag.png)\n",
    "\n",
    "There are some pros and cons between the two solutions.\n",
    "\n",
    "\n",
    "| Feature                   | Emb + vector search | RAG            |\n",
    "|---------------------------|---------------------|----------------|\n",
    "| Design                    | simple              | complex        |\n",
    "| UI                        | Text search UI      | Text chat UI   |\n",
    "| Summarization of result   | No                  | Yes            |\n",
    "| Multi-turn (Context aware)| No                  | Yes            |\n",
    "| Latency                   | millisecs           | seconds        |\n",
    "| Cost                      | lower               | higher         |\n",
    "| Hallucinations            | No risk             | Some risk      |\n",
    "\n",
    "The Embedding + vector search pattern we have looked at with this tutorial provides simple, fast and low cost semantic search functionality with the LLM intelligence. RAG adds context-aware text chat experience and result summarization to it. While RAG provides the more \"Gen AI-ish\" experience, it also adds a risk of hallucination and higher cost and time for the text generation.\n",
    "\n",
    "To learn more about how to build a RAG solution, you may look at [Building Generative AI applications made easy with Vertex AI PaLM API and LangChain](https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain).\n",
    "\n",
    "### Resources\n",
    "\n",
    "To learn more, please check out the following resources:\n",
    "\n",
    "#### Documentations\n",
    "\n",
    "[Vertex AI Embeddings for Text API documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)\n",
    "\n",
    "[Vector Search documentation](https://cloud.google.com/vertex-ai/docs/matching-engine/overview)\n",
    "\n",
    "#### Vector Search blog posts\n",
    "\n",
    "[Vertex Matching Engine: Blazing fast and massively scalable nearest neighbor search](https://cloud.google.com/blog/products/ai-machine-learning/vertex-matching-engine-blazing-fast-and-massively-scalable-nearest-neighbor-search)\n",
    "\n",
    "[Find anything blazingly fast with Google's vector search technology](https://cloud.google.com/blog/topics/developers-practitioners/find-anything-blazingly-fast-googles-vector-search-technology)\n",
    "\n",
    "[Enabling real-time AI with Streaming Ingestion in Vertex AI](https://cloud.google.com/blog/products/ai-machine-learning/real-time-ai-with-google-cloud-vertex-ai)\n",
    "\n",
    "[Mercari leverages Google's vector search technology to create a new marketplace](https://cloud.google.com/blog/topics/developers-practitioners/mercari-leverages-googles-vector-search-technology-create-new-marketplace)\n",
    "\n",
    "[Recommending news articles using Vertex AI Matching Engine](https://cloud.google.com/blog/products/ai-machine-learning/recommending-articles-using-vertex-ai-matching-engine)\n",
    "\n",
    "[What is Multimodal Search: \"LLMs with vision\" change businesses](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
