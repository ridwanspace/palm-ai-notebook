{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Text summarization produces a concise and fluent summary of a longer text document. There are two main text summarization types: extractive and abstractive. Extractive summarization involves selecting critical sentences from the original text and combining them to form a summary. Abstractive summarization involves generating new sentences representing the original text's main points. In this notebook, you go through a few examples of how large language models can help with generating summaries based on text.\n",
    "\n",
    "Learn more about text summarization in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/summarization-prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this tutorial, you will learn how to use generative models to summarize information from text by working through the following examples:\n",
    "\n",
    "* Transcript summarization\n",
    "* Summarizing text into bullet points\n",
    "* Dialogue summarization with to-dos\n",
    "* Hashtag tokenization\n",
    "* Title & heading generation\n",
    "* You also learn how to evaluate model-generated summaries by comparing to human-created summaries using ROUGE as an evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import vertexai\n",
    "from IPython.display import Markdown, display\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate service account (authentication)\n",
    "json_path = '../llm-ai.json' # replace with your own service account\n",
    "credentials = service_account.Credentials.from_service_account_file(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Vertex AI\n",
    "load_dotenv()\n",
    "vertexai.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Transcript summarization\n",
    "\n",
    "In this first example, we summarize a piece of text on Google's New LLM named Gemini AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Google DeepMind has built a new generation of AI models, inspired by the way people understand and interact with the world. \n",
       "This multimodal AI model, called Gemini, can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a very short summary, no more than three sentences, for the following article:\n",
    "\n",
    "AI has been the focus of my life's work, as for many of my research colleagues. \n",
    "Ever since programming AI for computer games as a teenager, and throughout my years as a neuroscience researcher trying to understand the workings of the brain, \n",
    "I’ve always believed that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.\n",
    "\n",
    "This promise of a world responsibly empowered by AI continues to drive our work at Google DeepMind. \n",
    "For a long time, we’ve wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. \n",
    "AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.\n",
    "\n",
    "\n",
    "Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. \n",
    "It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, \n",
    "operate across and combine different types of information including text, code, audio, image and video.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary = generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "    ).text\n",
    "\n",
    "# you may also change this using print\n",
    "display(Markdown(\n",
    "    summary\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a summary, we can ask for a `TL;DR`` (\"too long; didn't read\"). We can compare the differences between the outputs generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "AI has been the focus of my life's work. I believe that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.\n",
       "\n",
       "We've wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.\n",
       "\n",
       "Gemini is the result of large-scale collaborative efforts by teams across Google. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a TL;DR for the following article:\n",
    "\n",
    "AI has been the focus of my life's work, as for many of my research colleagues. \n",
    "Ever since programming AI for computer games as a teenager, and throughout my years as a neuroscience researcher trying to understand the workings of the brain, \n",
    "I’ve always believed that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.\n",
    "\n",
    "This promise of a world responsibly empowered by AI continues to drive our work at Google DeepMind. \n",
    "For a long time, we’ve wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. \n",
    "AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.\n",
    "\n",
    "\n",
    "Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. \n",
    "It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, \n",
    "operate across and combine different types of information including text, code, audio, image and video.\n",
    "\n",
    "TL;DR:\n",
    "\"\"\"\n",
    "\n",
    "tldr = generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "    ).text\n",
    "\n",
    "display(Markdown(\n",
    "    tldr\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summarize text into bullet points\n",
    "\n",
    "In the following example, we use same text on Gemini AI, but ask the model to summarize it in bullet-point form. Feel free to change the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- AI has been the focus of my life's work.\n",
       "- I believe that AI can benefit humanity in incredible ways.\n",
       "- We've wanted to build a new generation of AI models.\n",
       "- Gemini is the result of large-scale collaborative efforts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a very short summary in four bullet points for the following article:\n",
    "\n",
    "AI has been the focus of my life's work, as for many of my research colleagues. \n",
    "Ever since programming AI for computer games as a teenager, and throughout my years as a neuroscience researcher trying to understand the workings of the brain, \n",
    "I’ve always believed that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.\n",
    "This promise of a world responsibly empowered by AI continues to drive our work at Google DeepMind. \n",
    "For a long time, we’ve wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. \n",
    "AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.\n",
    "Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. \n",
    "It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, \n",
    "operate across and combine different types of information including text, code, audio, image and video.\n",
    "Bulletpoints:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "points = generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "    ).text\n",
    "\n",
    "display(Markdown(\n",
    "    points\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dialogue summarization with to-dos\n",
    "Dialogue summarization involves condensing a conversation into a shorter format so that you don't need to read the whole discussion but can leverage a summary. In this example, we ask the model to summarize an example conversation between an online retail customer and a support agent, and include to-dos at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Larry received the wrong item and wants to return it for a refund.\n",
       "The support agent processed the refund and Larry will receive his money back within 14 days.\n",
       "To-do's for the support agent:\n",
       "- Process the refund.\n",
       "- Send an email to Larry confirming the refund."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Please generate a summary of the following conversation and at the end summarize the to-do's for the support Agent:\n",
    "\n",
    "Customer: Hi, I'm Larry, and I received the wrong item.\n",
    "\n",
    "Support Agent: Hi, Larry. How would you like to see this resolved?\n",
    "\n",
    "Customer: That's alright. I want to return the item and get a refund, please.\n",
    "\n",
    "Support Agent: Of course. I can process the refund for you now. Can I have your order number, please?\n",
    "\n",
    "Customer: It's [ORDER NUMBER].\n",
    "\n",
    "Support Agent: Thank you. I've processed the refund, and you will receive your money back within 14 days.\n",
    "\n",
    "Customer: Thank you very much.\n",
    "\n",
    "Support Agent: You're welcome, Larry. Have a good day!\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "todos = generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=256, top_k=40, top_p=0.8\n",
    "    ).text\n",
    "\n",
    "display(\n",
    "    Markdown(todos)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hashtag tokenization\n",
    "\n",
    "Hashtag tokenization is the process of taking a piece of text and getting the hashtag \"tokens\" out of it. We can use this, for example, if we want to generate hashtags for our social media campaigns. In this example, we take this tweet from [Google Cloud](https://twitter.com/googlecloud/status/1649127992348606469) and generate some hashtags we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "EarthDay"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Tokenize the hashtags of this tweet:\n",
    "\n",
    "Google Cloud\n",
    "@googlecloud\n",
    "How can data help our changing planet? 🌎\n",
    "\n",
    "In honor of #EarthDay this weekend, we’re proud to share how we’re partnering with\n",
    "@ClimateEngine\n",
    " to harness the power of geospatial data and drive toward a more sustainable future.\n",
    "\n",
    "Check out how → https://goo.gle/3mOUfts\n",
    "\"\"\"\n",
    "hashtag = generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=256, top_k=40, top_p=0.8\n",
    "    ).text\n",
    "\n",
    "display(\n",
    "    Markdown(hashtag)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Title & heading generation\n",
    "\n",
    "Below, we ask the model to generate five options for possible title/heading combos for a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. How AI is used for the benefit of culture\n",
       "2. Google Arts & Culture Lab experiments with AI\n",
       "3. AI in the Arts & Culture Lab\n",
       "4. AI for culture\n",
       "5. AI in the Arts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a title for this text, give me five options:\n",
    "Whether helping physicians identify disease or finding photos of “hugs,” AI is behind a lot of the work we do at Google. And at our Arts & Culture Lab in Paris, we’ve been experimenting with how AI can be used for the benefit of culture.\n",
    "Today, we’re sharing our latest experiments—prototypes that build on seven years of work in partnership the 1,500 cultural institutions around the world.\n",
    "Each of these experimental applications runs AI algorithms in the background to let you unearth cultural connections hidden in archives—and even find artworks that match your home decor.\"\n",
    "\"\"\"\n",
    "title = generation_model.predict(\n",
    "        prompt, temperature=0.8, max_output_tokens=256, top_k=1, top_p=0.8\n",
    "    ).text\n",
    "\n",
    "display(\n",
    "    Markdown(title)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can evaluate the outputs from summarization tasks using [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) as an evalulation framework. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) are measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as `n-gram`, `word sequences`, and `word pairs` between the computer-generated summary to be evaluated and the ideal summaries created by humans.\n",
    "\n",
    "The first step is to install the ROUGE library.\n",
    "\n",
    "`pip install rouge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Quantum computers work by manipulating qubits.\n",
       "Qubits are sensitive to errors, and the problem worsens as quantum computers grow.\n",
       "Quantum error correction protects information by encoding it across multiple physical qubits to form a “logical qubit”.\n",
       "By encoding larger numbers of physical qubits on our quantum processor into one logical qubit, we hope to reduce the error rates to enable useful quantum algorithms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "ROUGE = Rouge()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Provide a very short, maximum four sentences, summary for the following article:\n",
    "\n",
    "Our quantum computers work by manipulating qubits in an orchestrated fashion that we call quantum algorithms.\n",
    "The challenge is that qubits are so sensitive that even stray light can cause calculation errors — and the problem worsens as quantum computers grow.\n",
    "This has significant consequences, since the best quantum algorithms that we know for running useful applications require the error rates of our qubits to be far lower than we have today.\n",
    "To bridge this gap, we will need quantum error correction.\n",
    "Quantum error correction protects information by encoding it across multiple physical qubits to form a “logical qubit,” and is believed to be the only way to produce a large-scale quantum computer with error rates low enough for useful calculations.\n",
    "Instead of computing on the individual qubits themselves, we will then compute on logical qubits. By encoding larger numbers of physical qubits on our quantum processor into one logical qubit, we hope to reduce the error rates to enable useful quantum algorithms.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "candidate = generation_model.predict(\n",
    "    prompt, temperature=0.1, max_output_tokens=1024, top_k=40, top_p=0.9\n",
    ").text\n",
    "\n",
    "display(\n",
    "    Markdown(candidate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a human-generated summary that we will use to compare to the candidate generated by the model. We will call this `reference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"\"\"\n",
    "Quantum computers are sensitive to noise and errors. \n",
    "To bridge this gap, we will need quantum error correction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the candidate and reference to evaluate the performance. In this case, ROUGE will give us:\n",
    "\n",
    "* `rouge-1`, which measures unigram overlap\n",
    "* `rouge-2`, which measures bigram overlap\n",
    "* `rouge-l`, which measures the longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.5263157894736842,\n",
       "   'p': 0.20408163265306123,\n",
       "   'f': 0.29411764303200694},\n",
       "  'rouge-2': {'r': 0.2222222222222222,\n",
       "   'p': 0.06557377049180328,\n",
       "   'f': 0.10126581926614336},\n",
       "  'rouge-l': {'r': 0.47368421052631576,\n",
       "   'p': 0.1836734693877551,\n",
       "   'f': 0.26470587832612463}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROUGE.get_scores(candidate, reference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
