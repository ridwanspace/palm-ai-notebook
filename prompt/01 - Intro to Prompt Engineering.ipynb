{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook covers the essentials of prompt engineering, including some best practices.\n",
    "\n",
    "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview).\n",
    "\n",
    "## Objective\n",
    "In this notebook, you learn best practices around prompt engineering -- how to design prompts to improve the quality of your responses.\n",
    "\n",
    "This notebook covers the following best practices for prompt engineering:\n",
    "\n",
    "* Be concise\n",
    "* Be specific and well-defined\n",
    "* Ask one task at a time\n",
    "* Turn generative tasks into classification tasks\n",
    "* Improve response quality by including examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import vertexai\n",
    "from IPython.display import Markdown, display\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate service account (authentication)\n",
    "json_path = '../llm-ai.json' # replace with your own service account\n",
    "credentials = service_account.Credentials.from_service_account_file(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Vertex AI\n",
    "load_dotenv()\n",
    "vertexai.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt engineering best practices\n",
    "\n",
    "Prompt engineering is all about how to design our prompts so that the response is what we were indeed hoping to see.\n",
    "\n",
    "The idea of using \"unfancy\" prompts is to minimize the noise in our prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
    "\n",
    "In this section, you'll cover the following best practices when engineering prompts:\n",
    "\n",
    "* Be concise\n",
    "* Be specific, and well-defined\n",
    "* Ask one task at a time\n",
    "* Improve response quality by including examples\n",
    "* Turn generative tasks to classification tasks to improve safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Be Concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Not recommended`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The name of the electronic shop could be \"The Gadget Store\". This name is catchy and easy to remember, and it also conveys the idea that the shop sells a variety of electronic gadgets, including televisions and phones."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"What do you think could be a good name for an \n",
    "electronic shop that specializes in selling television and phone?\n",
    "\"\"\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recommended`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* The Gadget Store\n",
       "* The Electronic Boutique\n",
       "* The Tech Zone\n",
       "* The Digital Depot\n",
       "* The Smart Home\n",
       "* The Future Shop\n",
       "* The Gadget Guru\n",
       "* The Tech Wizard\n",
       "* The Electronics Emporium"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Suggest a name for an electronic shop that sells television and phone\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Be specific, and well-defined\n",
    "\n",
    "_Suppose that we want to brainstorm creative ways to describe aurora._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Not recommended`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "s Auroras are a natural light display in the sky that is caused by the interaction of charged particles from the sun with the Earth's atmosphere. They are most commonly seen at high latitudes, such as near the poles, but can sometimes be seen at lower latitudes as well. Auroras are often referred to as \"northern lights\" or \"southern lights\" depending on their location.\n",
       "\n",
       "Auroras are caused by the interaction of charged particles from the sun with the Earth's atmosphere. The sun emits a constant stream of charged particles, called the solar wind. When these particles interact with the Earth's magnetic field, they are deflected towards the poles. At the poles, the particles collide with atoms in the atmosphere, causing them to emit light.\n",
       "\n",
       "The colors of auroras are determined by the type of atoms that are excited by the solar wind particles. The most common colors are green and red, but auroras can also be blue, yellow, or purple. The intensity of an aurora is also determined by the strength of the solar wind.\n",
       "\n",
       "Auroras are a beautiful and awe-inspiring natural phenomenon. They are a reminder of the vastness of space and the interconnectedness of our planet with the rest of the universe."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Tell me about aurora\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recommended`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Auroras are a natural phenomenon that occurs in the Earth's atmosphere.\n",
       "* They are caused by the interaction of charged particles from the sun with the Earth's magnetic field.\n",
       "* Auroras are typically seen in high-latitude regions, such as the Arctic and Antarctic.\n",
       "* They can be seen at any time of day or night, but they are most commonly seen at night.\n",
       "* Auroras are often seen as a green or pink glow in the sky, but they can also be red, yellow, blue, or white.\n",
       "* Auroras can be very bright, and they can sometimes be seen from hundreds of miles away.\n",
       "* Auroras are a beautiful and awe-inspiring sight, and they are a popular tourist attraction in many high-latitude regions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Generate a list of ways what makes aurora unique phenomenon compared to others\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ask one task at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Not recommended`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The best way to store fresh vegetables is to keep them in a cool, dark place. This will help to slow down the process of spoilage. You can also store vegetables in the refrigerator, but be sure to wrap them in plastic wrap or place them in a sealed container to prevent them from drying out.\n",
       "\n",
       "Your mom is probably yelling at you because you are not storing your vegetables properly. This could be causing them to spoil faster, which is why she is concerned. Be sure to follow the tips above to help keep your vegetables fresh and healthy.\n",
       "\n",
       "Here are some additional tips for storing fresh vegetables:\n",
       "\n",
       "* Wash vegetables thoroughly before storing them.\n",
       "* Remove any damaged or bruised vegetables.\n",
       "* Store vegetables in a single layer, so that they have plenty of air circulation.\n",
       "* Do not store vegetables near fruits, as the ethylene gas released by fruits can cause vegetables to spoil faster.\n",
       "* Check your vegetables regularly and discard any that have spoiled."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What is the best way to store fresh veggies and why my mom is always yelling at me?\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recommended`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The best way to store fresh veggies is to keep them in a cool, dark place. This will help to preserve their freshness and nutrients. You can also store them in the refrigerator, but be sure to wrap them in plastic wrap or place them in a sealed container to prevent them from drying out."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What is the best way to store fresh veggies?\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There could be a number of reasons why your mom is always yelling at you. It's important to try to understand what might be causing her behavior so that you can both work together to improve your relationship.\n",
       "\n",
       "Some possible reasons why your mom might be yelling at you include:\n",
       "\n",
       "* **She's stressed out.** If your mom is under a lot of stress, it can be difficult for her to control her emotions. This could lead to her yelling at you even when you don't think you've done anything wrong.\n",
       "* **She's trying to control you.** Some parents yell at their children as a way to control their behavior. They may believe that yelling is the only way to get their children to listen to them.\n",
       "* **She's not communicating effectively.** Your mom may not be able to express her feelings in a healthy way. This could lead to her yelling at you as a way to release her frustration.\n",
       "* **She has unrealistic expectations of you.** Your mom may expect you to be perfect. When you don't meet her expectations, she may yell at you as a way to express her disappointment.\n",
       "\n",
       "If you think your mom's yelling is due to stress, you can"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Why my mom is always yelling at me?\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch Out for Hallucination\n",
    "\n",
    "\n",
    "Despite being trained on vast amounts of data, LLMs can still produce text containing statements that lack grounding in reality. These responses, often referred to as \"hallucinations,\" stem from the models' limited memorization capabilities. Simply prompting an LLM to provide a citation is not a reliable solution, as they have been known to fabricate or misrepresent references. Addressing hallucinations remains a fundamental challenge for LLMs and an active area of research. Therefore, it is crucial to exercise caution when relying on LLM outputs, as they may appear confident and correct but are actually inaccurate.\n",
    "\n",
    "Paradoxically, in creative applications, hallucinating can be a desirable feature. Repeatedly using prompts like the one below can reveal instances where the LLM confidently, yet incorrectly, asserts that \"Neil Armstrong was the first Indonesia to visit the moon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The first Indonesian to land on the Moon was Neil Armstrong."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Who was the first Indonesian to land on the Moon?\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn generative tasks into classification tasks to reduce output variability\n",
    "\n",
    "\n",
    "__Generative tasks lead to higher output variability__\n",
    "\n",
    "The prompt below results in an open-ended response, useful for brainstorming, but response is highly variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* **Contribute to open source projects.** This is a great way to learn new technologies and get your code reviewed by other developers. There are many open source projects out there, so you can find one that interests you and start contributing.\n",
       "* **Take online courses or tutorials.** There are many online courses and tutorials available that can help you improve your programming skills. You can find courses on a variety of topics, from basic programming to advanced topics.\n",
       "* **Read programming blogs and articles.** This is a great way to stay up-to-date on the latest trends in programming. There are many great programming blogs and articles out there, so you can find ones that cover topics that interest you.\n",
       "* **Practice, practice, practice!** The best way to improve your programming skills is to practice. Write code, solve problems, and learn from your mistakes. The more you practice, the better you'll become."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I'm a full-time employee. Recommend me a programming activity to improve my skills.\"\n",
    "\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Classification tasks reduces output variability__\n",
    "\n",
    "The prompt below results in a choice and may be useful if you want the output to be easier to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I would suggest learning Python. Python is a general-purpose programming language that is easy to learn and has a wide range of applications. It is used for web development, data science, machine learning, and many other tasks. There are many resources available to learn Python, including online tutorials, books, and courses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"I'm a full-time employee. Which of these activities do you suggest and why:\n",
    "a) learn Python\n",
    "b) learn Javascript\n",
    "c) learn Golang\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve response quality by including examples\n",
    "\n",
    "Incorporating examples into prompts is another effective strategy for enhancing response quality. By providing in-context examples, the LLM gains a clearer understanding of how to respond appropriately. Typically, one to five examples (shots) suffice to improve response quality. However, excessive examples can lead to overfitting, diminishing response quality.\n",
    "\n",
    "Analogous to classical model training, the quality and distribution of examples are paramount. Select examples that accurately represent the scenarios the model should learn from, and maintain a distribution of examples (e.g., the number of examples per class in classification tasks) that aligns with your actual data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Zero-shot prompt__\n",
    "\n",
    "Below is an example of zero-shot prompting, where we don't provide any examples to the LLM within the prompt itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Positive. The tweet is about the new menu of a restaurant. The person said \"I loved the new menu of this restaurant!\". It is a positive sentiment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: I loved the new menu of this restaurant!\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. One-shot prompt__\n",
    "\n",
    "Below is an example of one-shot prompting, where we provide one example to the LLM within the prompt to give some guidance on what type of response we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "negative"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: I loved the new menu of this restaurant!\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: That was super awful. 😠\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Few-shot prompt__\n",
    "\n",
    "Below is an example of few-shot prompting, where we provide one example to the LLM within the prompt to give some guidance on what type of response we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: I loved the new YouTube video you made!\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: That was super awful😠\n",
    "Sentiment: negative\n",
    "\n",
    "Tweet: Something surprised me about this menu - it was actually original and I've never found anything like this before. \n",
    "Try it - you will not regret it.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "display(Markdown(generation_model.predict(prompt=prompt, max_output_tokens=256).text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Choosing between zero-shot, one-shot, few-shot prompting methods`__\n",
    "\n",
    "Which prompt technique to use will solely depends on our goal. The `zero-shot` prompts are more open-ended and can give creative answers, while `one-shot` and `few-shot` prompts teach the model how to behave so we can get more predictable answers that are consistent with the examples provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
