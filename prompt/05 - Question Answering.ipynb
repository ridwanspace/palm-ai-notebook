{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Large language models can be used for various natural language processing tasks, including question-answering (Q&A). These models are trained on a vast amount text data and can generate high-quality responses to a wide range of questions. One thing to note here is that most models have cutoff dates regarding their knowledge, and asking anything too recent might yield an incomplete, imaginative or incorrect answer (i.e. a hallucination).\n",
    "\n",
    "This notebook covers the essentials of prompts for answering questions using a generative model. In addition, it showcases the open domain (knowledge available on the public internet) and closed domain (knowledge that is more private - typically enterprise or personal knowledge).\n",
    "\n",
    "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview#prompt_structure).\n",
    "\n",
    "## Objective\n",
    "By the end of the notebook, we should be able to write prompts for the following:\n",
    "\n",
    "**Open domain** questions:\n",
    "\n",
    "* Zero-shot prompting\n",
    "* Few-shot prompting\n",
    "\n",
    "**Closed domain** questions:\n",
    "\n",
    "* Providing custom knowledge as context\n",
    "* Instruction-tune the outputs\n",
    "* Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import vertexai\n",
    "from IPython.display import Markdown, display\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate service account (authentication)\n",
    "json_path = '../llm-ai.json' # replace with your own service account\n",
    "credentials = service_account.Credentials.from_service_account_file(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Vertex AI\n",
    "load_dotenv()\n",
    "vertexai.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "Question-answering capabilities require providing a prompt or a question that the model can use to generate a response. The prompt can be a few words or a few complete sentences, depending on the complexity of the question.\n",
    "\n",
    "When creating a question-answering prompt, it is essential to be specific and provide as much context as possible. It helps the model understand the intent behind the question and generate a relevant response. For example, if you want to ask:\n",
    "```bash\n",
    "    \"What is the capital of France?\",\n",
    "\n",
    "    then a good prompt could be:\n",
    "\n",
    "    \"Please tell me the name of the city that serves as the capital of France.\"\n",
    "```\n",
    "\n",
    "\n",
    "In addition to being specific, the prompt should also be grammatically correct and free of spelling errors. It helps the model generate a response that is easy to understand and contains fewer errors or inaccuracies.\n",
    "\n",
    "By providing specific, context-rich prompts, you can help the model understand the intent behind the question and generate accurate and relevant responses.\n",
    "\n",
    "Below are some differences between the **open domain** and **closed domain** categories for question-answering prompts.\n",
    "\n",
    "* **Open domain**: All questions whose answers are available online already. They can belong to any category, like history, geography, countries, politics, chemistry, etc. These include trivia or general knowledge questions, like:\n",
    "- `Q: Who won the Olympic gold in swimming?`\n",
    "- `Q: Who is the President of [given country]?`\n",
    "- `Q: Who wrote [specific book]\"?`\n",
    "\n",
    "Keep in mind the training cutoff of generative models, as questions involving information more recent than what the model was trained on might give incorrect or imaginative answers.\n",
    "\n",
    "* **Closed domain**: If we have some internal knowledge base not available on the public internet, then those belong to the closed domain category.\n",
    "We can pass that \"private\" knowledge as context to the model. If prompted correctly, the model is more likely to answer from within the context provided and less likely to give answers beyond that from the open internet.\n",
    "\n",
    "Consider the example of building a Q&A bot over your internal product documentation. In this case, you can pass the complete documentation to the model and prompt it only to answer based on that.\n",
    "\n",
    "Typical prompt for closed domain:\n",
    "```bash\n",
    "Prompt: f\"\"\" Answer from the below context: \\n\\n\n",
    "    \t   context: {your knowledge base} \\n\n",
    "    \t   question: {question specific to that knowledge base}  \\n\n",
    "    \t   answer: {to be predicted by model} \\n\n",
    "    \t\"\"\"\n",
    "```\n",
    "\n",
    "Below are some examples to understand these different types of prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Open Domain\n",
    "\n",
    "#### 1. Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Dwight D. Eisenhower, Republican"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Q: Who was President of the United States in 1955? Which party did he belong to?\\n\n",
    "            A:\n",
    "         \"\"\"\n",
    "\n",
    "# you may use print instead of display\n",
    "display(Markdown(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "            max_output_tokens=256,\n",
    "            temperature=0.1,\n",
    "        ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Mount Everest"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Q: What is the tallest mountain in the world?\\n\n",
    "            A:\n",
    "         \"\"\"\n",
    "# you may use print instead of display\n",
    "display(Markdown(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "            max_output_tokens=20,\n",
    "            temperature=0.1,\n",
    "        ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Few-shot prompting\n",
    "\n",
    "Let's say we want to a get a short answer from the model (like only a specific name). To do so, we can leverage a few-shot prompt and provide examples to the model to illustrate the expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alexander Fleming"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Q: Who is the current President of France?\\n\n",
    "            A: Emmanuel Macron \\n\\n\n",
    "\n",
    "            Q: Who invented the telephone? \\n\n",
    "            A: Alexander Graham Bell \\n\\n\n",
    "\n",
    "            Q: Who wrote the novel \"1984\"?\n",
    "            A: George Orwell\n",
    "\n",
    "            Q: Who discovered penicillin?\n",
    "            A:\n",
    "         \"\"\"\n",
    "\n",
    "# you may use print instead of display\n",
    "display(Markdown(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "            max_output_tokens=20,\n",
    "            temperature=0.1,\n",
    "        ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero-shot prompting** vs **Few-shot prompting**\n",
    "\n",
    "Zero-shot prompting can be useful for quickly generating text for new tasks, but the quality of the generated text may be lower than that of a few-shot prompt with well-chosen examples. Few-shot prompting is typically better suited for tasks that require a high degree of specificity or domain-specific knowledge, but requires some additional thought and potentially data to set up the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Closed Domain\n",
    "\n",
    "#### 1. Adding internal knowledge as context in prompts\n",
    "\n",
    "Imagine a scenario where we would like to build a question-answering bot that takes in internal documentation and lets users ask questions about it.\n",
    "\n",
    "In the example below, the Google Cloud Storage and content policy documentation is added to the prompt, so that the PaLM API can use that to answer subsequent questions with the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "Answer the question given in the contex below:\n",
      "Context: \n",
      "Storage and content policy \n",
      "\n",
      "How durable is my data in Cloud Storage? \n",
      "\n",
      "Cloud Storage is designed for 99.999999999% (11 9's) annual durability, which is appropriate for even primary storage and\n",
      "business-critical applications. This high durability level is achieved through erasure coding that stores data pieces redundantly\n",
      "across multiple devices located in multiple availability zones.\n",
      "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the\n",
      "write is acknowledged as successful. Checksums are stored and regularly revalidated to proactively verify that the data\n",
      "integrity of all data at rest as well as to detect corruption of data in transit. If required, corrections are automatically\n",
      "made using redundant data. Customers can optionally enable object versioning to add protection against accidental deletion.\n",
      "?\n",
      "\n",
      "Question: How is high availability achieved? \n",
      "\n",
      "Answer:\n",
      "\n",
      "[Response]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The high availability of Cloud Storage is achieved through erasure coding that stores data pieces redundantly across multiple devices located in multiple availability zones."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Storage and content policy \\n\n",
    "How durable is my data in Cloud Storage? \\n\n",
    "Cloud Storage is designed for 99.999999999% (11 9's) annual durability, which is appropriate for even primary storage and\n",
    "business-critical applications. This high durability level is achieved through erasure coding that stores data pieces redundantly\n",
    "across multiple devices located in multiple availability zones.\n",
    "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the\n",
    "write is acknowledged as successful. Checksums are stored and regularly revalidated to proactively verify that the data\n",
    "integrity of all data at rest as well as to detect corruption of data in transit. If required, corrections are automatically\n",
    "made using redundant data. Customers can optionally enable object versioning to add protection against accidental deletion.\n",
    "\"\"\"\n",
    "\n",
    "question = \"How is high availability achieved?\"\n",
    "\n",
    "prompt = f\"\"\"Answer the question given in the contex below:\n",
    "Context: {context}?\\n\n",
    "Question: {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"[Prompt]\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"[Response]\")\n",
    "\n",
    "# you may use print instead of display\n",
    "display(Markdown(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "        ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instruction-tuning outputs\n",
    "\n",
    "Another way to help out language models is to provide additional instructions to frame the output in the prompt. To ensure the model doesn't respond to anything outside the context, the prompt can specify that the response should be _\"Information not available in provided context\"_ if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "Answer the question given the context below as {Context:}. \n",
      "\n",
      "If the answer is not available in the {Context:} and you are not confident about the output,\n",
      "please say \"Information not available in provided context\". \n",
      "\n",
      "\n",
      "Context: \n",
      "Storage and content policy \n",
      "\n",
      "How durable is my data in Cloud Storage? \n",
      "\n",
      "Cloud Storage is designed for 99.999999999% (11 9's) annual durability, which is appropriate for even primary storage and\n",
      "business-critical applications. This high durability level is achieved through erasure coding that stores data pieces redundantly\n",
      "across multiple devices located in multiple availability zones.\n",
      "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the\n",
      "write is acknowledged as successful. Checksums are stored and regularly revalidated to proactively verify that the data\n",
      "integrity of all data at rest as well as to detect corruption of data in transit. If required, corrections are automatically\n",
      "made using redundant data. Customers can optionally enable object versioning to add protection against accidental deletion.\n",
      "?\n",
      "\n",
      "Question: What machined are required for hosting Vertex AI models? \n",
      "\n",
      "Answer:\n",
      "\n",
      "[Response]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Information not available in provided context"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What machined are required for hosting Vertex AI models?\"\n",
    "prompt = f\"\"\"Answer the question given the context below as {{Context:}}. \\n\n",
    "If the answer is not available in the {{Context:}} and you are not confident about the output,\n",
    "please say \"Information not available in provided context\". \\n\\n\n",
    "Context: {context}?\\n\n",
    "Question: {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"[Prompt]\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"[Response]\")\n",
    "\n",
    "# you may use print instead of display\n",
    "display(Markdown(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "            max_output_tokens=256,\n",
    "            temperature=0.3,\n",
    "        ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Leonardo da Vinci painted the Mona Lisa during the Italian Renaissance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Context:\n",
    "The term \"artificial intelligence\" was first coined by John McCarthy in 1956. Since then, AI has developed into a vast\n",
    "field with numerous applications, ranging from self-driving cars to virtual assistants like Siri and Alexa.\n",
    "\n",
    "Question:\n",
    "What is artificial intelligence?\n",
    "\n",
    "Answer:\n",
    "Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "The Wright brothers, Orville and Wilbur, were two American aviation pioneers who are credited with inventing and\n",
    "building the world's first successful airplane and making the first controlled, powered and sustained heavier-than-air human flight,\n",
    " on December 17, 1903.\n",
    "\n",
    "Question:\n",
    "Who were the Wright brothers?\n",
    "\n",
    "Answer:\n",
    "The Wright brothers were American aviation pioneers who invented and built the world's first successful airplane\n",
    "and made the first controlled, powered and sustained heavier-than-air human flight, on December 17, 1903.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "The Mona Lisa is a 16th-century portrait painted by Leonardo da Vinci during the Italian Renaissance. It is one of\n",
    "the most famous paintings in the world, known for the enigmatic smile of the woman depicted in the painting.\n",
    "\n",
    "Question:\n",
    "Who painted the Mona Lisa?\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "display(Markdown(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "        ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Extractive Question-Answering\n",
    "\n",
    "In the next example, the generative model is guided to understand the meaning of the question and the passage, and to identify the relevant information in the passage that answers the question. The model is given a question and a passage of text, and is asked to find the answer to the question within the passage. The answer is typically a phrase or sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reduced moist tropical vegetation cover in the basin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Background: There is evidence that there have been significant changes in Amazon rainforest vegetation over the last 21,000 years through the Last Glacial Maximum (LGM) and subsequent deglaciation.\n",
    "Analyses of sediment deposits from Amazon basin paleo lakes and from the Amazon Fan indicate that rainfall in the basin during the LGM was lower than for the present, and this was almost certainly\n",
    "associated with reduced moist tropical vegetation cover in the basin. There is debate, however, over how extensive this reduction was. Some scientists argue that the rainforest was reduced to small,\n",
    "isolated refugia separated by open forest and grassland; other scientists argue that the rainforest remained largely intact but extended less far to the north, south, and east than is seen today.\n",
    "This debate has proved difficult to resolve because the practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin, and both\n",
    "explanations are reasonably well supported by the available data.\n",
    "\n",
    "Q: What does LGM stands for?\n",
    "A: Last Glacial Maximum.\n",
    "\n",
    "Q: What did the analysis from the sediment deposits indicate?\n",
    "A: Rainfall in the basin during the LGM was lower than for the present.\n",
    "\n",
    "Q: What are some of scientists arguments?\n",
    "A: The rainforest was reduced to small, isolated refugia separated by open forest and grassland.\n",
    "\n",
    "Q: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
    "A: 21,000.\n",
    "\n",
    "Q: What caused changes in the Amazon rainforest vegetation?\n",
    "A: The Last Glacial Maximum (LGM) and subsequent deglaciation\n",
    "\n",
    "Q: What has been analyzed to compare Amazon rainfall in the past and present?\n",
    "A: Sediment deposits.\n",
    "\n",
    "Q: What has the lower rainfall in the Amazon during the LGM been attributed to?\n",
    "A:\n",
    "\"\"\"\n",
    "# you may use print instead of display\n",
    "display(Markdown(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "        ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "We can evaluate the outputs of the question and answering task if the ground truth answers of each question are available. In zero-shot prompting, we can only use open domain questions. However, with closed domain questions, we can add context and evaluate similarly. To showcase how that will work, start by creating a simple dataframe with questions and ground truth answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_groundtruth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a website browser address bar, what does “www” stand for?</td>\n",
       "      <td>World Wide Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first woman to win a Nobel Prize</td>\n",
       "      <td>Marie Curie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the Earth’s largest ocean?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       question  \\\n",
       "0  In a website browser address bar, what does “www” stand for?   \n",
       "1                  Who was the first woman to win a Nobel Prize   \n",
       "2                What is the name of the Earth’s largest ocean?   \n",
       "\n",
       "  answer_groundtruth  \n",
       "0     World Wide Web  \n",
       "1        Marie Curie  \n",
       "2  The Pacific Ocean  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_data = {\n",
    "    \"question\": [\n",
    "        \"In a website browser address bar, what does “www” stand for?\",\n",
    "        \"Who was the first woman to win a Nobel Prize\",\n",
    "        \"What is the name of the Earth’s largest ocean?\",\n",
    "    ],\n",
    "    \"answer_groundtruth\": [\"World Wide Web\", \"Marie Curie\", \"The Pacific Ocean\"],\n",
    "}\n",
    "qa_data_df = pd.DataFrame(qa_data)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data with questions and ground truth answers, we can call the PaLM 2 generation model to each review row using the `apply` function. Each row will use the dynamic prompt to predict the answer using the PaLM API. We will save the results in `answer_prediction` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_groundtruth</th>\n",
       "      <th>answer_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a website browser address bar, what does “www” stand for?</td>\n",
       "      <td>World Wide Web</td>\n",
       "      <td>World Wide Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first woman to win a Nobel Prize</td>\n",
       "      <td>Marie Curie</td>\n",
       "      <td>Marie Curie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the Earth’s largest ocean?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>Pacific Ocean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       question  \\\n",
       "0  In a website browser address bar, what does “www” stand for?   \n",
       "1                  Who was the first woman to win a Nobel Prize   \n",
       "2                What is the name of the Earth’s largest ocean?   \n",
       "\n",
       "  answer_groundtruth answer_prediction  \n",
       "0     World Wide Web    World Wide Web  \n",
       "1        Marie Curie       Marie Curie  \n",
       "2  The Pacific Ocean     Pacific Ocean  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_answer(row):\n",
    "    prompt = f\"\"\"Answer the following question as precise as possible.\\n\\n\n",
    "            question: {row}\n",
    "            answer:\n",
    "              \"\"\"\n",
    "    return generation_model.predict(\n",
    "        prompt=prompt,\n",
    "    ).text\n",
    "\n",
    "\n",
    "qa_data_df[\"answer_prediction\"] = qa_data_df[\"question\"].apply(get_answer)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to evaluate the answers predicted by the PaLM API. However, it will be more complex than the text classification since the answers may differ from ground truth and may be presented in slightly more/fewer words.\n",
    "\n",
    "For example, We can observe the question \"What is the name of the Earth's largest ocean?\" and see that model predicted \"Pacific Ocean\" when a ground truth label is \"The Pacific Ocean\" with the extra \"The.\" Now, if we use the simple classification metrics, then we will consider this as a wrong prediction since original and predicted strings have a difference. However, we can see that the answer is correct since an extra \"The\" is causing the issue. It's a simple string comparison problem.\n",
    "\n",
    "The solution to string comparison where both ground_thruth and predicted may have some extra or fewer letters, one approach is to use a fuzzy matching algorithm. Fuzzy string matching uses [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) to calculate the differences between two strings.\n",
    "\n",
    "For example, the Levenshtein distance between \"kitten\" and \"sitting\" is 3, since the following 3 edits change one into the other, and there is no way to do it with fewer than 3 edits:\n",
    "\n",
    "* kitten → sitten (substitution of \"s\" for \"k\"),\n",
    "* sitten → sittin (substitution of \"i\" for \"e\"),\n",
    "* sittin → sitting (insertion of \"g\" at the end).\n",
    "\n",
    "Here's another example, but this time using fuzzywuzzy library, which gives us the same Levenshtein distance between two strings but in ratio. The ratio raw score measures the string's similarity as an int in the range [0, 100]. For two strings X and Y, the score is defined by int(round((2.0 * M / T) * 100)) where T is the total number of characters in both strings, and M is the number of matches in the two strings.\n",
    "\n",
    "Read more here about the [ratio formula](https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Ratio.html) :\n",
    "\n",
    "We can see one example to understand this further.\n",
    "\n",
    "```bash\n",
    "String1: \"this is a test\"\n",
    "String2: \"this is a test!\"\n",
    "\n",
    "Fuzz Ratio => 97  #\n",
    "\n",
    "Fuzz Partial Ratio => 100  #Since most characters are the same and in a similar sequence, the algorithm calculates the partial ratio as 100 and ignores simple additions (new characters). \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "First, install the package fuzzywuzzy and python-Levenshtein:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "!pip install -q python-Levenshtein --upgrade --user\n",
    "!pip install -q fuzzywuzzy --upgrade --user\n",
    "```\n",
    "\n",
    "Then compute a score to perform fuzzy matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_groundtruth</th>\n",
       "      <th>answer_prediction</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a website browser address bar, what does “www” stand for?</td>\n",
       "      <td>World Wide Web</td>\n",
       "      <td>World Wide Web</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first woman to win a Nobel Prize</td>\n",
       "      <td>Marie Curie</td>\n",
       "      <td>Marie Curie</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the Earth’s largest ocean?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>Pacific Ocean</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       question  \\\n",
       "0  In a website browser address bar, what does “www” stand for?   \n",
       "1                  Who was the first woman to win a Nobel Prize   \n",
       "2                What is the name of the Earth’s largest ocean?   \n",
       "\n",
       "  answer_groundtruth answer_prediction  match_score  \n",
       "0     World Wide Web    World Wide Web          100  \n",
       "1        Marie Curie       Marie Curie          100  \n",
       "2  The Pacific Ocean     Pacific Ocean          100  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def get_fuzzy_match(df):\n",
    "    return fuzz.partial_ratio(df[\"answer_groundtruth\"], df[\"answer_prediction\"])\n",
    "\n",
    "\n",
    "qa_data_df[\"match_score\"] = qa_data_df.apply(get_fuzzy_match, axis=1)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average match score of all predicted answer from PaLM 2 is :  100.0  %\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"the average match score of all predicted answer from PaLM 2 is : \",\n",
    "    qa_data_df[\"match_score\"].mean(),\n",
    "    \" %\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we get `100%` as the mean score, even though some predictions were missing some words. That means we are very close to the ground truth, and some answers are just missing the exact verboseness of the ground truth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
