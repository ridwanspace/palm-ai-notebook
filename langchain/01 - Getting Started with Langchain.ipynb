{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LangChain?\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "`TL;DR` \n",
    "\n",
    "LangChain makes the complicated parts of working & building with language models easier. It helps do this in two ways:\n",
    "\n",
    "1. **Integration** - Bring external data, such as your files, other applications, and API data, to LLMs\n",
    "2. **Agents** - Allows LLMs to interact with its environment via decision making and use LLMs to help decide which action to take next\n",
    "\n",
    "![Langchain Image](../assets/langchain.png)\n",
    "\n",
    "To build effective Generative AI applications, it is key to enable LLMs to interact with external systems. This makes models data-aware and agentic, meaning they can understand, reason, and use data to take action in a meaningful way. The external systems could be _public data corpus_, _private knowledge repositories_, _databases_, _applications_, _APIs_, or _access to the public internet via Google Search_.\n",
    "\n",
    "Here are a few patterns where LLMs can be augmented with other systems:\n",
    "\n",
    "* Convert natural language to SQL, executing the SQL on database, analyze and present the results\n",
    "* Calling an external webhook or API based on the user query\n",
    "* Synthesize outputs from multiple models, or chain the models in a specific order\n",
    "\n",
    "It may look trivial to plumb these calls together and orchestrate them but it becomes a mundane task to write glue code again and again e.g. for every different data connector or a new model. That’s where LangChain comes in!\n",
    "\n",
    "\n",
    "## Why LangChain?\n",
    "\n",
    "LangChain’s modular implementation of components and common patterns combining these components makes it easier to build complex applications based on LLMs. LangChain enables these models to connect to data sources and systems as agents to take action.\n",
    "\n",
    "1. **Components** are abstractions that works to bring external data, such as your documents, databases, applications,APIs to language models. LangChain makes it easy to swap out abstractions and components necessary to work with LLMs.\n",
    "\n",
    "2. **Agents** enable language models to communicate with its environment, where the model then decides the next action to take. LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
    "\n",
    "## LangChain & Vertex AI\n",
    "\n",
    "[Vertex AI PaLM foundational models](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) — Text, Chat, and Embeddings — are officially integrated with the [LangChain Python SDK](https://python.langchain.com/en/latest/index.html), making it convenient to build applications on top of Vertex AI PaLM models. You can now create Generative AI applications by combining the power of Vertex PaLM models with the ease of use and flexibility of LangChain.\n",
    "\n",
    "* [LangChain with Vertex AI PaLM for LLMs](https://python.langchain.com/en/latest/modules/models/llms/integrations/google_vertex_ai_palm.html)\n",
    "* [LangChain with Vertex AI PaLM for Chat](https://python.langchain.com/en/latest/modules/models/chat/integrations/google_vertex_ai_palm.html)\n",
    "* [LangChain with Vertex AI Embedding API for Text](https://python.langchain.com/en/latest/modules/models/text_embedding/examples/google_vertex_ai_palm.html)\n",
    "\n",
    "\n",
    "## Objectives\n",
    "This notebook provides an introductory understanding of Langchain components and use cases of LangChain with Vertex PaLM APIs.\n",
    "\n",
    "* Introduce LangChain components\n",
    "* Showcase LangChain + Vertex PaLM API - Text, Chat and Embedding\n",
    "* Summarizing a large text\n",
    "* Question/Answering from PDF (retrieval based)\n",
    "* Chain LLMs with Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "import vertexai\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# LangChain\n",
    "import langchain\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.0.352\n",
      "Vertex AI SDK version: 1.38.1\n"
     ]
    }
   ],
   "source": [
    "# check Langchain\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "# check Vertex AI\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate service account (authentication)\n",
    "json_path = '../llm-ai.json' # replace with your own service account\n",
    "credentials = service_account.Credentials.from_service_account_file(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Vertex AI\n",
    "load_dotenv()\n",
    "vertexai.init(project=os.environ[\"PROJECT_ID\"], # replace with your own project\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Components\n",
    "\n",
    "Let’s take a quick tour of LangChain framework and concepts to be aware of. LangChain offers a variety of modules that can be used to create language model applications. These modules can be combined to create more complex applications, or can be used individually for simpler applications.\n",
    "\n",
    "![Langchain Component](../assets/langchain-component.png)\n",
    "\n",
    "* **Models** are the building block of LangChain providing an interface to different types of AI models. Large Language Models (LLMs), Chat and Text Embeddings models are supported model types.\n",
    "* **Prompts** refers to the input to the model, which is typically constructed from multiple components. LangChain provides interfaces to construct and work with prompts easily - Prompt Templates, Example Selectors and Output Parsers.\n",
    "* **Memory** provides a construct for storing and retrieving messages during a conversation which can be either short term or long term.\n",
    "* **Indexes** help LLMs interact with documents by providing a way to structure them. LangChain provides Document Loaders to load documents, Text Splitters to split documents into smaller chunks, Vector Stores to store documents as embeddings, and Retrievers to fetch relevant documents.\n",
    "* **Chains** let you combine modular components (or other chains) in a specific order to complete a task.\n",
    "* **Agents** are a powerful construct in LangChain allowing LLMs to communicate with external systems via Tools and observe and decide on the best course of action to complete a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Working with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM model for text using langchain\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Text**\n",
    "\n",
    "Text is the natural language way to interact with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Baku is the capital and largest city of Azerbaijan. It is located on the Caspian Sea and has a population of over 2.1 million people. Baku is a major economic and cultural center in Azerbaijan and is home to many historical and cultural sites, including the Maiden Tower and the Shirvanshahs' Palace."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll be working with simple strings (that'll soon grow in complexity!)\n",
    "my_text = \"What is the capital city of Azerbaijan?\"\n",
    "\n",
    "response = llm(my_text)\n",
    "\n",
    "# you may use print or display\n",
    "display(Markdown\n",
    "    (response)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Chat Messages**\n",
    "\n",
    "Chat is like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "* System - Helpful context that tells the AI what to do\n",
    "* Human - Messages intended to represent the user\n",
    "* AI - Messages showing what the AI responded with\n",
    "\n",
    "For more information, see [LangChain Documentation for Chat Models](https://python.langchain.com/en/latest/modules/models/chat/getting_started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat using Langchain\n",
    "chat = ChatVertexAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Hello! How can I help you today?')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example to use Chat\n",
    "chat([HumanMessage(content=\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You could try making a potato-based dish like mashed potatoes, potato soup, or potato salad.\n"
     ]
    }
   ],
   "source": [
    "# add more example by using conversation-like style\n",
    "res = chat(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"\n",
    "        ),\n",
    "        HumanMessage(content=\"I like potato, what should I eat?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# you may use print or display\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass more chat history w/ responses from the AI (memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To make mashed potatoes, you will need the following ingredients:\n",
      "\n",
      "- Potatoes: Russet potatoes are the best choice for mashed potatoes because they have a high starch content, which makes them creamy and fluffy.\n",
      "- Milk: Whole milk is best for mashed potatoes because it adds richness and flavor.\n",
      "- Butter: Butter adds flavor and richness to mashed potatoes.\n",
      "- Salt and pepper: To taste.\n",
      "- Optional ingredients: You can also add other ingredients to your mashed potatoes, such as sour cream, cheese, chives, or bacon.\n"
     ]
    }
   ],
   "source": [
    "res = chat(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"What are the ingredients required for making a mashed potato?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The ingredients I mentioned can serve approximately 4-6 people. This is based on the assumption that each person will have a moderate serving of each ingredient. However, the actual number of people who can enjoy the ingredients may vary depending on individual appetites and portion sizes.\n"
     ]
    }
   ],
   "source": [
    "res = chat([HumanMessage(content=\"How many people could enjoy the ingredients you said?\")])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models - The interface to the AI brains\n",
    "LangChain supports 3 model primitives:\n",
    "\n",
    "* LLMs\n",
    "* Chat Models\n",
    "* Text Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Language Model (LLM)**\n",
    "\n",
    "Language model does text in ➡️ text out!\n",
    "\n",
    "[LangChain](https://python.langchain.com/en/latest/modules/models/llms/integrations/google_vertex_ai_palm.html) LLMs are integrated with [Vertex AI PaLM API for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summer is the season that comes after spring. It is the warmest season of the year and is characterized by long days and warm weather. Summer is the time for vacations, outdoor activities, and summer fun.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What season comes after spring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Chat Model**\n",
    "\n",
    "Chat model that takes a series of messages and returns a message output.\n",
    "\n",
    "[LangChain Chat Model](https://python.langchain.com/en/latest/modules/models/chat/integrations/google_vertex_ai_palm.html) is integrated with [Vertex AI PaLM API for Chat](https://cloud.google.com/vertex-ai/docs/generative-ai/chat/chat-prompts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " To travel to Yogyakarta, you can take a flight from your current location to Yogyakarta International Airport (YIA). Several airlines operate direct flights to YIA, including Garuda Indonesia, Batik Air, and Lion Air. The flight duration varies depending on your departure city, but it typically takes around 1-2 hours.\n",
       "\n",
       "Once you arrive at YIA, you can take a taxi or ride-hailing service to your hotel or destination in Yogyakarta. The airport is located about 10 kilometers from the city center, and the journey takes approximately 20-30 minutes.\n",
       "\n",
       "Alternatively, you can also travel to Yogyakarta by"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful AI bot to figure out travel plans.\"),\n",
    "        HumanMessage(content=\"I would like to go to Yogyakarta, how should I do this?\"),\n",
    "    ]\n",
    ")\n",
    "# you may use print or display\n",
    "display(Markdown(res.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Text Embedding Model**\n",
    "\n",
    "Embeddings are a way of representing data–almost any kind of data, like text, images, videos, users, music, whatever–as points in space where the locations of those points in space are semantically meaningful. Embeddings transform your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Vectors are often used when comparing two pieces of text together. An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors.\n",
    "\n",
    "[LangChain Text Embedding Model](https://python.langchain.com/en/latest/modules/models/text_embedding/examples/google_vertex_ai_palm.html) is integrated with [Vertex AI Embedding API for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings).\n",
    "\n",
    "BTW: `Semantic` means _'relating to meaning in language or logic.'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model_name will become a required arg for VertexAIEmbeddings starting from Feb-01-2024. Currently the default is set to textembedding-gecko@001\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for playing soccer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 768\n",
      "Here's a sample: [0.01769391819834709, 0.0001060579888871871, 0.01302630640566349, 0.03995654731988907, 0.03265838697552681]...\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f\"Your embedding is length {len(text_embedding)}\")\n",
    "print(f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prompt\n",
    "\n",
    "Prompts are text used as instructions to your model. For more details have a look at the notebook [Intro to prompt engineering](https://github.com/ridwanspace/vertex-ai-gcp-notebook/blob/main/prompt/01%20-%20Intro%20to%20Prompt%20Engineering.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The statement is wrong because Wednesday comes after Tuesday, not Monday.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Prompt Template**\n",
    "\n",
    "[Prompt Template](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html) is an object that helps to create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an `f-string` in python but for prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "LLM Output: You should visit the Colosseum, the Pantheon and the Trevi Fountain.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Rome\")\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(\"-----------\")\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Example Selectors**\n",
    "\n",
    "[Example selectors](https://python.langchain.com/en/latest/modules/prompts/example_selectors.html) are an easy way to select from a series of examples to dynamically place in-context information into our prompt. Often used when the task is nuanced or has a large list of examples.\n",
    "\n",
    "Check out different types of example selectors [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # This is the number of examples to produce.\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Input: student\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Select a noun!\n",
    "my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classroom'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Output Parsers**\n",
    "\n",
    "[Output Parsers](https://python.langchain.com/en/latest/modules/prompts/output_parsers.html) help to format the output of a model. Usually used for structured output.\n",
    "\n",
    "Two main ideas:\n",
    "\n",
    "1. **Format Instructions**: An autogenerated prompt that tells the LLM how to format it's response based off desired result\n",
    "\n",
    "2. **Parser**: A method to extract model's text output into a desired structure (usually `json`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# How you would like your reponse structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"bad_string\", description=\"This a poorly formatted user input string\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"good_string\", description=\"This is your response, a reformatted response\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly including country, city and state names\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to dbln!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly including country, city and state names\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to dbln!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to dbln!\",\\n\\t\"good_string\": \"Welcome to Dublin!\"\\n}\\n```'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without parsing\n",
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to dbln!', 'good_string': 'Welcome to Dublin!'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with parsing\n",
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Indexes\n",
    "\n",
    "[Indexes](https://docs.langchain.com/docs/components/indexing/) refer to ways to structure documents for LLMs to work with them.\n",
    "\n",
    "\n",
    "* **Document Loaders**\n",
    "\n",
    "Dcoument loaders are ways to import data from other sources. See the [growing list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. There are more on [Llama Index](https://llamahub.ai/) as well that work with LangChain Document Loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 comments\n",
      "Here's a sample:\n",
      "\n",
      "What I Worked On\n",
      "\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(data)} comments\")\n",
    "print(f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Text Splitters**\n",
    "\n",
    "[Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) are a way to deal with input token limits of LLMs by splitting text into chunks.\n",
    "\n",
    "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/docs/modules/data_connection/document_transformers/) to see which is best for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")\n",
    "pg_work = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(pg_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 79 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "What I Worked On \n",
      "\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n",
      "just characters with strong feelings, which I imagined made them\n",
      "deep.The first programs I tried writing were on the IBM 1401 that our\n",
      "school district used for what was then called \"data processing.\"\n",
      "This was in 9th grade, so I was 13 or 14. The school district's\n",
      "1401 happened to be in the basement of our junior high school, and\n",
      "my friend Rich Draves and I got permission to use it. It was like\n",
      "a mini Bond villain's lair down there, with all these alien-looking\n",
      "machines — CPU, disk drives, printer, card reader — sitting up\n",
      "on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran. You had to\n",
      "type programs on punch cards, then stack them in the card reader\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview:\")\n",
    "print(texts[0].page_content, \"\\n\")\n",
    "print(texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Retrievers**\n",
    "\n",
    "[Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html) are a way of storing data such that it can be queried by a language model. Easy way to combine documents with language models.\n",
    "\n",
    "There are [many different types of retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html), the most widely supported is the VectoreStoreRetriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use [Facebook AI Similarity Search (FAISS)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/), a library and a vector database for similarity search and clustering of dense vectors. To generate dense vectors, a.k.a. embeddings, we use LangChain text embeddings model with [Vertex AI Embeddings for Text](https://python.langchain.com/en/latest/modules/models/text_embedding/examples/google_vertex_ai_palm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Connection reset.\n"
     ]
    }
   ],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embed your texts (save the text as embeddings)\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'VertexAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002747FC0DDC0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Connection reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did.By then there was a name for the kind of company Viaweb was, an\n",
      "\"application service provider,\" or ASP. This name didn't last long\n",
      "before it was replaced by \"software as a service,\" but it was cur\n",
      "\n",
      "not build a web app for making web apps? Why not let people edit\n",
      "code on our server through the browser, and then host the resulting\n",
      "applications for them?\n",
      "[9]\n",
      "You could run all sorts of services\n",
      "on t\n"
     ]
    }
   ],
   "source": [
    "# query the retriever to find similarity search\n",
    "docs = retriever.get_relevant_documents(\n",
    "    \"what types of things did the author want to develop or build?\"\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **VectorStores**\n",
    "\n",
    "[Vector Store](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) is a common type of index or a database to store vectors (numerical embeddings). Conceptually, think of them as tables with a column for embeddings (vectors) and a column for metadata.\n",
    "\n",
    "Example\n",
    "\n",
    "![Embedding Example](../assets/embedding-example.png)\n",
    "\n",
    "[Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
    "\n",
    "[Vertex AI Matching Engine](https://cloud.google.com/blog/products/ai-machine-learning/vertex-matching-engine-blazing-fast-and-massively-scalable-nearest-neighbor-search) is fully managed vector store on Google Cloud, developers can just add the embeddings to its index and issue a search query with a key embedding for the blazingly fast vector search.\n",
    "\n",
    "[**LangChain VectorStore is integrated with Vertex AI Matching Engine.**](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/matchingengine.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 52 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Connection reset.\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Connection reset.\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Connection reset.\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Connection reset.\n"
     ]
    }
   ],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 52 embeddings\n",
      "Here's a sample of one: [-0.015958987176418304, -0.014001290313899517, 0.04511090740561485]...\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(embedding_list)} embeddings\")\n",
    "print(f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vectorstore` stores your embeddings (☝️) and make them easily searchable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Memory\n",
    "\n",
    "[Memory](https://python.langchain.com/en/latest/modules/memory/getting_started.html) is the concept of storing and retrieving data in the process of a conversation. Memory helps LLMs remember information you've chatted about in the past or more complicated information retrieval.\n",
    "\n",
    "There are many types of memory, explore [the documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) to see which one fits your use case.\n",
    "\n",
    "\n",
    "* **ConversationBufferMemory**\n",
    "\n",
    "Memory keeps conversation state throughout a user’s interactions with an language model. `ConversationBufferMemory` memory allows for storing of messages and then extracts the messages in a variable.\n",
    "\n",
    "We'll use `ConversationChain` to have a conversation and load context from memory. We will look into Chains in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today?'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hi there! How can I help you today?\n",
      "Human: What is the capital of France?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Paris is the capital and largest city of France.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hi there! How can I help you today?\n",
      "Human: What is the capital of France?\n",
      "AI: Paris is the capital and largest city of France.\n",
      "Human: What are some popular places I can see in France?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Eiffel Tower, the Louvre Museum, and the Palace of Versailles are some of the most popular tourist destinations in France.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What are some popular places I can see in France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hi there! How can I help you today?\n",
      "Human: What is the capital of France?\n",
      "AI: Paris is the capital and largest city of France.\n",
      "Human: What are some popular places I can see in France?\n",
      "AI: The Eiffel Tower, the Louvre Museum, and the Palace of Versailles are some of the most popular tourist destinations in France.\n",
      "Human: What question did I ask first?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You asked me what the capital of France is.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What question did I ask first?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Chains ⛓️⛓️⛓️\n",
    "\n",
    "Chains are a generic concept in LangChain allowing to combine different LLM calls and action automatically.\n",
    "\n",
    "In simple words:\n",
    "```python\n",
    "Summary #1, Summary #2, Summary #3 --> Final Summary\n",
    "```\n",
    "\n",
    "\n",
    "There are [many applications of chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) search to see which are best for your use case.\n",
    "\n",
    "We'll cover a few of them:\n",
    "\n",
    "* **Simple Sequential Chains**\n",
    "\n",
    "[Sequential chains](https://python.langchain.com/en/latest/modules/chains/generic/sequential_chains.html) are a series of chains, called in deterministic order. `SimpleSequentialChain` are easy chains where each step uses the output of an LLM as an input into another. Good for breaking up tasks (and keeping the LLM focused)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCoq au vin\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mIngredients:\n",
      "\n",
      "* 1 cup dry red wine\n",
      "* 1 cup chicken broth\n",
      "* 1/2 cup brandy\n",
      "* 1/4 cup chopped shallots\n",
      "* 1/4 cup chopped garlic\n",
      "* 1 bay leaf\n",
      "* 1 teaspoon dried thyme\n",
      "* 1 teaspoon dried rosemary\n",
      "* 1/2 teaspoon salt\n",
      "* 1/4 teaspoon freshly ground black pepper\n",
      "* 1 3-pound chicken, cut into 8 pieces\n",
      "* 2 tablespoons olive oil\n",
      "* 1 tablespoon unsalted butter\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a large bowl, combine the wine, broth, brandy, shallots, garlic, bay leaf, thyme, rosemary, salt, and pepper. Add the chicken pieces and stir to coat. Cover and refrigerate for at least 30 minutes, or up to overnight.\n",
      "2. Heat the olive oil and butter in a large skillet over medium-high heat. Add the chicken pieces and cook until browned on all sides. Transfer the chicken to a plate.\n",
      "3. Add the marinade to the skillet and bring to a boil. Reduce heat to low and simmer for 15 minutes. Return the chicken to the skillet and cook until cooked through,\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Paris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Summarization Chain**\n",
    "\n",
    "[Summarization Chain](https://python.langchain.com/docs/modules/chains/popular/summarize) easily runs through a long numerous documents and get a summary.\n",
    "\n",
    "There are multiple chain types such as Stuffing, Map-Reduce, Refine, Map-Rerank. Check out [documentation](https://python.langchain.com/docs/modules/chains/how_to/) for other chain types besides `map-reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in the document = 3333\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Get ai help on networking tasks | Google Cloud BlogJump to ContentCloudBlogContact sales Get started for free CloudBlogSolutions & technologyAI & Machine LearningAPI ManagementApplication DevelopmentApplication ModernizationChrome EnterpriseComputeContainers & KubernetesData AnalyticsDatabasesDevOps & SREMaps & GeospatialSecurity & IdentityInfrastructureInfrastructure ModernizationNetworkingProductivity & CollaborationSAP on Google CloudStorage & Data TransferSustainabilityEcosystemIT LeadersIndustriesFinancial ServicesHealthcare & Life SciencesManufacturingMedia & EntertainmentPublic SectorRetailSupply ChainTelecommunicationsPartnersStartups & SMBTraining & CertificationsInside Google CloudGoogle Cloud Next & EventsGoogle Maps PlatformGoogle WorkspaceDevelopers & PractitionersTransform with Google CloudContact sales Get started for free AI & Machine LearningExplain and customize cloud networking with Duet AIDecember 22, 2023Max SaltonstallDeveloper Relations EngineerAmmett WilliamsDeveloper Relations EngineerJoin us at Google Cloud NextEarly bird pricing available now through Jan 31st.Register Cloud Networking gets tricky fast, especially if you have to interconnect cloud networks with on-prem network structures, and find creative ways to link them up just so. Duet AI can help network engineers with such tasks. Let's take a look at how it can help with setting up a Virtual Private Cloud (VPC) and related private connectivity.Help inside consoleIf you've not used Google's\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"inside consoleIf you've not used Google's Cloud Shell to create networks before, Duet AI's integrated chat assistance can guide you through the basics, and provide you with helpful commands. You can ask questions about how the tech works, how to use it, and what you need to get started. It can also provide implementation advice and commands to get you started.For example, you might ask:How can I use gcloud to create my first VPC network in Google Cloud?Duet AI's response should be similar to the following:And of course you can continue the conversation, using follow-up queries or prompts. Since Duet AI is specially trained on Google Cloud architecture, documentation and best practices, it's a great source of advice for learning about VPCs, DNS, IP address spaces and more.Speed up your workI often get confused about subnets, masking, IP spaces and v4 versus v6, so having an always available expert at hand speeds up my ability to get things done, without pausing to look up reference material so often.Try it out, learn more about how it can help, and check out more tips on how Duet AI helps you get your job done more quickly, so you have time for the fun stuff.Posted inAI & Machine LearningDevelopers & PractitionersNetworkingRelated articlesAI & Machine LearningDataflow and Vertex AI: Scalable and efficient model servingBy Barbara Amoros • 4-minute readPartnersZeotap builds marketer’s AI companion with Vertex AIBy Malavika Lakireddy • 5-minute readAI & Machine LearningInsights,\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"• 5-minute readAI & Machine LearningInsights, clustering models and visualizations made easy with Duet AIBy Debi Cabrera • 3-minute readHealthcare & Life SciencesNuclera aims to accelerate drug discovery with Google DeepMind AlphaFold2 on Vertex AIBy Gordon McInroy • 4-minute readFooter LinksFollow usGoogle CloudGoogle Cloud ProductsPrivacyTermsCookies management controlsHelpLanguage‪English‬‪Deutsch‬‪Français‬‪한국어‬‪日本語‬\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Duet AI can help network engineers with tasks such as setting up a Virtual Private Cloud (VPC) and related private connectivity.\n",
      "\n",
      "Duet AI is a tool that can help you create networks in Google Cloud. It can answer your questions about how the tech works, how to use it, and what you need to get started. It can also provide implementation advice and commands to get you started.\n",
      "\n",
      "• 5-minute read: AI & Machine Learning Insights, clustering models and visualizations made easy with Duet AI.\n",
      "• 3-minute read: Healthcare & Life Sciences Nuclera aims to accelerate drug discovery with Google DeepMind AlphaFold2 on Vertex AI.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Duet AI is a tool that can help you create networks in Google Cloud. It can answer your questions about how the tech works, how to use it, and what you need to get started. It can also provide implementation advice and commands to get you started.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    \"https://cloud.google.com/blog/products/ai-machine-learning/get-ai-help-on-networking-tasks\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"# of words in the document = {len(documents[0].page_content)}\")\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Question/Answering Chain**\n",
    "\n",
    "[Question Answering Chains](https://python.langchain.com/docs/use_cases/question_answering/) easily do QA over a set of documents using QA chain. There are multiple ways to do this with LangChain. We use [RetrievalQA chain](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html) which uses `load_qa_chain` under the hood.\n",
    "\n",
    "![QA](../assets/qa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest PDF files\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load GOOG's 10K annual report (92 pages).\n",
    "url = \"https://abc.xyz/investor/static/pdf/20230203_alphabet_10K.pdf\"\n",
    "loader = PyPDFLoader(url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 263\n"
     ]
    }
   ],
   "source": [
    "# split the documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"# of documents = {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAIEmbeddings(project=None, location='us-central1', request_parallelism=5, max_retries=6, stop=None, model_name='textembedding-gecko@001', client=<vertexai.language_models.TextEmbeddingModel object at 0x000002747E18C850>, client_preview=None, temperature=0.0, max_output_tokens=128, top_p=0.95, top_k=40, credentials=None, n=1, streaming=False, instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 18, 'lock': <unlocked _thread.lock object at 0x000002747DB70B70>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x000002747DBEB1C0>, 'embeddings_task_type_supported': False})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select embedding engine - we use Vertex PaLM Embeddings API\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Connection reset.\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
      "Retrying langchain_community.embeddings.vertexai.VertexAIEmbeddings._get_embeddings_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n"
     ]
    }
   ],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# you may use FAISS or Chroma\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose index to the retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain to answer questions\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Uses LLM to synthesize results from the search index.\n",
    "# We use Vertex PaLM Text API for LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"What was Alphabet's net income in 2022?\", 'result': \"Alphabet's net income in 2022 was $59,972.\", 'source_documents': [Document(page_content='Alphabet Inc.\\nCONSOLIDATED STATEMENTS OF INCOME\\n(in millions, except per share amounts)\\n Year Ended December 31,\\n 2020 2021 2022\\nRevenues $ 182,527 $ 257,637 $ 282,836 \\nCosts and expenses:\\nCost of revenues  84,732  110,939  126,203 \\nResearch and development  27,573  31,562  39,500 \\nSales and marketing  17,946  22,912  26,567 \\nGeneral and administrative  11,052  13,510  15,724 \\nTotal costs and expenses  141,303  178,923  207,994 \\nIncome from operations  41,224  78,714  74,842 \\nOther income (expense), net  6,858  12,020  (3,514) \\nIncome before income taxes  48,082  90,734  71,328 \\nProvision for income taxes  7,813  14,701  11,356 \\nNet income $ 40,269 $ 76,033 $ 59,972 \\nBasic net income per share of Class A, Class B, and Class C stock $ 2.96 $ 5.69 $ 4.59 \\nDiluted net income per share of Class A, Class B, and Class C stock $ 2.93 $ 5.61 $ 4.56 \\nSee accompanying notes.Table of Contents Alphabet Inc.\\n48', metadata={'page': 48, 'source': 'https://abc.xyz/investor/static/pdf/20230203_alphabet_10K.pdf'}), Document(page_content='restricted stock units and other  0  (9,754)  0  (1)  (9,755) \\nRepurchases of stock  (530)  (3,404)  0  (55,892)  (59,296) \\nSale of interest in consolidated entities  0  35  0  0  35 \\nNet income  0  0  0  59,972  59,972 \\nOther comprehensive income (loss)  0  0  (5,980)  0  (5,980) \\nBalance as of December 31, 2022  12,849 $ 68,184 $ (7,603) $ 195,563 $ 256,144 \\nSee accompanying notes.Table of Contents Alphabet Inc.\\n50', metadata={'page': 50, 'source': 'https://abc.xyz/investor/static/pdf/20230203_alphabet_10K.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "query = \"What was Alphabet's net income in 2022?\"\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result\n",
    "\n",
    "![Income](../assets/alphabet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How much office space reduction took place in 2023?', 'result': 'The office space reduction in 2023 was approximately $0.5 billion.', 'source_documents': [Document(page_content='For revenues by geography see Note 2 .\\nThe following table presents long-lived assets by geographic area, which includes property and equipment, net \\nand operating lease assets (in millions):\\nAs of December 31,\\n 2021 2022\\nLong-lived assets:\\nUnited States $ 80,207 $ 93,565 \\nInternational  30,351  33,484 \\nTotal long-lived assets $ 110,558 $ 127,049 \\nNote 16.    Subsequent Event  \\nIn January 2023, we announced a reduction of our workforce of approximately 12,000  roles. We expect to \\nincur employee severance and related charges of $1.9 billion  to $2.3 billion , the majority of which will be recognized in \\nthe first quarter of 2023.\\nIn addition, we are taking actions to optimize our global office space. As a result we expect to incur exit costs \\nrelating to office space reductions of approximately $0.5 billion  in the first quarter of 2023.  We may incur additional \\ncharges in the future as we further evaluate our real estate needs. Table of Contents Alphabet Inc.\\n83', metadata={'page': 83, 'source': 'https://abc.xyz/investor/static/pdf/20230203_alphabet_10K.pdf'}), Document(page_content='•Beginning in the first quarter of 2022, we suspended the vast majority of our commercial activities in Russia \\nand effectively ceased business activities of our Russian entity. The ongoing effect of these direct actions on \\nour financial results was not material. The broader economic effects resulting from the war in Ukraine on our \\nfuture financial results may be unpredictable.\\n•Repurchases of Cla ss A and Class C shares were $59.3 billion  for the year ended December 31, 2022 . See \\nNote 11  of the Notes to Consolidated Financial Statements included in Item 8 of this Annual Report on Form \\n10-K for additional information.\\n•Operating cash flow was $91.5 billion  for the year ended December 31, 2022.\\n•Capital expenditures, which primarily reflected in vestments in technical infrastructure , were $31.5 billion  for the \\nyear ended December 31, 2022.\\n•As of December 31, 2022 , we had  190,234  employees .\\nAddition ally, looking ahead to fiscal year 2023:\\n•In January 2023, we announced a reduction of our workforce of approximately 12,000  roles. We expect to \\nincur employee severance and related charges of $1.9 billion  to $2.3 billion , the majority of which will be \\nrecognized in the first quarter of 2023.\\nIn addition, we are taking actions to optimize our global office space. As a result we expect to incur exit costs \\nrelating to office space reductions of approximately $0.5 billion  in the first quarter of 2023. We may incur', metadata={'page': 31, 'source': 'https://abc.xyz/investor/static/pdf/20230203_alphabet_10K.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "query = \"How much office space reduction took place in 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result\n",
    "\n",
    "![Office reduction](../assets/alphabet2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
